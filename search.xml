<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[WebRTC 开发（三）架构设计剖析]]></title>
    <url>%2F2019%2F05%2F04%2Fwebrtc-development-3-analysis-of-architectural-design%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>音视频处理</tag>
        <tag>WebRTC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebRTC 开发（二）源码下载与编译]]></title>
    <url>%2F2019%2F05%2F02%2Fwebrtc-development-2-source-code-download-and-build%2F</url>
    <content type="text"><![CDATA[在使用任何工具之前，我们都有必要对工具做大概地的了解，做到粗犷但不失偏颇，这对我们选用工具和切入点是很关键的。本节的标题虽然是 WebRTC 源码下载与编译，但在这之前，我们有必要大概地了解 WebRTC，比如开发机构、免费性、支持的平台、功能亮点。 WebRTC 是一个免费开源的跨平台项目，由 Google，Mozilla，Opera 等支持，支持 Chrome，Firefox，Opera 以及 Android 和 iOS 平台，能够给浏览器、手机应用和物联网设备提供了实时互动能力。 WebRTC 是一组协议和 API。WebRTC 的起源可追溯到 2011年，经过六年多的时间的发展，在 2017年底 WebRTC 1.0 标准正式出炉。通过 WebRTC 的 Release Notes 可以看到现在最新的 release 版本是 M74 Release Noted。 2015年移动端直播的兴起，观众可以在手机端实时看主播的直播，但是观众与主播之间的沟通需要通过发弹幕来进行，这种交流的实时性较差，沟通不便利，观众参与感较差。2016年初移动端上出现了主播与观众之间可以通过实时视频聊天这种方式来沟通，即，视频连麦。那我们想实现这种视频连麦的功能该怎么做呢？ 现在通过 WebRTC 以及其它一些音视频工具，我们就可以搭建一个视频聊天工具，做到视频连麦，也能做到类似于微信视频群聊。关于各种实现视频连麦的方式，这里就不细说了，放到后面的章节来解说。 了解到 WebRTC 是个什么东西后，通常人的心里就会产生一种跃跃欲试的冲动，想试试，那试试就试试呗。“磨刀不误砍柴工”这句话是有道理的，当我们没有经验和没有人传授经验的时候，那我们就要琢磨这句“磨刀不误砍柴工”了。我们要砍柴做饭，找到一把刀拿起就跑到树林里去砍柴，结果发现刀太钝，砍柴真费劲。这个时候，我们就意识到砍柴刀要用磨刀石磨一磨，磨锋利了，就能提升砍柴的效率。我举这个例子，就想说明两点： 第一，先尝试可以加深理解，获取自己的经验。 第二，通过获取的经验，重新调整，比如调整做事步骤，加深理论学习等。 WebRTC 支持 Windows, Mac OS X, Linux, Android 和 iOS 平台，这里以 iOS 平台为例来描述 WebRTC 的源码下载和编译过程。 选择 Release 版本 step 1从 WebRTC 的 Release Notes 的 release 版本中选择最新的版本，当前最新版为 M74 Release Noted 123456789WebRTC M74 Release NotesWebRTC M74 branch (cut at r26981)SummaryWebRTC M74, currently available in Chrome's beta channel and as native libraries for Android and iOS, contains feature additions like RID/MID based Simulcast, multiple bug fixes, enhancements and stability/performance improvements. As with previous releases, we encourage all developers to run versions of Chrome on the Canary, Dev, and Beta channels frequently and quickly report any issues found. Please take a look at this page, for some pointers on how to file a good bug report. The help we have received has been invaluable! The Chrome release schedule can be found here. Native libraries for Android and iOS are built on a weekly basis and are available on JCenter and CocoaPods; the Changelog is available here. step 2进入分支 WebRTC M74 branch (cut at r26981) 1234567891011chromium / external / webrtc / branch-heads/m74741f9a0 Ignore duplicate sent packets in TransportFeedbackAdapter by Erik Språng · 2 weeks agoe6d5f62 Merge to M74: Allow setting ALR values for screen content again by Erik Språng · 4 weeks ago9c745ae Revert "Disable DTLS 1.0, TLS 1.0 and TLS 1.1 downgrade in WebRTC." by Benjamin Wright · 5 weeks ago1c9bf93 Merge to M74: Fix bug in vp8 FrameDropThreshold by Erik Språng · 5 weeks ago... step 3选择第一条提交记录 [741f9a0 Ignore duplicate sent packets in TransportFeedbackAdapter by Erik Språng · 2 weeks ago] 进入详情页 chromium / external / webrtc / 741f9a0679bc70682b056004f8421879352d1a8d 12345678910111213141516171819202122232425262728293031chromium / external / webrtc / 741f9a0679bc70682b056004f8421879352d1a8dcommit 741f9a0679bc70682b056004f8421879352d1a8d [log] [tgz]author Erik Språng &lt;sprang@webrtc.org&gt; Thu Apr 18 12:28:02 2019committer Erik Språng &lt;sprang@webrtc.org&gt; Fri Apr 19 17:51:34 2019tree 36ee311fa41a5d60ab98d87273f210ac90ed3fdfparent e6d5f62d010d7f6284504af1c7898894851b238c [diff]Ignore duplicate sent packets in TransportFeedbackAdapterBug: webrtc:10564, chromium:954139, webrtc:10509Change-Id: I617b58ef8cf5858d7a81aaa39884c5cc1ac2af6eReviewed-on: https://webrtc-review.googlesource.com/c/src/+/133564Commit-Queue: Erik Språng &lt;sprang@webrtc.org&gt;Reviewed-by: Sebastian Jansson &lt;srte@webrtc.org&gt;Cr-Original-Commit-Position: refs/heads/master@&#123;#27689&#125;(cherry picked from commit d50947ab516ec404b100752617fb828d05cf0e3d)Reviewed-on: https://webrtc-review.googlesource.com/c/src/+/133579Cr-Commit-Position: refs/branch-heads/m74@&#123;#20&#125;Cr-Branched-From: be7af9399ceb88171bf60b50419ff2dec8184fb9-refs/heads/master@&#123;#26981&#125;modules/congestion_controller/rtp/BUILD.gn[diff]modules/congestion_controller/rtp/send_time_history.cc[diff]modules/congestion_controller/rtp/send_time_history.h[diff]modules/congestion_controller/rtp/transport_feedback_adapter.cc[diff]modules/congestion_controller/rtp/transport_feedback_adapter.h[diff]modules/congestion_controller/rtp/transport_feedback_adapter_unittest.cc[diff]6 files changedtree: 36ee311fa41a5d60ab98d87273f210ac90ed3fdf... 获取到该分支的最新提交记录：commit 741f9a0679bc70682b056004f8421879352d1a8d。后面下载源码的时候要用到这个 commit 编号。 安装 depot_tools 工具包 下载源码的时候，要用到 depot_tools 工具包，这是 Chromium 官方推荐的工具包，具备下载、同步、编译、上传代码等功能。depot_tools 的详细介绍见 Using depot_tools。depot_tools 包含如下工具包： 12345678910111213gclientgclgit-clsvn [只在 Windows 上使用]drovercpplint.pypylintpresubmit_support.pyrepowtfweeklygit-gszsh-goodies step 1depot_tools 源码属于 Google 的服务，即墙外资源，在获取 depot_tools 源码前，先需要开启 VPN 服务，然后在终端执行命令 1git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git 123456suntongmiandeMacBook-Pro:~ suntongmian$ cd /Users/suntongmian/Documents/develop/webrtc suntongmiandeMacBook-Pro:webrtc suntongmian$suntongmiandeMacBook-Pro:webrtc suntongmian$ git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git正克隆到 'depot_tools'...fatal: unable to access 'https://chromium.googlesource.com/chromium/tools/depot_tools.git/': Failed to connect to chromium.googlesource.com port 443: Operation timed outsuntongmiandeMacBook-Pro:webrtc suntongmian$ 出现 “Failed to connect to chromium.googlesource.com port 443: Operation timed out” 问题后，需要检查 VPN 服务是否开启，网络状况是否良好。如果在 VPN 服务开启和网络状况良好的情况下，仍然不能 clone 代码成功，那就需要检查终端能否成功访问墙外的资源。 我开了 VPN 服务，网络状况也很好，也能通过 Google 浏览器访问资源，但就是不能 clone depot_tools 源码成功。这个时候，我通过命令 curl 来检查终端是否具备翻墙功能。 首先，在终端执行命令 1curl www.baidu.com 1234suntongmiandeMacBook-Pro:webrtc suntongmian$ curl www.baidu.com&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta content=always name=referrer&gt;&lt;link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&gt;&lt;title&gt;百度一下，你就知道&lt;/title&gt;&lt;/head&gt; &lt;body link=#0000cc&gt; &lt;div id=wrapper&gt; &lt;div id=head&gt; &lt;div class=head_wrapper&gt; &lt;div class=s_form&gt; &lt;div class=s_form_wrapper&gt; &lt;div id=lg&gt; &lt;img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129&gt; &lt;/div&gt; &lt;form id=form name=f action=//www.baidu.com/s class=fm&gt; &lt;input type=hidden name=bdorz_come value=1&gt; &lt;input type=hidden name=ie value=utf-8&gt; &lt;input type=hidden name=f value=8&gt; &lt;input type=hidden name=rsv_bp value=1&gt; &lt;input type=hidden name=rsv_idx value=1&gt; &lt;input type=hidden name=tn value=baidu&gt;&lt;span class="bg s_ipt_wr"&gt;&lt;input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus&gt;&lt;/span&gt;&lt;span class="bg s_btn_wr"&gt;&lt;input type=submit id=su value=百度一下 class="bg s_btn"&gt;&lt;/span&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=u1&gt; &lt;a href=http://news.baidu.com name=tj_trnews class=mnav&gt;新闻&lt;/a&gt; &lt;a href=http://www.hao123.com name=tj_trhao123 class=mnav&gt;hao123&lt;/a&gt; &lt;a href=http://map.baidu.com name=tj_trmap class=mnav&gt;地图&lt;/a&gt; &lt;a href=http://v.baidu.com name=tj_trvideo class=mnav&gt;视频&lt;/a&gt; &lt;a href=http://tieba.baidu.com name=tj_trtieba class=mnav&gt;贴吧&lt;/a&gt; &lt;noscript&gt; &lt;a href=http://www.baidu.com/bdorz/login.gif?login&amp;amp;tpl=mn&amp;amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb&gt;登录&lt;/a&gt; &lt;/noscript&gt; &lt;script&gt;document.write('&lt;a href="http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u='+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&amp;")+ "bdorz_come=1")+ '" name="tj_login" class="lb"&gt;登录&lt;/a&gt;');&lt;/script&gt; &lt;a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;"&gt;更多产品&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=ftCon&gt; &lt;div id=ftConw&gt; &lt;p id=lh&gt; &lt;a href=http://home.baidu.com&gt;关于百度&lt;/a&gt; &lt;a href=http://ir.baidu.com&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;&amp;copy;2017&amp;nbsp;Baidu&amp;nbsp;&lt;a href=http://www.baidu.com/duty/&gt;使用百度前必读&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;意见反馈&lt;/a&gt;&amp;nbsp;京ICP证030173号&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;suntongmiandeMacBook-Pro:webrtc suntongmian$ 可以看到，能很快获取到 www.baidu.com 的资源。说明网络是正常的。 其次，在终端执行命令 1curl www.google.com 123suntongmiandeMacBook-Pro:webrtc suntongmian$ curl www.google.comcurl: (7) Failed to connect to www.google.com port 80: Operation timed outsuntongmiandeMacBook-Pro:webrtc suntongmian$ 可以看到，出现了请求超时的问题。说明通过终端不能访问 Google 服务。那该怎么解决呢？ 我的 MacBook 电脑上使用了 ShadowsocksX 客户端来启动 VPN 服务，但只支持浏览器能使用 VPN 服务。我参考了 MAC 终端走代理服务器 一文中提到的方法来解决这个问题。 我的 ShadowsocksX 客户端中 Socks5 配置信息如下 123本地Socks5监听地址：127.0.0.1本地Socks5监听端口：10808连接超时：60 通过 Socks5 的配置信息，在终端中执行命令 12export http_proxy=socks5://127.0.0.1:10808export https_proxy=socks5://127.0.0.1:10808 123suntongmiandeMacBook-Pro:webrtc suntongmian$ export http_proxy=socks5://127.0.0.1:10808suntongmiandeMacBook-Pro:webrtc suntongmian$ suntongmiandeMacBook-Pro:webrtc suntongmian$ export https_proxy=socks5://127.0.0.1:10808 提示：执行后，只对当前终端起作用。重启终端后，默认失效。 启动了终端代理，然后再次执行命令 1curl www.google.com 123456789suntongmiandeMacBook-Pro:webrtc suntongmian$ curl www.google.com&lt;!doctype html&gt;&lt;html itemscope="" itemtype="http://schema.org/WebPage" lang="en"&gt;&lt;head&gt;&lt;meta content="Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for." name="description"&gt;&lt;meta content="noodp" name="robots"&gt;&lt;meta content="text/html; charset=UTF-8" http-equiv="Content-Type"&gt;&lt;meta content="/logos/doodles/2019/us-teacher-appreciation-week-2019-begins-4994791740801024-l.png" itemprop="image"&gt;&lt;meta content="Happy US Teacher Appreciation Week 2019!" property="twitter:title"&gt;&lt;meta content="Happy US Teacher Appreciation Week 2019! #GoogleDoodle" property="twitter:description"&gt;&lt;meta content="Happy US Teacher Appreciation Week 2019! #GoogleDoodle" property="og:description"&gt;&lt;meta content="summary_large_image" property="twitter:card"&gt;&lt;meta content="@GoogleDoodles" property="twitter:site"&gt;&lt;meta content="https://www.google.com/logos/doodles/2019/us-teacher-appreciation-week-2019-begins-4994791740801024-2x.jpg" property="twitter:image"&gt;&lt;meta content="https://www.google.com/logos/doodles/2019/us-teacher-appreciation-week-2019-begins-4994791740801024-2x.jpg" property="og:image"&gt;&lt;meta content="782" property="og:image:width"&gt;&lt;meta content="400" property="og:image:height"&gt;&lt;title&gt;Google&lt;/title&gt;&lt;script nonce="1cF5wiybAk0ZW7RYxuJPNw=="&gt;(function()&#123;window.google=&#123;kEI:'NETQXJuTBIqw0wLW9pvwDA',kEXPI:'0,1353747,57,1958,2422,697,528,591,139,224,756,819,1258,1893,57,528,144,206,441,226,138,17,482,2332972,322,329193,1294,12383,4855,32691,15248,867,12163,6381,854,2481,2,2,6801,364,3319,1263,4242,224,2209,269,4203,904,575,835,284,2,578,728,2432,58,2,1,3,1297,4323,3700,1267,774,2248,1409,3337,1146,5,2,2,1745,218,2595,3601,669,1050,1808,1397,81,7,1,2,488,620,29,1395,978,2632,5299,1288,2,622,3385,796,1221,37,622,298,753,120,1217,1364,1611,2736,1558,1503,2,631,2562,2,4,2,461,209,46,1764,1979,403,510,125,1594,1013,12,620,2228,655,19,91,228,1593,389,866,326,197,777,1,2,151,215,1017,300,608,97,756,98,392,29,400,992,1107,10,168,9,84,24,187,831,235,78,365,367,450,174,563,167,237,48,553,11,14,10,573,1089,856,37,5,394,141,5,371,10,25,177,130,193,5,55,1110,87,67,89,385,155,144,324,165,28,532,277,93,86,84,103,24,160,68,39,18,21,18,326,276,1226,84,143,291,39,11,59,15,10,112,226,415,183,23,713,213,152,123,105,433,526,1,3,7,7,1,2,185,565,394,11,4,236,97,23,490,646,639,57,45,55,23,22,293,370,327,39,19,163,359,35,82,62,6,452,886,74,337,5937571,2920,5997516,40,2799864,4,1572,549,333,444,1,2,80,1,900,583,1,312,1,8,1,2,2132,1,1,1,1,1,414,1,748,141,59,726,3,7,563,1,1907,6,3,11,84,4,8,8,2,4,4,22,22305075',authuser:0,kscs:'c9c918f0_NETQXJuTBIqw0wLW9pvwDA',kGL:'US'&#125;;google.sn='webhp';google.kHL='en';&#125;)();(function()&#123;google.lc=[];google.li=0;google.getEI=function(a)&#123;for(var b;a&amp;&amp;(!a.getAttribute||!(b=a.getAttribute("eid")));)a=a.parentNode;return b||google.kEI&#125;;google.getLEI=function(a)&#123;for(var b=null;a&amp;&amp;(!a.getAttribute||!(b=a.getAttribute("leid")));)a=a.parentNode;return b&#125;;google.https=function()&#123;return"https:"==window.location.protocol&#125;;google.ml=function()&#123;return null&#125;;google.time=function()&#123;return(new Date).getTime()&#125;;google.log=function(a,b,e,c,g)&#123;if(a=google.logUrl(a,b,e,c,g))&#123;b=new Image;var d=google.lc,f=google.li;d[f]=b;b.onerror=b.onload=b.onabort=function()&#123;delete d[f]&#125;;google.vel&amp;&amp;google.vel.lu&amp;&amp;google.vel.lu(a);b.src=a;google.li=f+1&#125;&#125;;google.logUrl=function(a,b,e,c,g)&#123;var d="",f=google.ls||"";e||-1!=b.search("&amp;ei=")||(d="&amp;ei="+google.getEI(c),-1==b.search("&amp;lei=")&amp;&amp;(c=google.getLEI(c))&amp;&amp;(d+="&amp;lei="+c));c="";!e&amp;&amp;google.cshid&amp;&amp;-1==b.search("&amp;cshid=")&amp;&amp;"slh"!=a&amp;&amp;(c="&amp;cshid="+google.cshid);a=e||"/"+(g||"gen_204")+"?atyp=i&amp;ct="+a+"&amp;cad="+b+d+f+"&amp;zx="+google.time()+c;/^http:/i.test(a)&amp;&amp;google.https()&amp;&amp;(google.ml(Error("a"),!1,&#123;src:a,glmm:1&#125;),a="");return a&#125;;&#125;).call(this);(function()&#123;google.y=&#123;&#125;;google.x=function(a,b)&#123;if(a)var c=a.id;else&#123;do c=Math.random();while(google.y[c])&#125;google.y[c]=[a,b];return!1&#125;;google.lm=[];google.plm=function(a)&#123;google.lm.push.apply(google.lm,a)&#125;;google.lq=[];google.load=function(a,b,c)&#123;google.lq.push([[a],b,c])&#125;;google.loadAll=function(a,b)&#123;google.lq.push([a,b])&#125;;&#125;).call(this);google.f=&#123;&#125;;var a=window.location,b=a.href.indexOf("#");if(0&lt;=b)&#123;var c=a.href.substring(b+1);/(^|&amp;)q=/.test(c)&amp;&amp;-1==c.indexOf("#")&amp;&amp;a.replace("/search?"+c.replace(/(^|&amp;)fp=[^&amp;]*/g,"")+"&amp;cad=h")&#125;;&lt;/script&gt;&lt;style&gt;#gbar,#guser&#123;font-size:13px;padding-top:1px !important;&#125;#gbar&#123;height:22px&#125;#guser&#123;padding-bottom:7px !important;text-align:right&#125;.gbh,.gbd&#123;border-top:1px solid #c9d7f1;font-size:1px&#125;.gbh&#123;height:0;position:absolute;top:24px;width:100%&#125;@media all&#123;.gb1&#123;height:22px;margin-right:.5em;vertical-align:top&#125;#gbar&#123;float:left&#125;&#125;a.gb1,a.gb4&#123;text-decoration:underline !important&#125;a.gb1,a.gb4&#123;color:#00c !important&#125;.gbi .gb4&#123;color:#dd8e27 !important&#125;.gbf .gb4&#123;color:#900 !important&#125;&lt;/style&gt;&lt;style&gt;body,td,a,p,.h&#123;font-family:arial,sans-serif&#125;body&#123;margin:0;overflow-y:scroll&#125;#gog&#123;padding:3px 8px 0&#125;td&#123;line-height:.8em&#125;.gac_m td&#123;line-height:17px&#125;form&#123;margin-bottom:20px&#125;.h&#123;color:#36c&#125;.q&#123;color:#00c&#125;.ts td&#123;padding:0&#125;.ts&#123;border-collapse:collapse&#125;em&#123;font-weight:bold;font-style:normal&#125;.lst&#123;height:25px;width:496px&#125;.gsfi,.lst&#123;font:18px arial,sans-serif&#125;.gsfs&#123;font:17px arial,sans-serif&#125;.ds&#123;display:inline-box;display:inline-block;margin:3px 0 4px;margin-left:4px&#125;input&#123;font-family:inherit&#125;a.gb1,a.gb2,a.gb3,a.gb4&#123;color:#11c !important&#125;body&#123;background:#fff;color:black&#125;a&#123;color:#11c;text-decoration:none&#125;a:hover,a:active&#123;text-decoration:underline&#125;.fl a&#123;color:#36c&#125;a:visited&#123;color:#551a8b&#125;a.gb1,a.gb4&#123;text-decoration:underline&#125;a.gb3:hover&#123;text-decoration:none&#125;#ghead a.gb2:hover&#123;color:#fff !important&#125;.sblc&#123;padding-top:5px&#125;.sblc a&#123;display:block;margin:2px 0;margin-left:13px;font-size:11px&#125;.lsbb&#123;background:#eee;border:solid 1px;border-color:#ccc #999 #999 #ccc;height:30px&#125;.lsbb&#123;display:block&#125;.ftl,#fll a&#123;display:inline-block;margin:0 12px&#125;.lsb&#123;background:url(/images/nav_logo229.png) 0 -261px repeat-x;border:none;color:#000;cursor:pointer;height:30px;margin:0;outline:0;font:15px arial,sans-serif;vertical-align:top&#125;.lsb:active&#123;background:#ccc&#125;.lst:focus&#123;outline:none&#125;&lt;/style&gt;&lt;script nonce="1cF5wiybAk0ZW7RYxuJPNw=="&gt;&lt;/script&gt;&lt;/head&gt;&lt;body bgcolor="#fff"&gt;&lt;script nonce="1cF5wiybAk0ZW7RYxuJPNw=="&gt;(function()&#123;var src='/images/nav_logo229.png';var iesg=false;document.body.onload = function()&#123;window.n &amp;&amp; window.n();if (document.images)&#123;new Image().src=src;&#125;if (!iesg)&#123;document.f&amp;&amp;document.f.q.focus();document.gbqf&amp;&amp;document.gbqf.q.focus();&#125;&#125;&#125;)();&lt;/script&gt;&lt;div id="mngb"&gt; &lt;div id=gbar&gt;&lt;nobr&gt;&lt;b class=gb1&gt;Search&lt;/b&gt; &lt;a class=gb1 href="http://www.google.com/imghp?hl=en&amp;tab=wi"&gt;Images&lt;/a&gt; &lt;a class=gb1 href="http://maps.google.com/maps?hl=en&amp;tab=wl"&gt;Maps&lt;/a&gt; &lt;a class=gb1 href="https://play.google.com/?hl=en&amp;tab=w8"&gt;Play&lt;/a&gt; &lt;a class=gb1 href="http://www.youtube.com/?gl=US&amp;tab=w1"&gt;YouTube&lt;/a&gt; &lt;a class=gb1 href="http://news.google.com/nwshp?hl=en&amp;tab=wn"&gt;News&lt;/a&gt; &lt;a class=gb1 href="https://mail.google.com/mail/?tab=wm"&gt;Gmail&lt;/a&gt; &lt;a class=gb1 href="https://drive.google.com/?tab=wo"&gt;Drive&lt;/a&gt; &lt;a class=gb1 style="text-decoration:none" href="https://www.google.com/intl/en/about/products?tab=wh"&gt;&lt;u&gt;More&lt;/u&gt; &amp;raquo;&lt;/a&gt;&lt;/nobr&gt;&lt;/div&gt;&lt;div id=guser width=100%&gt;&lt;nobr&gt;&lt;span id=gbn class=gbi&gt;&lt;/span&gt;&lt;span id=gbf class=gbf&gt;&lt;/span&gt;&lt;span id=gbe&gt;&lt;/span&gt;&lt;a href="http://www.google.com/history/optout?hl=en" class=gb4&gt;Web History&lt;/a&gt; | &lt;a href="/preferences?hl=en" class=gb4&gt;Settings&lt;/a&gt; | &lt;a target=_top id=gb_70 href="https://accounts.google.com/ServiceLogin?hl=en&amp;passive=true&amp;continue=http://www.google.com/" class=gb4&gt;Sign in&lt;/a&gt;&lt;/nobr&gt;&lt;/div&gt;&lt;div class=gbh style=left:0&gt;&lt;/div&gt;&lt;div class=gbh style=right:0&gt;&lt;/div&gt; &lt;/div&gt;&lt;center&gt;&lt;br clear="all" id="lgpd"&gt;&lt;div id="lga"&gt;&lt;a href="/search?ie=UTF-8&amp;amp;q=teacher+appreciation+week&amp;amp;oi=ddle&amp;amp;ct=120236436&amp;amp;hl=en&amp;amp;kgmid=/m/0_m0g58&amp;amp;sa=X&amp;amp;ved=0ahUKEwib_c2ljofiAhUK2FQKHVb7Bs4QPQgD"&gt;&lt;img alt="Happy US Teacher Appreciation Week 2019!" border="0" height="220" src="/logos/doodles/2019/us-teacher-appreciation-week-2019-begins-4994791740801024-l.png" title="Happy US Teacher Appreciation Week 2019!" width="430" id="hplogo" onload="window.lol&amp;&amp;lol()"&gt;&lt;br&gt;&lt;/a&gt;&lt;br&gt;&lt;/div&gt;&lt;form action="/search" name="f"&gt;&lt;table cellpadding="0" cellspacing="0"&gt;&lt;tr valign="top"&gt;&lt;td width="25%"&gt;&amp;nbsp;&lt;/td&gt;&lt;td align="center" nowrap=""&gt;&lt;input name="ie" value="ISO-8859-1" type="hidden"&gt;&lt;input value="en" name="hl" type="hidden"&gt;&lt;input name="source" type="hidden" value="hp"&gt;&lt;input name="biw" type="hidden"&gt;&lt;input name="bih" type="hidden"&gt;&lt;div class="ds" style="height:32px;margin:4px 0"&gt;&lt;input style="color:#000;margin:0;padding:5px 8px 0 6px;vertical-align:top" autocomplete="off" class="lst" value="" title="Google Search" maxlength="2048" name="q" size="57"&gt;&lt;/div&gt;&lt;br style="line-height:0"&gt;&lt;span class="ds"&gt;&lt;span class="lsbb"&gt;&lt;input class="lsb" value="Google Search" name="btnG" type="submit"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="ds"&gt;&lt;span class="lsbb"&gt;&lt;input class="lsb" value="I'm Feeling Lucky" name="btnI" onclick="if(this.form.q.value)this.checked=1; else top.location='/doodles/'" type="submit"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;&lt;td class="fl sblc" align="left" nowrap="" width="25%"&gt;&lt;a href="/advanced_search?hl=en&amp;amp;authuser=0"&gt;Advanced search&lt;/a&gt;&lt;a href="/language_tools?hl=en&amp;amp;authuser=0"&gt;Language tools&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;input id="gbv" name="gbv" type="hidden" value="1"&gt;&lt;script nonce="1cF5wiybAk0ZW7RYxuJPNw=="&gt;(function()&#123;var a,b="1";if(document&amp;&amp;document.getElementById)if("undefined"!=typeof XMLHttpRequest)b="2";else if("undefined"!=typeof ActiveXObject)&#123;var c,d,e=["MSXML2.XMLHTTP.6.0","MSXML2.XMLHTTP.3.0","MSXML2.XMLHTTP","Microsoft.XMLHTTP"];for(c=0;d=e[c++];)try&#123;new ActiveXObject(d),b="2"&#125;catch(h)&#123;&#125;&#125;a=b;if("2"==a&amp;&amp;-1==location.search.indexOf("&amp;gbv=2"))&#123;var f=google.gbvu,g=document.getElementById("gbv");g&amp;&amp;(g.value=a);f&amp;&amp;window.setTimeout(function()&#123;location.href=f&#125;,0)&#125;;&#125;).call(this);&lt;/script&gt;&lt;/form&gt;&lt;div id="gac_scont"&gt;&lt;/div&gt;&lt;div style="font-size:83%;min-height:3.5em"&gt;&lt;br&gt;&lt;div id="prm"&gt;&lt;style&gt;.szppmdbYutt__middle-slot-promo&#123;font-size:small;margin-bottom:32px&#125;.szppmdbYutt__middle-slot-promo a.ZIeIlb&#123;display:inline-block;text-decoration:none&#125;.szppmdbYutt__middle-slot-promo img&#123;border:none;margin-right:5px;vertical-align:middle&#125;&lt;/style&gt;&lt;div class="szppmdbYutt__middle-slot-promo" data-ved="0ahUKEwib_c2ljofiAhUK2FQKHVb7Bs4QnIcBCAQ"&gt;&lt;a class="NKcBbd" href="https://www.google.com/url?q=https://www.blog.google/outreach-initiatives/education/teacher-appreciation-week-2019/%3Futm_source%3Dgoogle%26utm_medium%3Dhpp%26utm_campaign%3Dtaw_2019&amp;amp;source=hpp&amp;amp;id=19012032&amp;amp;ct=3&amp;amp;usg=AFQjCNEqnJb2uHoMjg2Wud6MtvKcn2ILpg&amp;amp;sa=X&amp;amp;ved=0ahUKEwib_c2ljofiAhUK2FQKHVb7Bs4Q8IcBCAU" rel="nofollow"&gt;We&amp;#8217;re supporting teachers inspiring the next generation&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;span id="footer"&gt;&lt;div style="font-size:10pt"&gt;&lt;div style="margin:19px auto;text-align:center" id="fll"&gt;&lt;a href="/intl/en/ads/"&gt;Advertising?Programs&lt;/a&gt;&lt;a href="/services/"&gt;Business Solutions&lt;/a&gt;&lt;a href="/intl/en/about.html"&gt;About Google&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;p style="color:#767676;font-size:8pt"&gt;&amp;copy; 2019 - &lt;a href="/intl/en/policies/privacy/"&gt;Privacy&lt;/a&gt; - &lt;a href="/intl/en/policies/terms/"&gt;Terms&lt;/a&gt;&lt;/p&gt;&lt;/span&gt;&lt;/center&gt;&lt;script nonce="1cF5wiybAk0ZW7RYxuJPNw=="&gt;(function()&#123;window.google.cdo=&#123;height:0,width:0&#125;;(function()&#123;var a=window.innerWidth,b=window.innerHeight;if(!a||!b)&#123;var c=window.document,d="CSS1Compat"==c.compatMode?c.documentElement:c.body;a=d.clientWidth;b=d.clientHeight&#125;a&amp;&amp;b&amp;&amp;(a!=google.cdo.width||b!=google.cdo.height)&amp;&amp;google.log("","","/client_204?&amp;atyp=i&amp;biw="+a+"&amp;bih="+b+"&amp;ei="+google.kEI);&#125;).call(this);&#125;)();(function()&#123;var u='/xjs/_/js/k\x3dxjs.hp.en_US.Yv_bmieVcKQ.O/m\x3dsb_he,d/am\x3dwKAW/rt\x3dj/d\x3d1/rs\x3dACT90oG3DzwVTl5n4Q_0THsxrUHGQsAO3g';setTimeout(function()&#123;var a=document.createElement("script");a.src=u;google.timers&amp;&amp;google.timers.load&amp;&amp;google.tick&amp;&amp;google.tick("load","xjsls");document.body.appendChild(a)&#125;,0);&#125;)();(function()&#123;window.google.xjsu='/xjs/_/js/k\x3dxjs.hp.en_US.Yv_bmieVcKQ.O/m\x3dsb_he,d/am\x3dwKAW/rt\x3dj/d\x3d1/rs\x3dACT90oG3DzwVTl5n4Q_0THsxrUHGQsAO3g';&#125;)();function _DumpException(e)&#123;throw e;&#125;function _F_installCss(c)&#123;&#125;(function()&#123;google.spjs=false;&#125;)();google.sm=1;(function()&#123;var pmc='&#123;\x22Qnk92g\x22:&#123;&#125;,\x22RWGcrA\x22:&#123;&#125;,\x22U5B21g\x22:&#123;&#125;,\x22YFCs/g\x22:&#123;&#125;,\x22ZI/YVQ\x22:&#123;&#125;,\x22d\x22:&#123;&#125;,\x22sb_he\x22:&#123;\x22agen\x22:true,\x22cgen\x22:true,\x22client\x22:\x22heirloom-hp\x22,\x22dh\x22:true,\x22dhqt\x22:true,\x22ds\x22:\x22\x22,\x22ffql\x22:\x22en\x22,\x22fl\x22:true,\x22host\x22:\x22google.com\x22,\x22isbh\x22:28,\x22jsonp\x22:true,\x22msgs\x22:&#123;\x22cibl\x22:\x22Clear Search\x22,\x22dym\x22:\x22Did you mean:\x22,\x22lcky\x22:\x22I\\u0026#39;m Feeling Lucky\x22,\x22lml\x22:\x22Learn more\x22,\x22oskt\x22:\x22Input tools\x22,\x22psrc\x22:\x22This search was removed from your \\u003Ca href\x3d\\\x22/history\\\x22\\u003EWeb History\\u003C/a\\u003E\x22,\x22psrl\x22:\x22Remove\x22,\x22sbit\x22:\x22Search by image\x22,\x22srch\x22:\x22Google Search\x22&#125;,\x22ovr\x22:&#123;&#125;,\x22pq\x22:\x22\x22,\x22refpd\x22:true,\x22rfs\x22:[],\x22sbpl\x22:24,\x22sbpr\x22:24,\x22scd\x22:10,\x22sce\x22:5,\x22stok\x22:\x22s4ND7ehgr2lcHpcv5T93UySs4ho\x22,\x22uhde\x22:false&#125;&#125;';google.pmc=JSON.parse(pmc);&#125;)();&lt;/script&gt; &lt;/body&gt;&lt;/html&gt;suntongmiandeMacBook-Pro:webrtc suntongmian$ suntongmiandeMacBook-Pro:webrtc suntongmian$ 发现可以正常访问 www.google.com 的资源了。 解决了终端的代理问题后，再次执行命令 1git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git 12345678910suntongmiandeMacBook-Pro:webrtc suntongmian$ git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git正克隆到 'depot_tools'...remote: Sending approximately 24.21 MiB ...remote: Total 31833 (delta 22312), reused 31833 (delta 22312)接收对象中: 100% (31833/31833), 24.21 MiB | 1.25 MiB/s, 完成.处理 delta 中: 100% (22312/22312), 完成.suntongmiandeMacBook-Pro:webrtc suntongmian$ suntongmiandeMacBook-Pro:webrtc suntongmian$ lsdepot_toolssuntongmiandeMacBook-Pro:webrtc suntongmian$ 到此，可以看到 depot_tools 源码已经下载成功了。 step 2获取 depot_tools 文件夹所在的目录，在终端执行命令 1pwd 123456suntongmiandeMacBook-Pro:webrtc suntongmian$ lsdepot_toolssuntongmiandeMacBook-Pro:webrtc suntongmian$ suntongmiandeMacBook-Pro:webrtc suntongmian$ pwd/Users/suntongmian/Documents/develop/webrtcsuntongmiandeMacBook-Pro:webrtc suntongmian$ step 3添加环境变量，命令格式为 1export PATH=$PATH:/path/depot_tools 该命令会生成一个临时路径。其中，path 为上一步通过 pwd 命令获取的 depot_tools 文件夹所在目录 在终端执行如下命令 12suntongmiandeMacBook-Pro:webrtc suntongmian$ export PATH=$PATH:/Users/suntongmian/Documents/develop/webrtc/depot_toolssuntongmiandeMacBook-Pro:webrtc suntongmian$ step 4到此，depot_tools 的配置已经完成。想知道 depot_tools 是否成功安装，在终端执行命令 1fetch --help 123456789101112131415161718192021222324252627282930313233343536373839404142suntongmiandeMacBook-Pro:webrtc suntongmian$ fetch --helpusage: fetch.py [options] &lt;config&gt; [--property=value [--property2=value2 ...]]This script can be used to download the Chromium sources. Seehttp://www.chromium.org/developers/how-tos/get-the-codefor full usage instructions.Valid options: -h, --help, help Print this message. --nohooks Don't run hooks after checkout. --force (dangerous) Don't look for existing .gclient file. -n, --dry-run Don't run commands, only print them. --no-history Perform shallow clones, don't fetch the full git history.Valid fetch configs: android android_internal breakpad chromium config_util crashpad dart depot_tools goma_client gyp infra infra_internal inspector_protocol ios ios_internal nacl naclports node-ci pdfium skia skia_buildbot syzygy v8 webrtc webrtc_android webrtc_iossuntongmiandeMacBook-Pro:webrtc suntongmian$ 打印上面的信息，说明 depot_tools 工具包已经成功安装。接下来就可以通过 depot_tools 工具下载 WebRTC 源码了。 下载 WebRTC 源码 WebRTC 文件量较大，在下载之前，磁盘的剩余可用容量要满足： Linux: 6.4 GB.Linux (with Android): 16 GB (of which ~8 GB is Android SDK+NDK images).Mac (with iOS support): 5.6GB 我使用的是 MacBook Pro 2015款机器： 12345678910macOS Mojave版本 10.14.4MacBook Pro (Retina, 13-inch, Early 2015)处理器 2.7 GHz Intel Core i5内存 8 GB 1867 MHz DDR3启动磁盘 Macintosh HD图形卡 Intel Iris Graphics 6100 1536 MB磁盘空间 128 GB可用磁盘空间 17.11 GB 我的 Mac 电脑可用磁盘空间 17.11 GB，是足以装的下 5.6 GB 文件的，好了，接下来就可以放心下载源码了。 在终端执行命令 12fetch --nohooks webrtc_iosgclient sync -r 741f9a0679bc70682b056004f8421879352d1a8d 在 “安装 depot_tools 工具包” 的步骤中，通过 fetch –help，可以看到代码目录 webrtc，webrtc_android，webrtc_ios 等。我要在 iOS 平台上使用，就选择了 webrtc_ios。选择依据见 WebRTC Native Code, iOS 中提到的 “This will fetch a regular WebRTC checkout with the iOS-specific parts added.” 741f9a0679bc70682b056004f8421879352d1a8d 是 “选择 Release 版本” 步骤中获取的 M74 Release 版本的 commit 编号信息。 这2条命令执行时，要下载的文件比较多，需要耐心等待命令的执行结果。 执行编译 编译的方式有2种选择，即使用 ninja 和 Xcode，详见 WebRTC Native Code, iOS。 参考文献 [1] WebRTC Home [2] WebRTC becomes design-complete strengthening the Web Platform as a solid actor in the telecommunications arena [3] WebRTC 1.0: Real-time Communication Between Browsers, W3C Candidate Recommendation 02 November 2017 [4] WebRTC 1.0: Real-time Communication Between Browsers, W3C Candidate Recommendation 27 September 2018 [5] WebRTC Release Notes [6] WebRTC Native Code, Development [7] WebRTC Native Code, iOS [8] Using depot_tools [9] depot_tools_tutorial(7) Manual Page [10] MAC 终端走代理服务器]]></content>
      <tags>
        <tag>音视频处理</tag>
        <tag>WebRTC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebRTC 开发（一）引言]]></title>
    <url>%2F2019%2F05%2F01%2Fwebrtc-development-1-preface%2F</url>
    <content type="text"><![CDATA[三年前在 《聊一聊互动直播》中一文中谈了些对 WebRTC 和 RTC 的看法。现在 WebRTC 的热度不断攀升，我对 WebRTC 的热情也是一度高涨的，对实时音视频以及大数据、机器学习、深度学习这些方向的发展一直保持着关注。在七牛云工作了三年，这三年的远程办公经历，总结下来就是太忙了，工作和私人时间搅合在一起，感觉一天二十四小时都在工作，有些疲惫不堪，但收获也是有的。而今，经过一段时间的休息调整，状态恢复的不错，有时间思考，有时间写写自己的看法，还有点时间感受生活。 摧毁一个人，最直接最暴力的方式就是让他没有时间思考。没有时间思考的人，也是一个不懂得交流的人。如果一个人没有时间思考，也就没有自己的想法，更谈不上与人交流产生火花。时间是个很神奇的东西，能成就人，也能让人“几十年如一日”没有长进。特别是做程序员这行，思考力与实践力至关重要。对于生活与工作，作为人，我们更需要思考，会工作会生活。 2015年接触 WebRTC，那时候我还在深圳工作，2016年去了北京工作，在这四年多时间里，虽然一直都在从事 WebRTC 以及其它音视频相关的工作，我自认为在 WebRTC 这块还需要深入钻研，毕竟音视频这块的东西，要做的深入是必须要对音视频理论和流媒体协议掌握的有足够深度。在一个方向上走的越远，就越觉得掌握的东西越少。实践是检验真理的唯一标准，这个实践可以是完成一个小项目，写一篇文章，也可以是一场面试，一次与高手过招。所以，我就想写写关于 WebRTC 的一些分析性的文章，回顾过去，记录现在，以及展望将来。]]></content>
      <tags>
        <tag>音视频处理</tag>
        <tag>WebRTC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BF 算法与 KMP 算法]]></title>
    <url>%2F2019%2F04%2F29%2Fbf-algorithm-and-kmp-algorithm%2F</url>
    <content type="text"><![CDATA[最近温习数据结构与算法，在学习字符串的匹配算法 BM 算法、KMP 算法时，看到了两篇关于 KMP 算法讲解的文章，写的很经典。 阮一峰写的 字符串匹配的KMP算法 从头到尾彻底理解KMP BF 算法、KMP 算法匹配过程如下： 主串 S：“BBC ABCDAB ABCDABCDABDE” 模式串 P：”ABCDABD” BM 算法很简单，就是暴力匹配，以主串 S 为基准，模式串 P 从主串 S 的串头单步向后移动匹配到主串 S 的串尾。 123456789101112131415161718192021222324252627step 1.BBC ABCDAB ABCDABCDABDEABCDABDstep 2.BBC ABCDAB ABCDABCDABDE ABCDABD 单步移动step 3.BBC ABCDAB ABCDABCDABDE ABCDABD 单步移动step 4.BBC ABCDAB ABCDABCDABDE ABCDABD 单步移动 ... 单步移动step 11.BBC ABCDAB ABCDABCDABDE ABCDABD 单步移动 ... 单步移动 step 16. （经过16个回合匹配成功，模式串 P 相对于主串 S 向后移动了15个位置）BBC ABCDAB ABCDABCDABDE ABCDABD KMP 算法相比 BM 算法高效的原因在于，当模式串 P 的某个字符与主串 S 的字符不匹配时，模式串不是单步向后移动，而是向后跳跃几个位置再与主串 S 匹配。至于跳跃移动的步数，就是 KMP 算法的核心，即求 next。 12345求取 next 的公式：移动位数 next = 已匹配的字符数 - 对应的部分匹配值对应的部分匹配值的计算公式：对应的部分匹配值 = 已匹配的字符数构成的字符串的前缀和后缀的共同字符串的最大长度 123456789101112131415161718192021222324252627282930313233343536step 1.BBC ABCDAB ABCDABCDABDEABCDABD next = 0 - 0 = 0，匹配0个字符，ABCDAB 前缀后缀共同的最长元素 空 的长度为 0step 2.（向后移动：1）BBC ABCDAB ABCDABCDABDE ABCDABD next = 0 - 0 = 0，匹配0个字符，ABCDAB 前缀后缀共同的最长元素 空 的长度为 0step 3.（向后移动：1）BBC ABCDAB ABCDABCDABDE ABCDABD next = 0 - 0 = 0，匹配0个字符，ABCDAB 前缀后缀共同的最长元素 空 的长度为 0 step 4.（向后移动：1）BBC ABCDAB ABCDABCDABDE ABCDABD next = 0 - 0 = 0，匹配0个字符，ABCDAB 前缀后缀共同的最长元素 空 的长度为 0step 5.（向后移动：1）BBC ABCDAB ABCDABCDABDE ABCDABD next = 6 - 2 = 4，匹配6个字符，ABCDAB 前缀后缀共同的最长元素 AB 的长度为 2 step 6.（向后移动：4）BBC ABCDAB ABCDABCDABDE ABCDABD next = 2 - 0 = 2，匹配2个字符，AB 前缀后缀共同的最长元素 空 的长度为 0step 7.（向后移动：2）BBC ABCDAB ABCDABCDABDE ABCDABD next = 0 - 0 = 0，匹配0个字符，ABCDAB 前缀后缀共同的最长元素 空 的长度为 0step 8.（向后移动：1）BBC ABCDAB ABCDABCDABDE ABCDABD next = 6 - 2 = 4，匹配6个字符，ABCDAB 前缀后缀共同的最长元素 AB 的长度为 2 step 9.（向后移动：4。 经过9个回合匹配成功，模式串 P 相对于主串 S 向后移动了15个位置）BBC ABCDAB ABCDABCDABDE ABCDABD]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[音视频开发技能列表]]></title>
    <url>%2F2019%2F04%2F10%2Faudio-and-video-development-skills-list%2F</url>
    <content type="text"><![CDATA[流媒体协议 RTMP带你吃透RTMP直播推流实现RTMP协议的一些注意事项 HLSRTSPLive555 源码解析 (2) - RTSP 协议概述 SDPSDP 协议分析SDPSDP协议解析 RTP、RTCPRTP协议分析RTP/RTCP协议解析WebRTC的RTP、RTCP协议实现分析 传输层协议 TCP、UDP极客时间的专栏 趣谈网络协议 QUIC科普：QUIC协议原理分析QUIC协议初探-iOS实践让互联网更快的协议，QUIC在腾讯的实践及性能优化 文件格式 FLVH.264标准（二）FLV封装格式详解将h.264视频流封装成flv格式文件（一.flv格式）将h.264视频流封装成flv格式文件（二.开始动手）FFmpeg从入门到出家（FLV文件结构解析） 编解码 H.264/AVC深入浅出理解视频编码H.264结构H.264流媒体协议格式中的Annex B格式和AVCC格式深度解析码流格式: Annex-B, AVCC(H.264)与HVCC(H.265), extradata详解 H.265/HEVCFFmpeg从入门到出家（HEVC在RTMP中的扩展）基于iOS11的HEVC(H.265)硬编码/硬解码功能开发指南Web端H.265播放器研发解密 AACSEIFFmpeg从入门到精通——进阶篇，SEI那些事儿 开源工具 FFmpegFFmpeg FFmpeg-iOS-build-script FFmpeg从入门到出家（FFmpeg简析） fdk-aacfdk-aac fdk-aac-iOS-build-script x264x264 x264-iOS-build-script WebRTC一张图解释WebRTC底层协议结构实际中的WebRTC：STUN，TURN以及信令（一）RTC 专栏webrtchacksWebRTC Native 源码导读 LIVE555LIVE555 live555源代码分析live555 源代码简单分析1：主程序live555 分析—— OpenRtsplive555学习笔记－RTSP服务运作live555学习笔记－RTP打包与发送live555学习笔记－RTSPClient分析 SRSSRS NGINX-RTMPnginx-rtmp-module OpenGL 基础概念20分钟让你了解OpenGL ——OpenGL全流程详细解读一篇通俗易懂的讲解OpenGL ES的文章OpenGL ES入门详解iOS OpenGL 纹理坐标详解如何配置OpenGL ES的上下文（OpenGL ES2.0官方文档翻译）（译）OpenGL ES2.0 – Iphone开发指引（译）OpenGLES2.0 Iphone开发指引：第二部分，纹理贴图 基础应用OpenGL ES 2.0 iOS教程 iOS — OpenGLES之初步认识iOS — OpenGLES之着色器(shader)语法介绍iOS — OpenGLES之着色器(shader)的编译、链接及使用iOS — OpenGLES之简单的图形绘制iOS — OpenGLES之顶点缓存对象VBO[译] — OpenGL ES 2.0 for iPhone Tutorial Part 2: TexturesiOS — OpenGLES之图片纹理 西蒙iPhone-OpenGL ES 中文教程专题 从0打造一个GPUImage(1)从0打造一个GPUImage(2)从0打造一个GPUImage(3)从0打造一个GPUImage(4)从0打造一个GPUImage(5)从0打造一个GPUImage(6)如何调试shader 特效“抖音”式的酷炫短视频开发进阶在 iOS 中使用 GLSL 实现抖音特效 视频教程OpenGL ES 音视频测试 关于音视频测试的一点建议 优化策略 音视频开发中常见基础问题总结直播首屏耗时400ms以下的优化实践弱网优化、网络抖动、网络延时，这些问题，怎么处理？直播技术总结（三）ijkplayer的一些问题优化记录多媒体那些事-播放器漫谈（一）多媒体那些事-播放器漫谈（二）手机游戏直播 ： 悟空TV客户端设计与技术难点从直播CDN的原理说起，谈如何解决延时和连麦的老难题视频直播时的QoS策略超百万观众同场看直播流畅不卡顿，快手如何做到的？|首次披露 架构设计与应用 iOS流媒体直播整个框架介绍(HLS、RTSP)短视频客户端SDK设计与实现 博客 简书CSDN 公众号 LiveVideoStack观止云野狗何俊林编风网]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 SRS 在 VPS 上搭建流媒体服务器]]></title>
    <url>%2F2019%2F04%2F02%2Fuse-srs-establish-server-on-vps%2F</url>
    <content type="text"><![CDATA[闲话少说，直接步入正题。 一、登陆远程服务器我使用的是 MacBook Pro 笔记本，在 MacBook Pro 笔记本上的终端输入命令 ssh root@IP地址 -p ssh端口 1ssh root@111.11.111.111 -p 1111 其中，IP 地址和 SSH 端口换成你自己的 VPS 信息。 二、下载 SRS 源码在终端执行命令下载 SRS 源码 1git clone https://github.com/ossrs/srs 三、编译 SRS进入 srs/trunk 目录 1cd ./srs/trunk 在终端执行下面的命令编译 SRS 1./configure &amp;&amp; make 四、修改配置文件我没有对 ./conf/srs.conf 文件进行修改，使用的是默认配置。使用默认的配置，可跳过此步，直接进入下一步 “五、启动 SRS” 的操作。如果想改 vhost 或其它，就执行下面的操作。 1vi ./conf/srs.conf srs.conf 配置文件的默认参数为 12345678910111213141516171819202122# main config for srs.# @see full.conf for detail config.listen 1935;max_connections 1000;srs_log_tank file;srs_log_file ./objs/srs.log;http_api &#123; enabled on; listen 1985;&#125;http_server &#123; enabled on; listen 8080; dir ./objs/nginx/html;&#125;stats &#123; network 0; disk sda sdb xvda xvdb;&#125;vhost __defaultVhost__ &#123;&#125; 五、启动 SRS启动 SRS 服务 1./objs/srs -c conf/srs.conf 六、下载 FFmpeg下载 FFmpeg 1git clone https://git.ffmpeg.org/ffmpeg.git ffmpeg 七、编译 FFmpeg进入目录 ffmpeg 1cd ffmpeg 编译 FFmpeg 1./configure &amp;&amp; make 如果出现错误：nasm/yasm not found or too old. Use –disable-x86asm for a crippled build.，就需要安装 yasm，并再次执行 ./configure &amp;&amp; make 命令。如果未出错，跳过下面的操作直接进行下一步：八、FFmpeg 推流 的操作。 从 yasm 的版本列表 http://www.tortall.net/projects/yasm/releases 中找到合适的平台版本，通过以下命令安装 yasm 1wget http://www.tortall.net/projects/yasm/releases/yasm-1.3.0.tar.gz 1tar zxvf yasm-1.3.0.tar.gz 1cd yasm-1.3.0 1./configure 1make &amp;&amp; make install 再次进入目录 ffmpeg 执行命令 1./configure &amp;&amp; make 八、FFmpeg 推流使用 scp 命令上传文件到远程服务器上。scp 的使用见我的文章 使用 live555 在 VPS 上搭建流媒体服务器 1./ffmpeg -re -i ../ForBiggerFun.mp4 -f flv -y rtmp://111.11.111.111/live/teststreaming 其中，ForBiggerFun.mp4 视频文件在目录 ffmpeg 的上一层目录中。 九、播放视频流使用 VLC 或者 ffplay 播放 rtmp 视频流：rtmp://111.11.111.111/live/teststreaming 参考文献https://github.com/ossrs/srs/wiki]]></content>
      <tags>
        <tag>音视频处理</tag>
        <tag>服务器端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 live555 在 VPS 上搭建流媒体服务器]]></title>
    <url>%2F2019%2F03%2F30%2Fuse-live555-establish-server-on-vps%2F</url>
    <content type="text"><![CDATA[在上一篇文章 使用搬瓦工搭建 ShadowSocks 翻墙（VPN） 中提到了我购买了 VPS 服务资源，所谓物尽其用，仅让其充当 VPN 服务的工具有点浪费，可以在上面跑一些其它服务，比如流媒体直播服务、流媒体转码服务等。关于主机购买，也可以购买国内供应商的提供的主机服务，像阿里云、腾讯云、百度云等，可以关注他们的一些促销活动，在优惠力度大的时候囤机。 我的 VPS 服务器的操作系统为：1Operating system: Centos 7 x86_64 bbr 一、登陆 VPS 服务器我使用的是 MacBook Pro 笔记本，在 MacBook Pro 笔记本上的终端输入命令 ssh root@IP地址 -p ssh端口，其中 IP 地址和 SSH 端口换成你自己的 VPS 信息 1suntongmiandeMacBook-Pro:~ suntongmian$ ssh root@111.11.111.111 -p 1111 执行命令后的结果如下： 1234suntongmiandeMacBook-Pro:~ suntongmian$ suntongmiandeMacBook-Pro:~ suntongmian$ ssh root@111.11.111.111 -p 1111root@111.11.111.111's password: [root@host ~]# 二、安装 gcc 编译器给 VPS 主机安装 gcc 编译器 1[root@host ~]# yum install gcc-c++ 三、下载 live55 源码在主机根目录创建目录 streamingserver 123[root@host ~]# [root@host ~]# mkdir streamingserver[root@host ~]# 进入目录 streamingserver 123[root@host ~]# [root@host ~]# cd streamingserver[root@host streamingserver]# 从 live555 官网下载 live555 源码在终端执行命令下载或者直接在官网下载，以在终端执行命令下载为例子 1[root@host streamingserver]# curl -O http://www.live555.com/live.2019.03.06.tar.gz 四、解压在终端执行文件的解压缩命令 1[root@host streamingserver]# tar -xvf live.2019.03.06.tar.gz 使用 ls 命令查看当前目录下的文件 1234[root@host streamingserver]# [root@host streamingserver]# lslive live.2019.03.06.tar.gz[root@host streamingserver]# 五、编译第一步：进入 live55 源码目录 live 1[root@host streamingserver]# cd live 第二步：执行脚本 1[root@host live]# ./genMakefiles linux-64bit 因为我的系统是 Centos, 64bit，执行的是上面的脚本。如果系统是 Mac 系统，那么就要执行脚本 ./genMakefiles macosx 第三步：执行编译 1[root@host live]# make 六、启动流媒体服务第一步：进入 mediaServer 目录 123[root@host live]# [root@host live]# cd mediaServer[root@host mediaServer]# 第二步：启动流媒体服务 12[root@host mediaServer]# [root@host mediaServer]# ./live555MediaServer 命令执行后的结果如下： 1234567891011121314151617181920212223242526[root@host mediaServer]# [root@host mediaServer]# ./live555MediaServerLIVE555 Media Server version 0.96 (LIVE555 Streaming Media library version 2019.03.06).Play streams from this server using the URL rtsp://111.11.111.111/&lt;filename&gt;where &lt;filename&gt; is a file present in the current directory.Each file's type is inferred from its name suffix: ".264" =&gt; a H.264 Video Elementary Stream file ".265" =&gt; a H.265 Video Elementary Stream file ".aac" =&gt; an AAC Audio (ADTS format) file ".ac3" =&gt; an AC-3 Audio file ".amr" =&gt; an AMR Audio file ".dv" =&gt; a DV Video file ".m4e" =&gt; a MPEG-4 Video Elementary Stream file ".mkv" =&gt; a Matroska audio+video+(optional)subtitles file ".mp3" =&gt; a MPEG-1 or 2 Audio file ".mpg" =&gt; a MPEG-1 or 2 Program Stream (audio+video) file ".ogg" or ".ogv" or ".opus" =&gt; an Ogg audio and/or video file ".ts" =&gt; a MPEG Transport Stream file (a ".tsx" index file - if present - provides server 'trick play' support) ".vob" =&gt; a VOB (MPEG-2 video with AC-3 audio) file ".wav" =&gt; a WAV Audio file ".webm" =&gt; a WebM audio(Vorbis)+video(VP8) fileSee http://www.live555.com/mediaServer/ for additional documentation.(We use port 80 for optional RTSP-over-HTTP tunneling, or for HTTP live streaming (for indexed Transport Stream files only).) 从上面显示的信息可看出，live555 默认支持 .264, .265, .aac, ac3, .amr, .dv, .m4e, .mkv, .mp3, .mpg, .ogg, .ts, .vob, .wav, .webm 视频格式。 七、本地文件上传到服务器第一步：获取 .mkv 格式的视频文件 如果在网络上找不到合适的 .mkv 格式的视频文件，那么可以使用 ffmpeg 转码获取。ffmpeg 的安装，可参看我的文章 在Mac OSX上安装ffmpeg &amp;&amp; ffmpeg命令行将h264封装为mp4 在 MacBook Pro 笔记本的终端输入 ffmpeg 转码命令 1suntongmiandeMacBook-Pro:myTestVideo suntongmian$ ffmpeg -i SampleVideo_1280x720_10mb.mp4 -vcodec copy -acodec copy output.mkv 执行结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657suntongmiandeMacBook-Pro:myTestVideo suntongmian$ suntongmiandeMacBook-Pro:myTestVideo suntongmian$ lsSampleVideo_1280x720_10mb.mp4suntongmiandeMacBook-Pro:myTestVideo suntongmian$ suntongmiandeMacBook-Pro:myTestVideo suntongmian$ ffmpeg -i SampleVideo_1280x720_10mb.mp4 -vcodec copy -acodec copy output.mkv ffmpeg version 4.1.2 Copyright (c) 2000-2019 the FFmpeg developers built with Apple LLVM version 10.0.0 (clang-1000.11.45.5) configuration: --prefix=/usr/local/Cellar/ffmpeg/4.1.2 --enable-shared --enable-pthreads --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags='-I/Library/Java/JavaVirtualMachines/openjdk-11.0.2.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/openjdk-11.0.2.jdk/Contents/Home/include/darwin' --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libbluray --enable-libmp3lame --enable-libopus --enable-librubberband --enable-libsnappy --enable-libtesseract --enable-libtheora --enable-libvorbis --enable-libvpx --enable-libx264 --enable-libx265 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librtmp --enable-libspeex --enable-videotoolbox --disable-libjack --disable-indev=jack --enable-libaom --enable-libsoxr libavutil 56. 22.100 / 56. 22.100 libavcodec 58. 35.100 / 58. 35.100 libavformat 58. 20.100 / 58. 20.100 libavdevice 58. 5.100 / 58. 5.100 libavfilter 7. 40.101 / 7. 40.101 libavresample 4. 0. 0 / 4. 0. 0 libswscale 5. 3.100 / 5. 3.100 libswresample 3. 3.100 / 3. 3.100 libpostproc 55. 3.100 / 55. 3.100Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'SampleVideo_1280x720_10mb.mp4': Metadata: major_brand : isom minor_version : 512 compatible_brands: isomiso2avc1mp41 creation_time : 1970-01-01T00:00:00.000000Z encoder : Lavf53.24.2 Duration: 00:01:02.32, start: 0.000000, bitrate: 1347 kb/s Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 959 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default) Metadata: creation_time : 1970-01-01T00:00:00.000000Z handler_name : VideoHandler Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, 5.1, fltp, 383 kb/s (default) Metadata: creation_time : 1970-01-01T00:00:00.000000Z handler_name : SoundHandlerOutput #0, matroska, to 'output.mkv': Metadata: major_brand : isom minor_version : 512 compatible_brands: isomiso2avc1mp41 encoder : Lavf58.20.100 Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 959 kb/s, 25 fps, 25 tbr, 1k tbn, 12800 tbc (default) Metadata: creation_time : 1970-01-01T00:00:00.000000Z handler_name : VideoHandler Stream #0:1(und): Audio: aac (LC) ([255][0][0][0] / 0x00FF), 48000 Hz, 5.1, fltp, 383 kb/s (default) Metadata: creation_time : 1970-01-01T00:00:00.000000Z handler_name : SoundHandlerStream mapping: Stream #0:0 -&gt; #0:0 (copy) Stream #0:1 -&gt; #0:1 (copy)Press [q] to stop, [?] for helpframe= 1557 fps=0.0 q=-1.0 Lsize= 10249kB time=00:01:02.29 bitrate=1347.9kbits/s speed= 973x video:7298kB audio:2919kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.316002%suntongmiandeMacBook-Pro:myTestVideo suntongmian$ suntongmiandeMacBook-Pro:myTestVideo suntongmian$ lsSampleVideo_1280x720_10mb.mp4 output.mkvsuntongmiandeMacBook-Pro:myTestVideo suntongmian$ 第二步：上传本地文件到远端服务器 我使用的是 MacBook Pro 笔记本，在 MacBook Pro 笔记本上的终端上执行下面的命令，将视频文件上传到 VPS 服务器上。 1suntongmiandeMacBook-Pro:myTestVideo suntongmian$ scp -P 1111 output.mkv root@111.11.111.111:/root/streamingserver/live/mediaServer 命令执行后的结果如下： 12345suntongmiandeMacBook-Pro:myTestVideo suntongmian$ suntongmiandeMacBook-Pro:myTestVideo suntongmian$ scp -P 1111 output.mkv root@111.11.111.111:/root/streamingserver/live/mediaServerroot@111.11.111.111's password: output.mkv 100% 30MB 511.1KB/s 01:00 suntongmiandeMacBook-Pro:myTestVideo suntongmian$ 其中，myTestVideo 是我的 MacBook Pro 笔记本上存放测试视频的文件夹。/root/streamingserver/live/mediaServer 为 VPS 主机的目录层级。 关于 scp 命令的使用，可参看文章 Linux SSH远程文件/目录传输命令scp 第二步：检测文件是否上传成功 在 VPS 主机上执行命令 ls 查看相应的目录下是否存在文件 output.mkv。若存在 output.mkv，就说明上面的第一步操作成功了。 1234567[root@host mediaServer]# [root@host mediaServer]# lsCOPYING DynamicRTSPServer.o Makefile version.hhCOPYING.LESSER live555MediaServer Makefile.headDynamicRTSPServer.cpp live555MediaServer.cpp Makefile.tailDynamicRTSPServer.hh live555MediaServer.o output.mkv[root@host mediaServer]# 八、重新启动流媒体服务第一步：使用 ctrl+c 终止之前启动的流媒体服务 执行结果如下： 12^C[root@host mediaServer]# 第二步：重新启动流媒体服务 12[root@host mediaServer]# [root@host mediaServer]# ./live555MediaServer 九、测试视频流第一步：安装 VLC 视频播放器。我使用的是 MacBook Pro 笔记本，在笔记本上下载并安装了 VLC 视频播放器。 第二步：播放视频流。在 VLC 中打开视频流地址 rtsp://111.11.111.111/output.mkv，即可看到 live555 服务器推送出来的视频流。]]></content>
      <tags>
        <tag>音视频处理</tag>
        <tag>服务器端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用搬瓦工搭建 ShadowSocks 翻墙（VPN）]]></title>
    <url>%2F2019%2F03%2F29%2Festablish-vpn-server%2F</url>
    <content type="text"><![CDATA[为了查询资料的便利性，大部分时候需要使用 Google 浏览器，但是国内除了高校能默认支持访问 Google 的服务外，基本所有人想使用 Google 的服务都需要借助 虚拟专用网 VPN (Virtual Private Network) ) 工具。以前使用过一些 VPN 服务提供商提供的服务，出现过用了一段时间就使用不了了、服务不稳定、连接后访问网络资源慢，还有的干脆就无法使用。比如我花了 79 美金买过 NordVPN 的服务，网上的口碑和排名很靠前的。事实上真是花钱买烦恼，99%的概率连不上 NordVPN 服务，即使碰运气连上去了，但网速差的让人吐血。为这个事，与 NordVPN 的技术支持来往过好多封英文邮件，最终还是没有解决连不上服务器的问题。在国内想使用 Google 服务查询资料真是痛苦。搞笑的是，2018年12月初去韩国济州岛玩，在济州岛使用 NordVPN 服务倒是 99%以上概率连上服务器，网速也还可以。泪崩，在 NordVPN 上花的钱彻底打水漂了。好在工作所在公司提供了 VPN 服务，就一直使用到现在。 为能正常使用 Google 的资源以及考虑到数据访问的私密性，就开始考虑搭建一个私人的 VPN 服务。查询了一些资料，找到了 搬瓦工 (BandwagonHost) 。以下是搭建的流程： 一、购买主机服务 VPS参考该文的购买流程搬瓦工(BandwagonHost) 一键搭建ShadowSocks翻墙教程 二、登陆 VPS参考该文的登陆搬瓦工的 VPS 的流程Windows/Mac/Linux如何SSH远程连接/登陆搬瓦工 我使用的 MacBook Pro，以下为 Mac 系统上的 VPS 登陆流程： 在终端输入命令 ssh root@IP地址 -p ssh端口，其中 IP 地址和 SSH 端口换成你自己的 VPS 信息 1ssh root@111.11.111.111 -p 1111 三、安装 SSR 脚本参考该文的 Shadowsocks 安装流程搬瓦工(BandwagonHost)取消了一键安装Shadowsocks后，最新搬瓦工手动安装SS教程！ 第一步：等到出现 root@host ~ 字样，执行命令 1wget --no-check-certificate -O shadowsocks-all.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh 注意 (warning) : 国外服务器运行脚本时容易出错，如出现错误提示 bash: wget: command not found，可以请在先执行 yum -y install wget 命令。成功后，再执行上面的命令。如果没有出现提示错误，请略过。 第二步：等待上一步的命令执行结束后，继续执行命令 1chmod +x shadowsocks-all.sh 第三步：等待上一步的命令执行结束后，继续执行命令 1./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log 第四步：根据需要选择，不懂的话直接选1，或者默认回车。下面会提示你输入你的 SS SERVER 的密码和端口。不输入就是默认。跑完命令后会出来你的 SS 客户端的信息。 123456789101112131415161718Which Shadowsocks server you’d select:1) Shadowsocks-Python2) ShadowsocksR3) Shadowsocks-Go4) Shadowsocks-libevPlease enter a number (Default Shadowsocks-Python):1You choose = Shadowsocks-PythonPlease enter password for Shadowsocks-Python(Default password: teddysun.com):123456password = 123456Please enter a port for Shadowsocks-Python [1-65535](Default port: 11260):11260port = 11260 第五步：特别注意，由于 iPhone 端的的 wingy 目前只支持到 cfb，所以我们选择 aes-256-cfb，即7，回车。 第六步：当我们看到 Congratulations, Shadowsocks-Python server install completed! 时，则证明我们已经成功安装了 SS。请立即将这些信息复制下来加以保存，我们就会用到这几个比较重要的信息：主机服务器IP地址、端口号、密码和加密方式。上面的命令全部回车执行后，如果没有报错，即为执行成功，出现确认提示的时候，输入 y 后，回车即可。 这样的话我们就在搬瓦工 VPS 主机上完成了 SS 的手动安装，记录保存好你的上述信息：Server IP、Server Port、Password、Encryption Method，我们就可以在不同的设备终端找到相应的 SS 进行安装设置使用了。 1234567891011121314151617INFO: loading config from /etc/shadowsocks-python/config.json2019-03-28 05:14:24 INFO loading libcrypto from libcrypto.so.10Starting Shadowsocks successCongratulations, Shadowsocks-Python server install completed!Your Server IP : 111.11.111.111Your Server Port : 11260Your Password : 123456Your Encryption Method: aes-256-cfbYour QR Code: (For Shadowsocks Windows, OSX, Android and iOS clients)ss://xxxxxxxxxxxxxxxxxYour QR Code has been saved as a PNG file path:/root/shadowsocks_python_qr.pngWelcome to visit: https://teddysun.com/486.htmlEnjoy it! 四、使用 Shadowsocks 终端体验 VPN 服务根据设备类型，下载对应的平台软件，并设置好参数就可以畅享 VPN 服务了。 Windows：Github链接地址 Mac：Github链接地址 Android：Github链接地址 iPhone：Kite Ass Proxy：APP Store链接地址、FirstWingy：APP Store链接地址、SuperWingy：APP Store链接地址 我要在 Mac 和 iPhone 上使用，我下载了上面的 Mac 版本，但是没有下载上面的 iPhone 版本，iPhone 版本我用的是 Sockswitch]]></content>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac 上安装 Spark]]></title>
    <url>%2F2018%2F03%2F18%2Finstall-spark-in-mac%2F</url>
    <content type="text"><![CDATA[1. 安装 JDK1.1 检查 JDK 版本检查 Mac 上是否已经安装了 jdk，在终端上执行命令 java -version Spark 是使用 scala 语言开发的，如果要使用 Scala，就需要安装与之对应的 jdk 版本，Scala 的下载地址：http://www.scala-lang.org/download/。下载地址页面明确说明了要使用 Scala 就必须要求 java version 为 1.8 以上，也就是 Java 8 JDK 以上版本。 1.2 安装 JDK如果 Mac 上没有安装 jdk，jdk 的下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html，选择 Mac 版本的 jdk 下载，下载后安装。 安装完成后，在终端执行命令如下命令查看是否成功安装 java -version 2. 安装 Spark2.1 下载 SparkSpark 压缩包的下载地址为：http://spark.apache.org/downloads.html，选择合适的版本安装。我选择的是 spark-2.3.0-bin-hadoop2.7.tgz 版本。 2.2 解压 Spark将 spark-2.3.0-bin-hadoop2.7.tgz 解压到你的某个文件目录下。我的文件目录结构是 2.3 运行 Spark进入 Spark 所在的目录下，执行 Scala 脚本启动 Spark 根据终端的显示结果，在 Chrome 浏览器上访问 http://172.20.10.4:4040，会出现 Spark 的 web 页面，可以点击 Jobs，Stages，Storage，Environment，Executors 等菜单项看看 Spark 都运行了哪些服务，更加直观地了解 Spark 是个什么样的东西。 启动了 Spark，也在浏览器上看到了其可视化界面，接下来就来个例子试试 Spark。 在 spark-2.3.0-bin-hadoop2.7 所在的目录下，新建一个 testfile 目录，testfile 目录下新建一个文件 helloSpark，并写上简单几行文字。 在 bin 目录下执行脚本 spark-shell，并使用 Scala 统计 helloSpark 文件中的函数、文件中第一行的内容 helloSpark 文件和 spark-shell 脚本所在的目录如下图 注意：可能有些朋友不清楚 val lines = sc.textFile(&quot;../../testfile/helloSpark&quot;) 中 ../ 的意思，../ 表示上一层目录，../../ 表示上上层目录。 到此，Spark 的简单试用结束了。想了解 sc.textFile，大家自行 google 或 baidu。 对于初学者来说，有相应的视频配合的话，会对 Spark 的安装有更新直观的视觉体验。给大家推荐下视频：Spark 从零开始-Spark 安装]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【译】调查：苹果 HEVC 采用的影响]]></title>
    <url>%2F2018%2F02%2F26%2Ftranslate-Survey-The-Impact-of-Apples-HEVC-Adoption%2F</url>
    <content type="text"><![CDATA[英语原文：Survey: The Impact of Apple’s HEVC Adoption 调查：苹果 HEVC 采用的影响新的 Unisphere 报告显示OTT服务和其他视频发布商正在向HEVC转移，但同样对 AV1 感兴趣作者：Streaming Media 编辑人员发布时间：2018年1月23日 您是否想知道苹果决定在 HTTP Live Streaming（HLS）中添加 HEVC 对流发布社区意图部署 HEVC 有多大影响？最近一项对 Streaming Media 读者的调查显示，66.2％的受访者表示，苹果的举动产生了重大影响，到2018年年底，高达62.5％的人可能会向 HLS 中添加 HEVC。这些只是 Harmonic 公司和 Unisphere 研究机构今天联合发布的许多调查结果中的两个。这份名为“苹果 HEVC 采用的影响：基于调查的报告”的报告包括来自600多 Streaming Media 读者的回应。 这份可供下载的8页报告也显示了制片人对开放媒体联盟（AOM）AV1 编解码器的兴趣，66％以上的受访者表现出浓厚的兴趣。虽然目前超过25％的受访者分发使用 HEVC 编码的视频，但74％的受访者表示“已知或未知的内容使用费”是通过 HLS 部署 HEVC 的一个重要问题。 调查报告的作者，Streaming Media 特约编辑 Jan Ozer 说：“HEVC 推出五年多以来，总版税费用仍然未知，继续遏制了编解码器的采用。结果，在苹果加入 AOM 之前有33％的受访者计划在2018年及以后增加AV1，苹果加入 AOM，这只会增加受访者的兴趣。额外的27.3％对 VP9 表示相同的观点，显然，最好的事情是 HEVC IP 社区可以采取行动来促进 HEVC 的采用，从而降低版税结构和成本。” HEVC 的采用也受到其他几个因素的影响，如与 HEVC 编码相关的额外编码成本以及有关如何解决传统 HLS 终端的混淆。具体来说，当被问及他们如何处理传统设备时，48.3％回答他们不知道，表明在这一点上需要市场教育。那些避开 HEVC 的公司采用了许多其他技术来实现相同的好处，如内容感知编码和其他比特率缩减系统。 尽管对 AV1 的兴趣很浓厚，但采用并不是扣篮。受访者对比较质量，播放性能和编码时间表示了很大的担忧。令人意外的是，对潜在知识产权问题的关注并不是一个重大障碍。Ozer 评论说：“在 VP8 之前，专家和专利组织一直在使用开源编解码器来解决潜在的 IP 问题。在开放媒体联盟中，亚马逊，思科，谷歌，英特尔，微软，Mozilla，Netflix 以及现在的苹果都认为：出版界认为 IP 挑战的威胁并不真实。” 这项调查还测量了其他编解码器的兴趣，并提到了 V-NOVA PERSEUS，Divideon xvc 和 RealNetworks RealMedia HD，尽管53.8％的受访者当前没有计划添加任何编解码器。对高动态范围（HDR）视频的兴趣似乎在迅速增长；虽然只有9.8％的受访者在2017年底发布 HDR，但计划在2018年底之前发布 HDR 的比例为50.6％，49.4％的用户没有超出标准动态范围的计划。 “苹果HEVC采用的影响：基于调查的报告”，现在可以下载。]]></content>
      <tags>
        <tag>译文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【译】人工智能和机器学习将视频质量推向新的高度]]></title>
    <url>%2F2018%2F02%2F25%2Ftranslate-AI-and-Machine-Learning-Push-Video-Quality-to-New-Heights%2F</url>
    <content type="text"><![CDATA[英语原文：AI and Machine Learning Push Video Quality to New Heights 人工智能和机器学习将视频质量推向新的高度人工智能和机器学习以及深度学习和神经网络正在解决编码质量到隐藏字幕的 OTT（Over To Top）挑战。作者：Ankur Patel发布时间：2018-02-15 [对人工智能和机器学习如何彻底改变视频感兴趣？加入我们2月27日在伦敦的流媒体论坛，我们将介绍亚马逊，IBM等专注于人工智能和机器学习的演讲。] 自从1928年第一次播出电视节目以来，视频技术已经从模拟标准清晰度（SD）黑白电视到 OTT 数字高清（HD）流式传输到数百个的连接设备。根据思科最新的可视网络指数，到2021年视频流量将占所有互联网流量的82%，高于2016年的73%。而且，思科首席执行官 Chuck Robbins 预测，到2020年每小时将有100万台设备添加到网络中。 OTT 视频流最大的挑战是提供尽可能高的体验质量（QoE）和服务质量（QoS）。 根据马萨诸塞大学阿默斯特分校的 Ramesh K. Sitarman 教授发表的一篇论文，观众在2秒延迟后开始放弃一个视频，其后每秒钟有6％消失。缓冲和像素化会导致数字广告带来负面的用户体验和收入损失。通过根据需要切换比特率并通过带宽波动，自适应比特率（ABR）流已经被采用来保证最小化缓冲。 ABR 的概念解决了 OTT 流媒体的部分挑战。但是，考虑到移动用户的位置和连接性的动态变化，ABR不能完全消除移动手持设备上的重新缓冲和像素化，需要重新缓冲过去的事物。另外，诸如快进和快退等其他复杂性通常会导致播放停顿，从而产生负面的用户体验。 这些挑战的答案隐藏在人工智能（AI）和机器学习的新技术概念中。麻省理工学院的计算机科学和人工智能实验室（CSAIL）开发了 Pensieve 神经网络，这是一种人工智能（AI）系统，它使用机器学习来选择不同的现有算法，例如基于速率的算法，取决于网络条件的基于缓冲区的算法。 Pensieve 神经网络提前预测连接问题，并预测性地调整流分辨率，为无缓冲用户体验创建足够的回放缓冲区。实际上，这种方法不会完全消除缓冲，但它可以帮助减少缓冲，让我们更接近无缓冲视频流。使用 Pensieve 神经网络进行的现场实验减少了30％的重新缓冲，并将主要 QoE 矩阵增加了25％。然而，随着更全面的数据可用于训练 Pensieve 神经网络，总会有进一步改进的空间。 视频流也可以受益于机器学习技术的进步。 YouTube 和 Netflix 采用机器学习动态优化编码参数。这不仅增加了用户的 QoE 和 QoS，而且还减少了相同质量所需的比特数。使用机器学习的编码优化还可以帮助优化带宽使用率较低的成本。它还将减少以前用于手动优化的工程资源成本。就 YouTube 来说，神经网络（NN）用于动态预测视频编码量化等级（QL），该等级可以产生目标比特率，并且在单程中实现双通编码的性能。因此，它也将减少整体视频延迟和编码成本。 从手持移动设备到大屏幕电视的连接设备的可用性已经产生了许多挑战，因为不同的屏幕尺寸可以使感知视频质量产生巨大差异。静态编码模型不具有成本效益，因为它们不会将屏幕大小和场景复杂度计算在内。机器学习算法可用于基于视频的感知质量来实现“内容感知”编码。机器学习算法可以根据针对该特定屏幕尺寸的屏幕大小和目标感知质量来决定编码参数。例如，为了在两种不同的屏幕尺寸上实现相同的感知质量，一个屏幕所需的位数可能比另一个屏幕少得多。机器学习可以帮助我们即时执行此操作，从而减少带宽消耗并节省成本。 人工智能和机器学习可以为动态检测唇形同步和隐藏字幕（CC）文本同步问题提供有效的解决方案，否则需要主动眼球检测或使用侵入性方法，例如在基带视频中插入水印或指纹（SDI）和音频。由牛津大学计算机科学系使用名为 LipNet 的人工智能系统进行的实验可以识别出准确率为93.4％的单词，而与仅达到52.3％的人类专业人员相比，其准确性达到了93.4％。 Google DeepMind 项目进行的类似测试表明，AI 很容易胜过试图破译200个随机剪辑数据集的专业口头读者。 人工智能成功破译了所有单词的46.8％，而专业唇读者则破译了12.4％。市场上出现的产品使用AI和机器学习来检测唇形同步和 CC 文本同步问题。一种这样的产品是来至于 Multicoreware 公司的 Li​​pSync，它使用人工智能和深度学习来跟踪嘴唇的运动以测量视频-音频同步。 随着我们进入人工智能的世界，新的概念和理论正在出现，以优化内容生成，准备，交付，安全和演示。例如，深度神经网络的实施对 YouTube 视频推荐系统产生了巨大的积极影响。更为有希望的是基于人工智能和机器学习的下一代高度直观的网络，这将对 OTT 视频流产生巨大的积极影响，改变其采用和增长以及增强内容安全性。]]></content>
      <tags>
        <tag>译文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017年度总结]]></title>
    <url>%2F2017%2F12%2F31%2F2017-12-31-annual-summary%2F</url>
    <content type="text"><![CDATA[今天是2017年12月31日，2017年的最后一天。马上就2018年了，在2017年有什么收获呢。仔细想想，好像没有什么收获，也没有印象最深的事情。 工作中做出的一点成绩就是开发了 iOS 平台的短视频 SDK，服务了一批企业客户。从2017年5月到现在主要就是做短视频 SDK 的事情，为了赶进度，每天都过的很快，没什么感觉。2017年下半年开始带人做一些事情，算是走带团队的路线了，发现在带人上经验上还需要完善，这方面有些小收获。 生活中最大的变化就是自己想找个人结婚了。这一点改变对我而言，是个很大的变化，2017年上半年，我对结婚这件事都没怎么放在心上。但到了2017年10月后，不知怎么了就非常想结婚了。 2017年工作上没有什么大的提升和成就感，生活上也平平淡淡，这一年就这么过去了，有点可惜。2018年，就30岁了，房子装修、买车、结婚是最重要的事情了。2018年最最重要的事情就是领结婚证。 二〇一七年十二月三十一日，北京]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 gitment 制作 GitHub 博客的评论系统]]></title>
    <url>%2F2017%2F08%2F12%2Fuse-gitment-as-comment-system%2F</url>
    <content type="text"><![CDATA[多说评论系统在2017年6月1日关闭服务后，对于自建博客该使用哪款评论系统我犯愁了。我仔细对比了国内的一些评论系统，发现没有一款能比得上多说。虽然我之前的多说评论在导出时发生了数据错乱以至于数据无法导进新的评论系统，但是我从内心底对维护多说服务的技术人员表示认可。在知道多说即将关闭服务之后，我立即就选用了 Disqus。Disqus 真的很难用，一是需要翻墙，二是可视化界面做的太糟糕。不想在评论系统上大费周折，就将就用到了现在。终于忍无可忍，搜索了些资料发现了基于 GitHub issue 的 gitment 和 gitalk 两款评论插件。经过比较我选用了 gitment。 gitment 和 gitalk 这两款评论插件对于如何部署都没有说的很清晰。建议参考gitment readme 和 Add Gitment Support，即可部署成功。 本人博客基于 github托管平台 + hexo博客框架 + next博客主题 + gitment评论插件 资源构建。 二〇一七年八月十二日，北京]]></content>
      <tags>
        <tag>工作</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人生到处知何似，应似飞鸿踏雪泥]]></title>
    <url>%2F2017%2F07%2F04%2Frecalling-the-old-days%2F</url>
    <content type="text"><![CDATA[和子由渑池怀旧苏轼人生到处知何似，应似飞鸿踏雪泥。泥上偶然留指爪，鸿飞那复计东西。老僧已死成新塔，坏壁无由见旧题。往日崎岖还记否，路上人困蹇驴嘶。 每一场剧的谢幕，都将会有一场新剧情演绎。体会这过程，人生阶段的爱恨交加才能不偏不倚，急于实现内心的渴求，往往在达到目标之后深深地感悟到人生的那些阶段本不该如此，人生的每个阶段应该是快乐的、幸福的，简单的。虽有现实的，人性的因素，让内心波浪起伏，只能说经历太少，对人生和生活理解的并不透彻。人生历程的持续，人生观、世界观、价值观都在向前演绎，是变好了，还是变坏了，取决于生活的态度。社会的浮躁，人心的攀比，在这人与社会演进的长河中，是走在岸边看风景，还是被吸进河中陷于疲惫挣扎。人需要有欲望，需要尝试，才能感悟。千人千种经历千种感悟，不变的是最终都会回归到来到出生时最简单的世界。人生每十年是一个阶段，十岁前的自己，二十岁前的自己，三十岁前的自己，四十岁前的自己……想一想，社会教会了人识别复杂的事物，人也从复杂中的事物中明白了最简单的道理。人生不过百余年，有所得，有所失，患得患失才是罪过。往前一步，可以向后看，不能后悔但需反思。读懂内心，何处不是“面朝大海，春暖花开。” 二〇一七年七月四日，北京]]></content>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分析 H.265 + AAC 的 FLV 文件]]></title>
    <url>%2F2017%2F07%2F01%2Fanalyse-h265-flv%2F</url>
    <content type="text"><![CDATA[常见的 FLV 文件里封装的是 H.264 和 AAC 数据。对于 H.265(HEVC)，FLV 支不支持呢，答案是官方版本不支持。想用 FLV 封装 H.265 数据，那该怎么搞？首先，需要一套 H.265 的编解码器，其次，就是扩展 FLV 的头 header，其实是增加对 H.265 CodecID 的支持。 今年6月6日苹果开发者大会开放了 iOS 平台的 HEVC API，也就是开发者可以调用 iOS 系统的 API 进行 H.265 硬编码了，但是只能在 iOS 11.0 及以上版本使用。目前，iOS 11.0 正式版还未正式发布，需要等到今年的9月份。不建议开发者将自己的 iOS 设备刷到 iOS 11.0 beta 版，因为升级 beta 版后非常卡。经测试，苹果的 H.265 编码出来的图像质量还是可以的，但是消耗码率较高。 苹果开放 H.265 编解码 API 势必会影响到整个 H.265 行业的发展，但 H.265 离真正落地和普及还需要时间。明年（2018年）再看 H.265 对整个音视频行业的影响。 去年，我们团队就已经实现 H.265 直播推流和播放了，采用的是 H.265 软编码和软解码器，推流端使用 500K 码率就能推送 24 帧，720P 的视频流，设备的性能消耗在可接受的范围内，播放端画质相当清晰。 言归正传，分析下 H.265+AAC 的 FLV 文件。 关于 H.265 的 VPS、SPS、PPS 等知识点的解释，可参看文章：VPS SPS PPSHEVC编码结构：序列参数集SPS、图像参数集PPS、视频参数集VPSFFmpeg的HEVC解码器源代码简单分析：解析器（Parser）部分homerHEVC代码阅读（21）——基本流程HEVC概念缩写对照表 想了解关于 FLV 的更多细节请看文章rtmp直播推流（一）－－flv格式解析与封装]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习项目移植之使用 cmake 编译 iOS 平台库]]></title>
    <url>%2F2017%2F05%2F21%2Fuse-cmake-to-build-ios-lib%2F</url>
    <content type="text"><![CDATA[去年（2016年）做深度学习项目的移动端移植用到了 cmake，现在把当时写的一篇使用流程贴出来，主要目的是备忘。废话不多说，直接进入正题。 1. 下载 X11 并安装关于 Mac 版 X11，Mac 不再随附 X11，但 XQuartz 项目会提供 X11 服务器和客户端库。XQuartz 项目提供适用于 MacOS 的 X11 服务器和客户端库，网址是 https://www.xquartz.org。下载可用的最新版本并安装。具体说明见：关于 Mac 版 X11 2. 下载 cmake 的 dmg 格式并安装下载地址: https://cmake.org/download/，本文使用的是 Mac OSX 10.6 or later，cmake-3.7.1-Darwin-x86_64.dmg 版本。 3. 终端安装 cmake在终端执行命令： sudo &quot;/Applications/CMake.app/Contents/bin/cmake-gui&quot; --install 4. 进入包含 CMakeLists.txt 文件的源码文件夹 xxx 并执行命令得到 Mac 工程（注意：命令的空格）在终端依次执行下面的命令：（备注：xxx 为工程的名称） cd xxx mkdir build cd build cmake -G Xcode .. 5. 将 Mac 工程改为 iOS 工程(1) 进入 build 文件夹，打开 Project.xcodeproj 工程，Duplicate TARGET “xxx”，生成了1个新的 TARGET “xxx copy” （备注：xxx 为 TARGET 的名称，跟 章节 4 中的 xxx 是同名的，也就是说章节 3 中的 xxx 决定了此处 Xcode 项目的 TARGET 名称） (2) 将 TARGET “xxx copy” 改名为 “xxx-iOS” (3) 进入 TARGET “xxx-iOS” 的 Build Settings，将 Architectures 下的 Base SDK 改为 Latest iOS(iOS 10.2)，此时 Architectures，Supported Platforms，Valid Architectures 都会变成跟 iOS 平台相关的设置 (4) 修改 iOS Deployment Target 为想要支持的最低版本，此处设置支持的 iOS 最低版本为 iOS 7.0 (5) 在 Xcode 的左上角选择工程 xxx copy，接着进入 Manage Schemes，将 xxx copy 更名为 xxx-iOS 6. 编译 xxx-iOS 即可得到 iOS 静态库 libxxx-iOS.a7. 终端执行命令 lipo -info libxxx-iOS.a 查看 iOS 静态库支持的 Architectures （如：armv7，arm64，x86_64，i386）在终端执行命令： lipo -info libxxx-iOS.a 输出结果为 Architectures in the fat file: libxxx-iOS.a are: armv7 arm64 若要生成 x86_64，i386 模拟器架构，就需要在使用 Xcode 编译的时候编译相应的 x86_64，i386 库。]]></content>
      <tags>
        <tag>工作</tag>
        <tag>iOS</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017.5.20谈感受]]></title>
    <url>%2F2017%2F05%2F20%2F2017-5-20-idea%2F</url>
    <content type="text"><![CDATA[今天是5月20号，微信里各种撒狗粮、秀恩爱。这一切，都跟我没什么关系，该看资料看资料，该写代码写代码，该睡就睡。日子就是这么单调、有规律。 翻了翻博客里以前的文章，最近一年没怎么写技术性文章，写的都是些心情、感悟。工作忙是一个原因，生活中对未来的规划的不确定性也是一个导致自己静不下心来写技术性文章的原因。以前，还会看看文学方面的书，放松下。现在，书看的少了，写起东西来词穷，流水账的陈述，自己都看不顺眼了。文字这东西很奇妙，把内心的心情通过文字表述出来，能释放压力，缓解精神上的负担。当寂寞空虚冷的时候，就真正明白书和文字才是疗伤的良药。 写到这里，想起一句话，原文不记得了，意思大概是“当你没能力时，遇到想要呵护的人是那么地难，当你有能力时，那个对的人已经离你而去”。生活就是这样，得向前看，你只要往回看，伤心是难免的。 生活要积极向上，活得才有激情。没激情的生活，过久了就堕落了。我发现自己在精神上已经堕落了，生活中缺乏趣味，缺乏激情。2017年已经过去了一半，那就抓紧将来的时间，活得开心、快乐，才能有健康的精神面貌、世界观和人生观。 以上文字纯粹是流水账，想到什么写什么。用“该文言语简单、质朴、写实，有可取之处”来抚慰下自己词穷的囧态。好了，到了文章结束的时候了，嗯，那就结束吧。 二〇一七年五月二十日，北京]]></content>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[也无风雨也无晴]]></title>
    <url>%2F2017%2F05%2F14%2Fa-quiet-life%2F</url>
    <content type="text"><![CDATA[定风波/定风波·三月七日【作者】苏轼 【朝代】宋三月七日， 沙湖道中遇雨。 雨具先去， 同行皆狼狈， 余独不觉。 已而遂晴， 故作此。莫听穿林打叶声， 何妨吟啸且徐行。 竹杖芒鞋轻胜马， 谁怕？ 一蓑烟雨任平生。料峭春风吹酒醒， 微冷， 山头斜照却相迎。 回首向来潇瑟处， 归去， 也无风雨也无晴。 用苏轼老先生的一首词来表达我二零一七年上半年的心境。二零一七年上半年去了天津、上海、厦门、东莞、珠海，假装很开心，假装看起来不在乎，其实内心早已如凉水。人生中定的目标不应受环境和事物的影响而放弃，否则会活的很纠结。打开心结，释放内心，做一个洒脱的人，一个快乐的人。 二〇一七年五月十四日，北京]]></content>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 tensorflow 学习梵高画作]]></title>
    <url>%2F2017%2F01%2F19%2Ftensorflow-neural-style%2F</url>
    <content type="text"><![CDATA[配一台个人使用的深度学习机器大概会花费2万人民币，比如其中的核心部件英伟达 Titan X GPU 售价大概9000人民币左右，这个价格对于一般人还是略贵的。想深层次进入深度学习应用领域，花点钱是必须的。正所谓有投入才有产出。本人现在还没有入手深度学习机器的预算，所以使用 Mac Pro 利用 CPU 计算来玩玩简单点的深度学习项目。 我的 Mac Pro 配置为： 1234567OS X EI Capitan版本 10.11.6 (15G1212)MacBook Pro（13 英寸，2015 年初期）处理器 2.7 GHz Intel Core i5内存 8 GB 1867 MHz DDR3启动磁盘 Macintosh HD图形卡 Intel Iris Graphics 6100 1536 MB 1. 在 Mac 上安装 Anaconda 软件 2. 建立 tensorflow 的 conda 计算环境这里提一下有关 python 版本的问题，现在最稳定最常用的 python 版本是 2.7 和 3.5。入门 python 的话，建议从 2.7 版本入手，然后再过渡到 python 3.x 版本。我这里选用 tensorflow 依赖的 python 版本为 2.7。 在 Mac 终端执行命令： 1conda create -n tensorflow python=2.7 执行完命令后，可以看到 # # To activate this environment, use: # &gt; source activate tensorflow # # To deactivate this environment, use: # &gt; source deactivate tensorflow # 在 Mac 终端执行命令 source activate tensorflow 可以激活名为 tensorflow 的 conda 计算环境，执行命令 source deactivate tensorflow 能关闭 conda 计算环境。 3. 运行名为 tensorflow 的 conda 计算环境在 Mac 终端执行命令激活 conda 计算环境 1source activate tensorflow 命令执行完之后，终端最左边会出现 (tensorflow) ，说明名为 tensorflow 的 conda 计算环境被激活了。 4. 使用 pip 安装 tensorflow 深度学习框架仍然是在 Mac 终端执行命令： 1pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0rc0-py2-none-any.whl 安装过程中，出现以下错误提示，说明网络不好，或者需要使用 vpn 翻墙。多试几次就能安装成功了。 5. 验证 tensorflow 深度学习框架安装成功在 Mac 终端执行命令，进入 python 环境 1python 执行一个简单的 tensorflow 小程序来验证深度学习框架是否安装成功，小程序如下： import tensorflow as tf hello = tf.constant(&apos;Hello, TensorFlow!&apos;) sess = tf.Session() print sess.run(hello) a = tf.constant(10) b = tf.constant(32) print sess.run(a+b) 上面的小程序能正常运行，说明 tensorflow 深度学习框架安装成功了。此时，可以在软件 Anaconda 中查看名称为 tensorflow 的 conda 计算环境，如下图所示： 6. 加载 neural-style 神经网络训练代码项目 neural-style 位于 github 上，地址为 https://github.com/anishathalye/neural-style 创建或者选择一个文件夹来存放 neural-style 代码，使用 git clone 方式 download 代码，命令如下： 1git clone https://github.com/anishathalye/neural-style.git 项目 neural-style 会依赖 pillow，因此还需要安装 pillow，在终端执行命令： 1pip install pillow 7. 加载数据模型在终端执行命令： 1wget http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat 使用 wget 下载数据模型太慢了，简直不能忍，imagenet-vgg-verydeep-19.mat 文件还挺大，有576M。时间就是生命，生命不该浪费在下载上，果断使用 ctrl + c 中断了 wget 下载，如果你能忍受 wget ，那你就用 wget 吧，反正也能下载，就是慢点。为了节约点时间，神奇的淘宝就要闪亮登场了，我在淘宝上先花了1毛钱买了一天的迅雷会员，可是并没有像商家说的秒发账号密码。为了节约时间，果断地又花了1块钱买了一个月的迅雷会员，嗯，这家发货挺快的，接下来就是用迅雷加速下载链接 http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat imagenet-vgg-verydeep-19.mat 下载完成后，将其移动到 neural-style 根目录下，如下图所示： 8. 开启深度学习之梵高的大门进入 neural-style 目录，将需要处理的图片 nuc.jpg 放入目录 examples 中。nuc_output.jpg 为处理之后输出的文件名称。如果要处理图片 xyz.jpg 就将其放入 目录 examples 中，至于输出文件的名称可以随便设置，比如可以设置为 xyz_output.jpg。 在终端执行命令 A 1python neural_style.py --content ./examples/nuc.jpg --styles ./examples/1-style.jpg --output ./examples/nuc_output.jpg --iterations 1500 其中，1-style.jpg 为梵高大师画作，也就是需要学习的对象，这里也可以改为梵高的 2-style1.jpg 或 2-style2.jpg 来作为学习的对象。–iterations 1500 表示迭代次数，如果用笔记本或者普通的台式机就减少迭代次数，因为机器运算太慢了，1500 次迭代不知道要何年何月才能执行完。但是迭代次数在 1000 至 2000 之间处理的效果会比较理想。 在我的 Mac Pro 上设置的迭代次数为 100 次，得到处理后的图片效果还可以。需要提一下，在我的 Mac pro （配置前面已经给出） 上迭代1次大概会花费3分钟，迭代 100 次需要花费 5小时，毕竟是基于 CPU 计算，运算慢。要是有台 GPU 机器该有多爽，哈哈，想想而已，嘻嘻。 哦呵，出现了 error 提示，原因在于没有安装 scipy。在终端执行命令安装 scipy 1pip install scipy 将迭代次数改为 100次，再次执行命令 A OK，到此，所有步骤结束，我们可以看看经过深度学习处理之后的图片效果了。 9. 查看处理后的图片效果进入 neural-style/examples 目录，查看目标图片 nuc_output.jpg 梵高大师画作 1-style.jpg 如下： 选取了一张我就读过的大学的图书馆照片 nuc.jpg 作为需要处理的图片，如下： 迭代 100次，学习梵高大师画风后得到的处理后的目标图片 nuc_output.jpg，如下： 参考文献：http://wiki.jikexueyuan.com/project/tensorflow-zh/get_started/os_setup.html https://github.com/anishathalye/neural-style 迭代 1次，2次，10次，20次，30次，40次，100次输出的图片合成的 GIF 图，如下]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow 在 iOS 平台上的应用]]></title>
    <url>%2F2017%2F01%2F16%2Ftensorflow-iOS-application%2F</url>
    <content type="text"><![CDATA[Google 开源的深度学习框架 tensorflow 成为2016年最受欢迎的深度学习框架之一。tensorflow 除了支持 pc 端外，还较好的支持了 android，iOS 移动端平台。移动端作为现在互联网的终端主宰，tensorflow 毫无疑问地会引起移动互联网行业的广泛关注。深度学习在2016年的火爆，以及移动终端的主宰地位，作为程序员的我们，不玩玩 tensorflow 简直就 out 了。 1. fork Github 上的 tensorflow repotensorflow 的仓库 repo 位于 Github 上，我们作为开发者要在其基础上做开发，首先就需要 fork 一份 repo 到自己的 Github 账户下。 2. 参考 tensorflow 的官方使用文档编译（使用 tensorflow r1.0 版本）参考tensorflow 的官方使用文档编译 iOS 平台上的 tensorflow 库。 _Note: To use this library in an iOS application, see related instructions inthe iOS examples directory._ Install XCode 7.3 or more recent. If you have not already, you will need toinstall the command-line tools using xcode-select: 1xcode-select --install If this is a new install, you will need to run XCode once to agree to thelicense before continuing. Then install automake/libtool: 12brew install automakebrew install libtool Also, download the graph if you haven’t already: 1234mkdir -p ~/graphscurl -o ~/graphs/inception.zip \ https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip \ &amp;&amp; unzip ~/graphs/inception.zip -d ~/graphs/inception Note: Building all at once. If you just want to get the libraries compiled in a hurry, you can run thisfrom the root of your TensorFlow source folder: 1tensorflow/contrib/makefile/build_all_ios.sh This process will take around twenty minutes on a modern MacBook Pro. When it completes, you will have a library for a single architecture and thebenchmark program. Although successfully compiling the benchmark program is asign of success, the program is not a complete iOS app. To see TensorFlow running on iOS, the example Xcode project intensorflow/contrib/ios_examples shows how to use the staticlibrary in a simple app. 我使用的 Xcode 版本如下 我的 Mac Pro 设备参数如下，执行脚本 tensorflow/contrib/makefile/build_all_ios.sh 编译 iOS 平台的 tensorflow 库花费了90分钟。 libtensorflow-core.a 所在的路径和支持的 CPU 架构 armv7，armv7s，i386，x86_64，arm64 如下： 3. 打开 tensorflow/contrib/ios_examples 下的 Xcode 工程3.1 tf_ios_makefile_example.xcodeproj进入 /tensorflow/tensorflow/contrib/ios_examples/simple 文件夹打开工程 tf_ios_makefile_example.xcodeproj 运行 run 参考TensorFlow iOS Examples 文档 加载 iOS demo 工程所需要的数据模型，需要操作的命令如下 1234567mkdir -p ~/graphscurl -o ~/graphs/inception5h.zip \ https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip \ &amp;&amp; unzip ~/graphs/inception5h.zip -d ~/graphs/inception5hcp ~/graphs/inception5h/* tensorflow/contrib/ios_examples/benchmark/data/cp ~/graphs/inception5h/* tensorflow/contrib/ios_examples/camera/data/cp ~/graphs/inception5h/* tensorflow/contrib/ios_examples/simple/data/ 具体操作：先进入 tensorflow 的根目录，然后依次执行上面的命令。 此时，在 Xcode 中 clean（快捷键是 command + shift + k） 下 tf_ios_makefile_example.xcodeproj 工程。运行 run（快捷键是 command + r），success 后 iOS 设备出现的界面和点击按钮 Run Model 后的界面如下图所示： 3.2 camera_example.xcodeproj 可以识别出物体 3.3 benchmark.xcodeproj]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016年的总结]]></title>
    <url>%2F2016%2F12%2F27%2Fthe-summary-of-2016-year%2F</url>
    <content type="text"><![CDATA[2016年，换了一份工作，换了一个城市去工作，能走到今天很感谢以前RCTX的部门老大给自己机会进入音视频这个行业，也非常感谢同事对自己的帮助，特别是一起历经RCTX，YF共事的搭档jia哥。 前行的路上，是需要人帮助的，也是需要自我努力的。家人对自己的关心，就是自己精神上的支柱。朋友的帮助，是自己能走的更快的加速剂。 28岁的自己在经历社会的洗炼后，其实活的也不明白，要是真明白了，人生也就结束了。人生路越走越长，就会出现不明白的事，从来不苟同某某某在什么时候活明白了，生命的延续，就会诞生新的事物，除非在某个时刻停止了思考，停止了接收新的事物，此刻的活着与死无异，这就是此刻的明白。 2016年，经历了2家公司，8月5号从深圳辞职，8月8号到上海入职就加入项目的加急开发，期间没有一个缓冲让疲乏的身躯得到休整，心真的很疲惫。但是人接受的内外情景越多，人只会更强大，所以相信一切都是迎接美好而做的铺垫。 2017年加强锻炼，有个好的身体才能更好的体验生活。希望家人都健健康康的，希望身边的朋友知足长乐，希望自己的学业能更上一层楼。 二〇一六年十二月二十七日，北京]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[换了个博客主题，多说评论无法在博客中显示了]]></title>
    <url>%2F2016%2F11%2F01%2Fduo-shuo-is-poor%2F</url>
    <content type="text"><![CDATA[之前用的 hexo 博客主题不太美观，就想着换个，结果换了个博客主题，多说评论不能在博客中显示，评论只能在多说后台上看到。试着把文章和评论导出来再导入到多说后台，各种方法都试了，没搞成功。 文章评论不能显示的问题让我心情很不好，大量的评论要想一个不落的跟文章关联起来的可能性不大，内心里对恢复评论不作指望了，能恢复多少算多少吧。 想想这个事，真不该折腾，浪费了时间不说，关键是评论数据不能正常恢复显示很影响我的心情。先把评论数据保存，以后得闲，再整这个恢复评论数据显示的事情。说到以后得闲再做，估计这个事可能就被我遗忘了。 二〇一六年十一月一日，北京]]></content>
      <tags>
        <tag>工作</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我竟然恐婚了]]></title>
    <url>%2F2016%2F10%2F19%2Ffear-of-marriage%2F</url>
    <content type="text"><![CDATA[在二十岁出头的时候，认为恋爱、结婚是很值得期待、很幸福的一件事。可是到了如今的年龄，很害怕结婚。婚姻不是简简单单两个人的事情，要考虑到婚前两个家庭的关系，结婚时的开销，婚后小孩的培养以及夫妻之间感情生活的问题。分析我现在的这种状态，之所以有害怕结婚的状态是因为我认为未来有太多不可预期的因素，我现阶段能力不足以给予自身强有力的信心。 工作年限少，现阶段没什么积蓄，完全凭现阶段的努力给将来一个承诺，我自认为是一个较虚的构想，但未来是值得期待的。未来的不可确定性、现阶段的规划，我相信我能实现人生每个阶段所需要的生活需求。 年龄的增长，现在对事对物的看法趋于平和，毕竟不再是以前未经历社会世态的小年青了。以平和的态度，出世入世的心态，去拥有工作的战场，生活的小资情调。人生究竟是怎样的，活过了才知道。和谁一起走过，经历了才知道。 生活是不确定性的，但每个阶段的生活需要一个指标去指引，不然就会活的没有尽头，以至于看不到希望。 二〇一六年十月十九日，北京，昌平区]]></content>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊互动直播]]></title>
    <url>%2F2016%2F10%2F14%2FInteractive-live-broadcast%2F</url>
    <content type="text"><![CDATA[传统单人直播，形式上比较单调，以主播为主导，聊各种话题。观众与主播之间的沟通，基于即时通信（IM）技术以文字，表情，点赞等来与主播互动。这种互动是有延迟性的，单向的，一问一答的模式。为了增加观众与主播之间的强互动性，有必要引入实时通信（RTC）技术打造连麦互动直播。 即时通信中 XMPP 协议是被广泛的协议之一，XMPP 是基于文本标记语言 XML 的协议。打造一套属于自己的即时通信系统可以基于 XMPP 协议从零开发，也可以利用开源方案去实现，比如 openfire。openfire 是基于 XMPP 协议，采用 Java 编程语言开发的实时协作服务器，想快速实现一套 IM 方案，可以在该开源工程上进行改造，节省开发时间和成本。 音视频实时通信领域现广泛采用 Google 的开源方案 WebRTC。WebRTC 分为三部分：音频处理模块，视频处理模块，协议层模块。在使用 WebRTC 时，可以从中拆解出所需要的功能块去实现所需要的功能。 现在 WebRTC 被广泛推广，移动直播也从传统的单人直播进入到了多人连麦互动直播的应用场景。RTC 互动直播必将是直播行业中下一场厮杀的战场。 今天以讲师的角色在公司组织的直播技术培训课堂上讲了直播连麦涉及到的一些概念，方案设计。一节课讲授下来，我发现 RTC 可深挖，可改进的点还是比较多的。 二〇一六年十月十四日，北京，太阳宫中路12号]]></content>
      <tags>
        <tag>工作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何去爱]]></title>
    <url>%2F2016%2F10%2F12%2Fhow-to-love%2F</url>
    <content type="text"><![CDATA[如何去爱？关于这个问题，我想说的是到了一定年纪才会懂得。爱，不是一时情感的寄托，一时的喜好。看着小年轻们，经常上演死去活来的剧情，我内心深处想表达的是这些所谓的爱只是一时的爱慕，只是以后回忆青春年华的一个所谓的美景而已。年纪轻轻，没有太多的经历，自以为是男欢女爱，到底有多少能修到甜蜜与和谐的生活。到了一定年龄，经历过社会的锤炼，对男女之情才会有成熟的看法，对家庭的观念才有全局的认识。 古有三十而立，四十不惑之说，真的很佩服古人的总结。到三十的年纪，会多角度去看待事物，自身的角度，换位思考的角度。 在我还在读书生涯的时候，有人问我为什么不交女朋友。对于这个问题，当时考虑的是没有自我生存能力，未来充满太多不可预期的因素，怎么能轻易拉人进坑呢。 研究生毕业后，一直为了体现自己所学的价值，时间花在了学习和工作上。顿首回看，时间过的好快，两年多就这么过去了。脑中留下的印象就是加班，学习，加班，学习的场景。至于如何去哄女孩子，不会啊。连个对象都没有，怎么操作，哈哈。 随着年龄的增长，更加懂得责任，懂得对未来的规划。遇见对的人，一切都是值得期待的，也是值得努力的。 二〇一六年十月十二日，夜，北京，太阳宫中路12号]]></content>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论 IT 技术的有效期]]></title>
    <url>%2F2016%2F10%2F08%2Ftalk-about-IT-technology%2F</url>
    <content type="text"><![CDATA[IT，互联网行业的代名词，代表着科技前沿、市场趋势，无时不刻在改变我们的生活。有那么一群活跃在 IT 行业的人群，美言之软件开发工程师，自嘲之码农，对 IT 技术的革新有着最深刻的体会。旧的技术被取代，程序员若不及时跟上市场的节奏，被淘汰是必然的结局。技术的新旧更替，处在程序开发最上层的程序员的反应最敏感，受到的冲击最大。有点实力的公司，要彰显市场的竞争力，在引领市场获取未来竞争力的驱动下，招募一批研究员、专家、首席科学家去做理论研究。这批做理论的人对 IT 市场的应激反应肯定是延迟的。理论的过时经历的时间更长，理论间的交叉以及相互引入改进，让理论存活的时间变长，也让做理论研究的 IT 人群有长远发展的空间，不至于时刻面临着技术失效的局面。 理论研究与应用开发，对于 IT 从业人员不可或缺。应用开发用到的技术更新换代快，受制于开发平台，倘若开发平台衰退，顶层的应用开发工程分分钟失业，塞班系统的消失，致使当年大批人面临失业转岗，说不定下一个就是 Android，iOS。作为 IT 职场人士，要时刻保持着对市场敏锐的嗅觉。一种新的技术和平台的火爆，下一刻就是人满为患，需求过剩。2015年春节之前 iOS 应用开发人员的需求可以用疯狂来形容，全民创业，国家鼓励大学生创业，O2O 的火热，催生了 iOS 培训市场的不正常的膨胀，大量 iOS 开发人员噌的一下就冒出来了。2015年下半年到2016年上半年，市场上 iOS 开发工程师过剩，iOS 应用开发工程师的薪资待遇下降回归理性，但真正有能耐的人却又是少数，水平差又无基础理论知识支撑的以市场回报率为导向的开发者面临失业和裁员的局面是肯定的。 做软件开发，数据结构，算法，操作系统，编译原理，开发语言 C / C++ / JAVA，这些必备的技能要非常扎实。要想发展的更好，需要学习和实践软件架构。 自从进入音视频技术行业，我考虑的更多的是如何从这个已存在多年的技术领域跳跃到一个新的方向去做更有理论深度的行业。我这样考虑的原因是知道技术是有时效性的。时效性意味着回报，意味着市场需求。考虑到自己几年后就三十岁了，如何保持自己的核心竞争力和实现自己高收益的目标，是我现阶段必须慎重决策的事情。 二〇一六年十月八日]]></content>
      <tags>
        <tag>工作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[下一个十年]]></title>
    <url>%2F2016%2F10%2F07%2Fnext-ten-years%2F</url>
    <content type="text"><![CDATA[今晚跟高中要好的同学耀（某知名学校硕士）相聚，聊的东西都深入内心。八年未见，我们从十多岁的小青年变成了年近三十岁的大叔，相见仍然是那么熟悉，岁月的痕迹留在了我们脸上和身体健康状态上。每一个行业都有它的亮点，也有它的痛点，不必羡慕，不必感概，因为深处其中时，真的没有那么光鲜，没有那么轻松，没有那么自以为是。我们每个人都在为前途、命运在决斗，遍体鳞伤仍然奋斗不止。 每一代人有每一代人的痛楚，我和高中同学两人都是八零后，此处只谈农村八零后的切身感受，七零后离我太远了，九零后虽近但时代大背景已大不相同。农村的八零后这一代，真的就像是一个品牌的过渡产品，过渡产品意味着什么，试验品。八零后没有赶上九十年代互联网的发起，下海经商的浪潮，但城市与农村的差距却在这些兴起的商业中拉开了距离。 农村八零后的受教育背景虽然比七零后好一点，但是工作机会，社会竞争力已不可同年而语；相比九零后，八零后受到的教育中规中矩，没有九零后思想活跃。二十一世纪科技发展突飞猛进，八零后接受的信息根本无法与九零后抗衡，思想的活跃受益于社会经济的发展和生活观念的变化。八零后处在一个前有老资格的七零后，后有新奇玩法的九零后的时间轴上，压力是有的，工作的压力，结婚的压力，买房的压力，培养子女的压力等。好在八零后，算是比较务实的一代人，虽然是一代试验品，但好在没有自甘堕落。 我们无法选择自己所在的时代背景，唯有一颗上进的心，既成全了自己，也慰藉了父母的内心。农村子女的出路，只有读书。现在流行的素质教育，地方特色的高考保护主义，让城市与农村孩子的距离更远了，导致了拼爹，拼妈的潮流。拼出来的是血，还是汗，看自己能否体谅父母了。 心情比较沉重，写出的东西看着充满低沉的气息。跟同学聊完房贷，装修，生活成本后，我对个人的生活重新审视了一遍，彻底明白开源节流，细水长流的内涵和长远意义。工作两年多以来，只关注自己所学知识转化为物质回报的比例，忽视了物质的管理和利用。 下一个十年，要面对什么？婚姻，子女，个人业务转型。当为自己考虑时，都觉得还好，为下一代着想时，想想就累，想着是不是要再买一套大一点的房子，学校是不是要上好一点的，长大了是不是要出国留学。 人为什么而活，人活着到底感受到了什么？ 二〇一六年十月七日，深夜]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三年内个人发展规划]]></title>
    <url>%2F2016%2F10%2F06%2Fprivate-development-in-three-years%2F</url>
    <content type="text"><![CDATA[互联网发展日新月异，处在该行业就需要紧跟行业发展动态和高新技术理论发展趋势。这不仅关乎个人长远发展，也关乎家庭生活的质量，所以，需要做好长远实际的规划。 如何做好规划，这个问题因人而异。但都离不开学习环节。学习伴随我们的一生，一生的工作，一生的生活。我们工作效率的提升，生活质量的提升，都需要通过不断地学习、接受新的理念来修正过去的观念。 处在互联网这个非线性时变的系统中，我该如何有效精准地预测系统的实际输出呢。经验，理论是建立预测模型的基础，将工作经验，人生阅历，接收到的新理论作为训练数据，进而得出一个切合自身实际的预测输出。获取到一个理论模型之后，接下来的环节就是不断在实践过程中进行修正调参，以更符合期望。 我的三年规划，早已经做好了，一切围绕着贴近精神和生活的实际需求。以假设脱离实际，是空谈；以实际摒弃假设，是现实。作为人类群体中的个体，要活的有特色。生活中需要现实，也需要幻想。想要在一片蓝天看见彩虹，需要幻想才能看到诗一般的远方。想要在一片蓝天之下有自己的立足之地，需要现实才能占到属于自己的一份领地。无论如何，良好的心态，是我们拥有灿烂时光的根基。 二〇一六年十月六日]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对当下的思考]]></title>
    <url>%2F2016%2F10%2F02%2Fliving-in-the-present%2F</url>
    <content type="text"><![CDATA[当下是未来的基础，未来的憧憬犹如当下的场景，都离不开物质和精神的依赖。由于工作的变动，身心很是疲惫，看事待物都游离着低迷的气息。表面上的光鲜，私底下的痛楚和悲凉，只有在夜深人静的时候，在心底强行抹掉，把当前遇到的事情当作一种对人生的考验，痛而高仰头颅散发出坚而不催的积极向上的气息。 在北京的日子，自己的忙碌，无暇接听家人的电话。当自己闲下来的时候，觉得自己真的好无力。生活的现实，不得不让人在最应该珍惜一种感情的时候，却以一种强硬的语气和态度应对。父母面容的沧桑，不断苍老的白发袭来，身体没有以前那样灵活，我将这些看在心里，说不出的酸楚。我信奉的一句话是“美好的东西，都是煎熬之后才得到的”，同时欲望的膨胀，也是煎熬的过程中加剧痛苦的催化剂。没有美好的结局，只希望捕捉到的过程中的瞬间和阶段是一段美好的回忆。珍惜平时所见所遇，无需过急赶往远方，柔和的心态才是体验生活的缓冲剂。 回顾自己的生活，走得太快，提前透支自己，脑中搜寻近几年美好的记忆寥寥无几，大部分是自己为了追逐欲望的场面。近几年活过吗，为谁活过，答案是为了欲望活着。 在北京的日子，很感谢一起走过七年的同学，在自己精神压力最大的时候，约我一起出去游玩，驱除我的压力。在自己需要找人诉说的时候，Jane 学妹能够倾听我的充满情绪化的言语。人生阶段，该散的人散了，还在身边的，那就是自己最应该珍惜的人。人生中有几个知己，自己应该知足了。 毕业2年多一点，心中最愧疚面对的人，是自己的父母。自己大大小小的事情，他们都为我操心。看着父母的付出，我有着意志前行。一个人，精神是根基，精神若没了，也就废了。 念博士，这一直是我心中放不下的一件事。以我现在的精力已经根本无力施行。就把它当做一个美好的憧憬，进而不断激励自己去努力、去奋斗。 什么时候是创业的时机，我认为是在三十五岁左右。 当下就是认清自己，知长补短，让自己具备审时度势作出决策的能力。 人生之乐，有一乐是阅读，品着茶，看着书，思绪萦绕，穿越内心和精神是多么的充实。 二〇一六年十月二日]]></content>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对OpenGL开发实践的思考]]></title>
    <url>%2F2016%2F09%2F29%2Fopengl-practice-description%2F</url>
    <content type="text"><![CDATA[在使用一个工具的时候，首先要搞清楚它的定义［见 wiki ：OpenGL］，其次才是用它来做实践开发。我个人对它的理解是，OpenGL 是一个用于做图形处理的工具库，基于 GPU 硬件资源用来绘制和渲染 2D，3D 图形。要做比较炫的图形渲染，图形变化，选择 OpenGL 来开发可以降低设备对 CPU 资源的消耗。现在流行的 VR 开发，无外乎也是基于 OpenGL 来做的。知道 OpenGL 能干什么了，随之而来的是写个小程序把我们对 OpenGL 的好奇心实践下。 怎样写出一个自己能懂的程序？自己写的程序难道自己不懂吗，可能你真的不懂。接触一个新的事物时，我们大多数时候，就是寻找资源去仿造，至于原理和用法为什么是那样，我们其实是一头雾水。人类的本质是具备探索新事物运行原理的，在本质的驱动下，我们或多或少会去研究为什么是那样。研究的深入层度，无形中划分了不同人在该领域的级别。那么如何去探究 OpenGL？网络博客，论坛，QQ群，还是书籍。我的看法是，快速入门就是筛选一些写的精简的博客，然后买几本该方向很权威和专业的书籍细看。 写了这么多，我想表达的是，要有自己学习新东西的方法和策略。]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>OpenGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在上海总部工作一周的感受]]></title>
    <url>%2F2016%2F09%2F25%2Fa-week-in-Shanghai-City%2F</url>
    <content type="text"><![CDATA[2016年9月19日～2016年9月24日，在上海总部工作了一周，收获颇多。对于我而言，很感谢团队和公司的付出。 人到一定阶段，处在一个和谐进取的团队中，人的感官和认识不再是那种不入流的小民市井看法。 以前，看到某些颁奖典礼上一些人说的大客套话，感谢公司，感谢团队，让人听起来真的好假。为什么觉得好假，在于没有经历过别人的处境。在上海的一周，我确实体会到了一个人在舞台的光鲜，其背后有很多默默付出和支持的小伙伴们，说一些感谢的话，看起来是大话，用心去体验，那是发自内心最真实的声音。现在的社会，是群体，单打独斗不是主流，合群求同化异是趋势。对于知识交流，人际交流，只有放开，才能拥抱更大更广的空间。保持谦逊，保持温和，保持敬畏，保持进取，视野就会大一点，进步就会快一点。 溪水会长流，溪水也会干涸，身处的世界有无所不在的变化。害怕变化，就是拒绝机会，结果就是细水断流。保持拥抱变化的接受态度，以变应变来维持细水长流。细水聚集，见泉眼。 二〇一六年九月二十五日，北京，昌平区]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[近一个月在北京的感受]]></title>
    <url>%2F2016%2F09%2F06%2Fthe-feeling-of-Beijing-the-latest-month%2F</url>
    <content type="text"><![CDATA[来北京了近一个月了，感觉不太适应。从生活方面，工作节奏上，都显出很多问题。 生活方面，不适应这边的饮食，没南方吃的东西多，也没南方吃饭的地方多。在北京没有吃好过一顿早餐，也只吃过几顿比较满意的中午餐。我觉得要是这样下去，身体营养肯定跟不上，精力也会出现不足的问题。我准备每天自己做饭，早餐自己做，如果去公司上班，就带午饭过去。要是在家办公，就一天三顿都自己做。做饭，有助于培养耐心，细心，有助于做事流程上的统筹规划，最重要的是能吃上比较满意的饭菜。来北京近一个月了，对于我住的地方我都不熟悉，不知道附近有哪些大型超市，大型商场。每天都宅在家里办公，感觉整个人的状态都不打对劲。现在正当年，身体恢复都很快，但是人要活动，要呼吸新的空气，才能持久保持正常以及健康的状态。 工作方面，不适应远程办公的节奏。以前，都是早上去公司，晚上从公司离开。现在，突然不需要去公司办公，找不到上班工作的感觉了，我的工作时间安排出现了问题。自由办公，关键在于时间协调，每天工作任务的安排，这所有的东西都离不开时间的合理安排。在这种工作环境下，我的拖延症表现的很明显，时间的利用率也不高。反思了这段时间的工作，我首先要解决的是我的执行力问题，即解决拖延症，要在有效的时间里完成规定的计划和任务。]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对远程办公和时间统筹的思考]]></title>
    <url>%2F2016%2F09%2F03%2Fthink-of-telecommuting-and-time-co-ordination%2F</url>
    <content type="text"></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生命的精神支柱]]></title>
    <url>%2F2016%2F08%2F31%2Fspiritual-handholds-on-life%2F</url>
    <content type="text"><![CDATA[人太拼，总有累的时候，当疲惫驱入身心，人很容易垮掉。古语有云“休养生息”，疲惫的身躯需要休整，劳累的精神需要调节。在现在这个快节奏的社会，停下来去看看身边的风景是多么的奢侈。一味地向前，时间匆匆而过，记忆中却了无良辰美景。 累了，想放下，仔细一想身上肩负的担子，家人的期望，只能继续行走在疲惫的人生路上。逼近而立之年，心态不复以前，岁月不知不觉让人明白每个年龄段该有的心智，该有的责任感，该有的生活观，该有的工作价值。 每当自己很疲惫的时候，家人是自己最强大的精神信念，这种信念让我有勇气有决心面对眼前的疲惫不堪。 人生的意义是，能有充裕的时间和家人一起看看风景。 二〇一六年八月三十一日，北京，昌平区]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[北京见闻]]></title>
    <url>%2F2016%2F08%2F25%2Fsee-in-Beijing%2F</url>
    <content type="text"><![CDATA[在上海待了一周，就从上海乘动车来北京了。我想起了汪峰的一首歌《北京北京》 歌曲名:北京北京 演唱:汪峰 当我走在这里的每一条街道 我的心似乎从来都不能平静 就让花朵妒忌红名和电气致意 我似乎听到了他这不慢的心跳 我在这里欢笑我在这里哭泣 我在这里活着也在这死去 我在这里祈祷 我在这里迷惘 我在这里寻找 在这里寻求 北京 北京 咖啡管与广场又散着天气 就象夜空的哪月亮的距离 人们在挣扎中相互告慰和拥抱 寻找着 追著着 夜夜时的睡梦 我在这欢笑 我们在这哭泣 我在这活着也在这死去 我在这祈祷 我在这迷惘 我在这寻找 在这追求 如果有一天我不得不离去 我希望人们把我埋葬在这里 在这忘了感觉到我在存在 在这有太多有我眷恋的东西 我在这欢笑 我在这哭泣 我在这里活着也在这死去 我在这里祈祷 我在这里迷惘 我在这里寻找 也在这死去 我的认知是，不管在哪里都有竞争，都有失落，都有灿烂。山不转路转，路不转人转，人不转心转。心念一转，自在逍遥。 8月13号到的北京，现在已经有2周了，我对北京没什么感觉，就是一个工作的地方而已。看重会变成负担，平常心能体验到生活中的美。工作是我们不碌碌无为的物质基础，生活是我们平常心感受美的精神基础。我对北京的感受就这么多，谈不上喜欢，也谈不上不喜欢，平平淡淡的感觉。 2016年8月 写于北京 亲笔 二〇一六年八月，北京]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[上海见闻]]></title>
    <url>%2F2016%2F08%2F24%2Fsee-in-Shanghai%2F</url>
    <content type="text"><![CDATA[从深圳乘动车去上海的旅程中，我是兴奋的，可以增加新的阅历，丰富自己思考人生的素材。带着一颗充满期待的心，经历一夜的行程到了上海。 第一次到上海，上海并不是电视宣传和网络上宣称的那样华丽，我的第一感受是一个有着沧桑和陈旧感的都市。或许这就是她的魅力，释放着她的历史与过去。 我的落脚点是张江高科（张江软件园），到的那天是周六，在出租车上看张江软件园，看不到人影，这和深圳高新园一样，这里只是上班的地点而已，周一到周五上完班，周末软件园显得那么的安静，静的有一丝可怕和空寂。在张江全季酒店办了一周的入住手续，一路过来有点累，到上海的当天休息了半天。第一次来上海，可不能浪费了时间，总得出去转转呀，见识下所谓的大上海。 当一种东西，可以说是憧憬到来时，其实并没有想象中的完美和无暇，更多的是得到一种残缺，十全十美只是一种美好的期待，只是一种对人生充满美好信念的驱动力。正因为我们得到的是不完美的，有瑕疵的，我们才继续有动力去追寻我们心中那趋于完美，趋于内心期望值的远方。 上海，一个活在口碑相传的都市，真正去感受一番，终归只会得到一句“不过如此”。确实是，不过如此而已。 不论一个城市怎样，都寄载了不同人的梦想，不同人的期望。任何东西只存在单面是不可能的，双面或多面才丰富了整个世界和人生观。在上海，和硕士同学、师弟们游览了下黄浦江附近的景点，夜色下的黄浦江周边都被笼罩在霓虹灯下，更凸显了都市的浮华和不实。虽然不喜欢这种浮华的场景，但是不能因为不喜欢而排斥。 和同学、师弟们相聚，我感叹时间易逝，青春不在，感叹在学校的学生过早期望步入繁杂的社会而不安抚躁动的心去感受在学校学习的生涯。在这越来越快节奏的社会，学生们充满着躁动，急于步入看似美好的凡尘。学校培养机制的问题，学生为了将来生存的问题，一些问题交错在一起，社会源源不断的新生力量都处在一场莫名的躁动和追逐的历史河流中。 我试图摆脱第一份工作给我带来的不利影响，但现在仍没有彻底平复。我希望自己能心态自然，平和的去接受和觉悟，我发现我很难做到。活在一场人生追逐的决斗场中，结局无外乎胜利了继续战斗，失败了就出局。 上海的小伙伴，让我感受了一种有柔有刚的氛围，比较喜欢。期望不要过高，一起走过，一起奋进，几年后看现在至少是充实的，有收获的。每个人都有自己的角色，我找到了我扮演的角色，我喜欢做技术，喜欢接触一些新的自己不懂的东西然后掌握它，这就是工作。生活中以一种坦然开放的心态去融入去表达。 硕士毕业2年2个月了，我希望自己一直保持一颗学习的心态，以谦卑的心态去汲取别人身上的优点，学习是伴随我们一生的灵魂工程。学会爱，学会包容，学会理解，学会提升，人生才会过的如灿烂阳光，绚烂星夜。 二〇一六年八月，北京]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启程去上海]]></title>
    <url>%2F2016%2F08%2F05%2Fgo-to-Shanghai%2F</url>
    <content type="text"><![CDATA[今天办完了离职手续。先去上海待一段时间。对于上海之行，我还是很期待的。 路线安排是深圳北站到上海虹桥站。我在广州待过近3个月，深圳待过2年多，北京还是2011年3月去过一次，上海我没有去过。不到北上广深看一圈，不足以了解现今的互联网圈。对于此次行程，我有点兴奋。世界那么大，我想出去看看。 真正到了那里我会有什么样的感受，见后续博文。]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于雷霄骅离世的思考]]></title>
    <url>%2F2016%2F08%2F04%2Flei-xiao-hua-passed-away%2F</url>
    <content type="text"><![CDATA[8月3号晚上6点多，坐在我旁边的同事说“网上有新闻说雷霄骅过世了”，听到这个消息，我的看法是这肯定是假消息，雷博士被人黑了。后来，各大门户网站包括凤凰网，人民日报官微都有报道雷神过世了。当确认这个消息时，我感觉到身边缺少了点东西，就是那种亲近事物忽然不在的痛楚。不想说雷神是人才，这个是事实，不用我多说。说天妒英才，我认为没有表达出一种更真诚更真切的惋惜。 雷神在流媒体行业做出的贡献很大，因为有很多做音视频技术的人员都有拜读过他写过的文章，博客中的文章在流媒体技术方面有广度和深度，条理清晰，易懂。作为音视频行业中的一员，曾经我从他的文章中获益匪浅。 网上的报道和音视频技术qq群中，都说到雷博是一个认真负责，谦逊，好学的人，这些毫无争议。昨天，也就是8月3号，从晚上6点多一直到凌晨1点，雷博创建的音视频技术群，以及其它音视频技术开发群都表达了对这位年仅25岁即将博三的大神的不幸离世的可惜。谈论的焦点有以下几点： 第一，募捐。对于募捐，有跟雷博父母沟通过的人说雷博父母不需要捐助。 第二，创立基金。对于基金，有人表示成立有难度，有人同意。根据网上的消息以及跟雷博父母交流过人的说法，创立基金帮助一些学生是雷博父母的意愿，雷博父母将孩子的存款和学校的赔偿都用于这个基金。对于父母的这种做法，我非常敬佩。斯坦福大学的创建，不也是内心强大的父母为了孩子创立的吗。 第三，完成雷博未完成的项目。雷博创建的音视频技术qq群中不乏音视频技术大牛，提议共同维护和开发雷博未完成的项目。对于这种做法，在项目方许可情形下，我是认同的，可以聊以慰藉离开的雷博，也可以让雷博的导师以及学校通过这件事能够深刻认识到学校研究生培养中的问题。 第四，继续维护雷博创建的音视频技术论坛。雷博的音视频技术论坛，上线没多久，需要后期完善，我相信在广大音视频技术人才和IT开发人员的努力下，能够将雷博的精神延续下去。这种精神，是一种无偿分享技术心得的奉献。 第五，工作和身体的权衡。身体是革命的本钱，但是很多时候我们身不由己，不得不为了工作，学习，生活去做损害自身身体健康的事情。有人说，遇到这种事博士不念了、换导师，经历过研究生学习生涯的人都会觉得这种说法是不靠谱的。说出这种看法的人，有没有身临其境，有没有经历过真正的高压科研，有没有从事过为老板卖命做项目的事。如果没有，我只想说你是幸运的，但同时你的看法是粗浅的。 网上消息说中国传媒大学说雷博是酗酒死亡，还说雷博家属没有做尸检无法确认死因。对于国内这种遇事就撇清责任的做法，只有畜生可苟同。出事前在实验室工作，而且过世在实验室所在楼宇，学校有脸说没有责任。也没见雷博的导师发声，遇事躲闪，恐怕是深谙厚黑之学、中庸之道，却有违师德、良知。 不谈论国家体制，不谈论高校体制，不谈论高校研究生培养方式，不谈论道德良知，不谈论法制，我真不知道该谈什么了，我不知道在我们这个国有什么是不能干的了。当博导硕导变身为老板，当学生变为被奴役被压榨的打工仔，我们所谓的师生之礼仪，传道解惑是不是都被现实的金钱和权欲吞噬了。 二〇一六年八月四日，周四，深圳，南山区科技园]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[离开深圳去北京]]></title>
    <url>%2F2016%2F08%2F03%2Fleave-Shenzhen-for-Beijing%2F</url>
    <content type="text"><![CDATA[工作近3年，在深圳这个地方也待了2年半了。从毕业前对深圳的憧憬，到如今的匆匆离开，最终会被尘封到记忆深处。 在深圳的这几年，担任过iOS应用开发工程师，播放器开发工程师，编解码工程师，个人的职业经历比大部分 IT 职场人士丰富些。经历过飘在眼前的大额金钱，源于自己初入社会被人带偏离，其中也有自己个人的原因，浮华之物本能入进口袋却失之交臂，但是看清了社会的一角；经历过给很多公司接入所在公司业务的技术支持，看到了形形色色、层次不齐的 IT 职场人士，自己的提升非常之快；经历过公司被上市公司收购之后的资产重组和裁员；经历过一个人干一个平台底层开发，从无到有，到线上版本迭代多次。在深圳的2年多时间里，自己收获了什么？ 第一，个人的价值得到了提升。衡量自己价值最直接的标准是市场的薪资待遇。 第二，个人的身体状况欠佳。这对于it人士来说，精力充沛和身体状况佳才能走的更高。 第三，经历的事情太多，发现自己对人不那么热情了，受第一份工作的影响很大。 人生所到之处，有爱、有恨、有悔、有悟。如何让自己活的更有价值，更有意义才是自己存于世的根本。 二〇一六年八月三日，深圳，南山区科技园]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用openh264将iOS摄像头实时视频流编码为h264文件]]></title>
    <url>%2F2016%2F07%2F11%2Fuse-openh264-encode-iOS-camera-video-to-h264%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用fdk-aac将iOS麦克风实时音频流编码为aac文件]]></title>
    <url>%2F2016%2F07%2F08%2Fuse-fdk-aac-encode-iOS-mic-pcm-to-aac%2F</url>
    <content type="text"><![CDATA[近几天终于有点空闲时间写写东西了，将之前项目中自己做的东西更新上来，写写自己的心得感想。 iOS系统上的音频硬编码器，可实现将pcm音频数据编码为aac格式的数据。但是，对于低码率下的音频编码，就是它的软肋了。通过函数扫描iOS各系统上音频硬编码支持的情况发现，aac-lc编码都是支持的，苹果的官方文档也说的很清楚。对于aac-he-v2编码，iOS现在所有的系统都不支持，aac-he编码需要iOS9.0以上系统才支持音频硬编码。 做直播的时候，利用iOS的音频硬编码器可以将码率降到80Kb/s，用128Kb/s虽然可以让音质更好，但有点浪费带宽，当主播的网络状况不好时，这就成了一个影响观众流畅观看的一个负面因子，一般选用96Kb/s的码率，这个码率在现今国内网络条件下，还是显得有那么点大。为降低码率同时保证音频音质的情形下，选择合适的编码器才是在有限带宽下优化直播体验的最好武器。 音频编码器有很多，比如常见的有faac，vo_aacenc，fdk-aac，以及ffmpeg自带的音频编码器。ffmpeg3.0及之后的版本都已经移除了对faac，vo_aacenc的支持，对于ffmpeg3.0之后版本音频编码器的支持情况可以看看它的官网说明，从官网上看，ffmpeg组织建议使用他们的原生native音频编码器，可见，ffmpeg自带的编码器还是不错的。但是，为了一个音频编码功能引入一个这么强壮且庞大的库，有点不划算，虽然可裁剪ffmpeg，也可以将fdk-aac编译进ffmpeg中，但是如果应用中引入了基于ffmpeg开发的播放器的时候，ffmpeg库冲突的情形可能会出现。再说，有比ffmpeg原生音频编码器更好的fdk-aac音频编码器存在，就不需要考虑ffmpeg了，即便用ffmpeg做编码很方便。因为我们是用ffmpeg时间长了，就习惯依赖于ffmpeg了。 fdk-aac音频编码器用起来也很简单，跟单独用faac差不多，但比faac编码效果好。使用fdk-aac实现了aac-lc，aac-he，aac-he-v2 采样率为44.1KHz，通道数为2（立体声），码率为32Kb/s的aac编码。]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对理论学习与应用的一些思考]]></title>
    <url>%2F2016%2F07%2F07%2Ftalk-about-theory-study-and-application%2F</url>
    <content type="text"><![CDATA[待续。。。]]></content>
      <tags>
        <tag>工作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[硕士毕业2年对现阶段的思考]]></title>
    <url>%2F2016%2F07%2F06%2Ffeeling-of-TwoYear-after-master-graduation%2F</url>
    <content type="text"><![CDATA[目前，鉴于对自己的人生规划，做出了离开深圳去北京发展的决定。为什么去北京？第一是工作赚钱，第二是找个学校读个博士，第三是有了一定物质基础和条件要走出国门。 首先谈第一点，工作赚钱。毕业前我就在深圳正式工作了，到目前已经2年3个月时间了，做过iOS应用开发工程师，播放器开发工程师，以及如今的编解码工程师。其实，2014年做iOS应用开发工程师，要是坚持做到现在待遇也不会差，但是，我认为一个硕士出身，还是做理论的，要我天天画一些界面简直是羞耻，没前途，那就换方向。对于iOS开发这块，2011年3月，就去北京中关村实习过，本科毕业后，硕士阶段也做过iOS应用开发，内心底是抵触的，是拒绝的。读博士，才是我的真爱。后来发生了很多事情，放下读博的念头到深圳找了份iOS开发的工作，这家公司最后被**互动用6个亿收购了，该公司的1个月的收入顶1个一般互联网公司1年的收入，我在里面做什么，时间过去这么久了不想再多说了。人就是得不到的就会憧憬，说句不好听的话，就是犯贱。但是，骨子里的犯贱从另一方面来讲，这人有激情。2015年年后，找了一家在音视频行业有口碑的工作，正式进入流媒体行业工作，这也是我非常喜欢的一个工作方向，该公司还不错，做过2008年北京奥运会手机端直播，2015年给中国好声音提供终端视频直播平台，还有一些其它的比如什么青运会视频直播什么的。在该公司，是我个人学习和发展最快的时间段，做自己喜欢的事，是一种享受。后来，公司被收购重组，我决定离开了，拿了赔偿走人了。内心深处，挺感谢给我一个机会，一个平台进入流媒体的部门boss的。工作在换，薪资在涨，人心也越来越膨胀，这也符合我的个性，但这也是一大性格弊端。如今的我，在那个人人都欠它一个会员的公司工作着，感觉到了工作方向上遇到了瓶颈，突然间我就有种“你什么都会，其实你什么都不会”的迷失感，以前读博的念头又重新燃起，换工作加薪的想法也随之而来。更多时候，享受这种很变态的痛苦中有快感的感觉。 其次，对于读博这件事，其实我本身有兴趣的，也有欲望的。但是，我深入剖析自己后，读博也只是自己赚钱的一个支撑条件。物质基础决定精神世界，精神世界没有物质口粮支撑说什么都会沦为瞎扯淡。拿个博士学历，我敢肯定的说我自己就是为了满足自己的那么点欲望，其实对我赚钱能有多大的帮助作用，其实没什么影响。 再次，至于出国这个事，有多少人都是很憧憬的，因为看到了国内的种种乱象。对于搞研究，我是特别喜欢的，但是要我没钱去干自己喜欢的事，我是不情愿的。活都活不起了，喜欢的事能持续多久呢。 现在是2016年7月，硕士毕业2年了，2年中发生了很多事情，经历了职场的万象，被人当枪使，经历过裁员风波，经历过家人生病自己无能为力的痛楚。如今，虽然待遇有每月＊万人民币，但是我觉得很少，因为我觉得自身的价值还没有发挥出来，可提升的地方很多，也没有达到我的预期值。我给自己定的规划是工作3年内月薪要超过＊万人民币。我觉得这不是一个口号，是一个可行的，可实现的目标。当一个人的追求，就是为了钱的时候，某种程度上会限制自身更快的发展，这也是一种悲哀。 前2天向公司上级提了离职的事情，谈论了自己最真实的最直接的想法。 直播项目做了半年多了，自身没什么提升，每天就是各种需求导致各种改代码，时间就这么过去了，可惜。 接下去会做视频算法方面，什么编码解码器用现成的玩没什么意思，改进或者搞些新的策略，我喜欢图像这快，嗯，这就是兴趣。为什么不选择音频算法方向，毕竟这个太小众了，到头来在合理的时间内拿不到期望的结果无疑是一种精力的浪费。选择做什么，是关键。 明年，也要考虑找个合适的人谈谈人生。 明年，我就29岁了。哈哈，时间过的好快，结婚是个什么玩意，婚姻又是个什么玩意。一年后再发表感受。]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[移动端直播应用的开发流程]]></title>
    <url>%2F2016%2F05%2F09%2Flive-broadcast-development-process%2F</url>
    <content type="text"><![CDATA[Part 1. 推流端推流，就是将采集到的音频，视频数据通过流媒体协议发送到流媒体服务器。 一、选择流媒体协议现在直播应用，采用 RTMP 协议居多，也有部分使用 HLS 协议。 采用RTMP协议，就要看下它与流媒体服务器交互的过程，RTMP协议的默认端口是1935，采用 TCP 协议。并且需要了解 FLV 的封装格式。 采用 HLS 协议，因为涉及到切片，延时会比较大，需要了解 TS 流。 二、采集音视频数据做直播，数据的来源不可缺少，就是采集摄像头，麦克风的数据。 iOS 平台上采集音视频数据，需要使用 AVFoundation.Framework 框架，从 captureSession 会话的回调中获取音频，视频数据。 三、硬编码，软编码音视频数据软编码就是利用 CPU 资源来压缩音视频数据，硬编码与之相反。 软编码的话，现在广泛采用 FFmpeg 库结合编码库来实现，FFmpeg+X624 来编码视频数据 YUV/RGB 输出 H264 数据，FFmpeg+fdk_aac 来编码音频数据 PCM 输出 AAC 数据。 四、根据所选流媒体协议封包音视频数据将音频，视频打包成 packet。 五、与服务器交互发送封包数据根据所选流媒体协议，发送相应指令连接服务器，连接服务器成功后，就可以发送 packet 数据了。 Part 2. 拉流端拉流，就是从流媒体服务器获取音频，视频数据。 一、解析协议播放器端根据URL解析所用的流媒体协议（RTMP，HLS）。 二、解封装解封装，就是 demux 的过程，从容器格式（FLV，TS）中，分离出音视频数据。 三、解码解码，就是把获取到的数据解压缩，恢复成原始数据。解码就是将 H264 变成 YUV，AAC 变成 PCM。 解码可以使用软解码，硬解码。 软解码就是利用 CPU 资源去解压缩数据，采用的方式是FFmpeg解码。 硬解码，对于 iOS 平台来说，可以使用 VideoToolbox.Framework（该框架只能在iOS 8.0及以上系统使用）硬解码视频数据。Android 平台上，可以使用 MediaCodec 来硬解码视频数据。 四、渲染数据采用 OpenGL 渲染 YUV 数据，呈现视频画面。将PCM送入设备的硬件资源播放，产生声音。 iOS 播放流式音频，使用 Audio Queue 的方式，即，利用 AudioToolbox.Framework 框架。]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重温《Objective-C高级编程iOS与OSX多线程和内存管理》一书]]></title>
    <url>%2F2016%2F03%2F24%2Freview-multithreading-and-memory-management-for-ios-and-osx%2F</url>
    <content type="text"></content>
      <tags>
        <tag>iOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在iOS上硬编码推流－编码aac（六）]]></title>
    <url>%2F2016%2F03%2F23%2Fhw-encode-and-transfer-in-ios-platform-encode-aac-part6%2F</url>
    <content type="text"></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在iOS上硬编码推流－麦克风数据采集（五）]]></title>
    <url>%2F2016%2F03%2F22%2Fhw-encode-and-transfer-in-ios-platform-mic-part5%2F</url>
    <content type="text"></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在iOS上硬编码推流－硬编码h264（四）]]></title>
    <url>%2F2016%2F03%2F20%2Fhw-encode-and-transfer-in-ios-platform-videotoolbox-encode-h264-part4%2F</url>
    <content type="text"></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在iOS上硬编码推流－实时图像滤镜（三）]]></title>
    <url>%2F2016%2F03%2F19%2Fhw-encode-and-transfer-in-ios-platform-real-time-image-filter-part3%2F</url>
    <content type="text"></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在iOS上硬编码推流－图像融合（二）]]></title>
    <url>%2F2016%2F03%2F18%2Fhw-encode-and-transfer-in-ios-platform-image-merging-part2%2F</url>
    <content type="text"><![CDATA[采用UIGraphics将两幅图绘制到同一个画布上输出，达到图像融合的简单效果。 1234567891011121314151617181920 UIImage *supermanImage = [UIImage imageNamed:@"superman.png"]; UIImage *moneyImage = [UIImage imageNamed:@"money.png"]; CGSize supermanSize = [supermanImage size]; CGSize moneySize = [moneyImage size]; // NSLog(@"s : %f,%f \n m : %f,%f", supermanSize.width, supermanSize.height, moneySize.width, moneySize.height); UIGraphicsBeginImageContext(supermanSize); [supermanImage drawInRect:CGRectMake(0, 0, supermanSize.width, supermanSize.height)]; [moneyImage drawInRect:CGRectMake(0, 0, moneySize.width, moneySize.height)]; UIImage *mergeImage = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); mergeImageView = [[UIImageView alloc] init]; mergeImageView.image = mergeImage; mergeImageView.frame = self.view.bounds; [self.view addSubview:mergeImageView]; demo地址：https://github.com/depthlove/STMImageMerging]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在iOS上硬编码推流－摄像头数据采集（一）]]></title>
    <url>%2F2016%2F03%2F17%2Fhw-encode-and-transfer-in-ios-platform-camera-part1%2F</url>
    <content type="text"><![CDATA[该文的内容在我之前的文章中已经实现过，但是为了结构清晰起见，本文将相机控制和采集视频数据功能封装为单独的库。 iOS上使用AVFoundation.framework框架来调用系统相机并获取视频数据。视频数据可以根据设定的参数，可采集到RGB或YUV数据，一般使用的是GBRA32，420v，420f，下面演示相机的调用和视频数据的获取。 a)引入框架的头文件#import &lt;AVFoundation/AVFoundation.h&gt; b)调用的类遵守协议AVCaptureVideoDataOutputSampleBufferDelegate c)声明变量 AVCaptureSession *captureSession; AVCaptureDevice *captureDevice; AVCaptureDeviceInput *captureDeviceInput; AVCaptureVideoDataOutput *captureVdieoDataOutput; d)实现 e)demo地址 https://github.com/depthlove/STMCamera]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人在音视频开发方面的经历分享启动啦]]></title>
    <url>%2F2016%2F03%2F16%2Fshare-my-project-experience-in-audio-and-video-development%2F</url>
    <content type="text"><![CDATA[本人小硕一枚，科班出身，毕业后在公司做过iOS应用开发工程师，搞过iOS项目移植到flash、unity3d平台的开发，在流媒体行业内实力不错的公司做过播放器开发工程师，现在在一家互联网公司做音视频编解码工程师。 以前，就想着把自己实战经历过的项目经验总结并分享下，个人觉得在分享的过程中，会加深对知识点和技术的认识，同时也能帮助下一些人。值得开心的是，去年4月份左右，具体时间记不得请了，北京的一家做游戏应用内支付的公司的iOS工程师看到我的博客园博客后，联系我寻求iOS项目移植到flash平台的帮助。经过半个月时间与我沟通，项目移植成功上线了。在小型互联网公司，工程师可谓是孤军奋战。分享自己所学和经历是一件很兴奋，很开心的事。 分享安排：先分享iOS平台上的音视频开发 再分享Android平台上的音视频开发 最后分享PC平台上的音视频开发]]></content>
      <tags>
        <tag>工作</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS平台上编码之CMTime，CMTimeMake，CMTimeMakeWithSeconds的作用]]></title>
    <url>%2F2016%2F03%2F15%2FCMTime-CMTimeMake-CMTimeMakeWithSeconds-in-iOS-encode%2F</url>
    <content type="text"><![CDATA[在iOS平台上使用iOS8及以上系统的VideoToolbox进行硬编码，会涉及到CMTime，CMTimeMake，CMTimeMakeWithSeconds的使用。下面说下这几个结构体的作用。 CoreMedia.framework的CMTime.h中CMTime，CMTimeMake，CMTimeMakeWithSeconds的定义如下： （备注：关于CMTime可翻看下苹果官网api文档http://developer.apple.com/library/mac/#documentation/CoreMedia/Reference/CMTime/Reference/reference.html） 123456789101112131415/*! @typedef CMTime @abstract Rational time value represented as int64/int32.*/typedef struct&#123; CMTimeValue value; /*! @field value The value of the CMTime. value/timescale = seconds. */ CMTimeScale timescale; /*! @field timescale The timescale of the CMTime. value/timescale = seconds. */ CMTimeFlags flags; /*! @field flags The flags, eg. kCMTimeFlags_Valid, kCMTimeFlags_PositiveInfinity, etc. */ CMTimeEpoch epoch; /*! @field epoch Differentiates between equal timestamps that are actually different because of looping, multi-item sequencing, etc. Will be used during comparison: greater epochs happen after lesser ones. Additions/subtraction is only possible within a single epoch, however, since epoch length may be unknown/variable. */&#125; CMTime; 12345678910/*! @function CMTimeMake @abstract Make a valid CMTime with value and timescale. Epoch is implied to be 0. @result The resulting CMTime.*/CM_EXPORT CMTime CMTimeMake( int64_t value, /*! @param value Initializes the value field of the resulting CMTime. */ int32_t timescale) /*! @param timescale Initializes the timescale field of the resulting CMTime. */ __OSX_AVAILABLE_STARTING(__MAC_10_7,__IPHONE_4_0); /*! @function CMTimeMakeWithSeconds @abstract Make a CMTime from a Float64 number of seconds, and a preferred timescale. @discussion The epoch of the result will be zero. If preferredTimeScale is &lt;= 0, the result will be an invalid CMTime. If the preferred timescale will cause an overflow, the timescale will be halved repeatedly until the overflow goes away, or the timescale is 1. If it still overflows at that point, the result will be +/- infinity. The kCMTimeFlags_HasBeenRounded flag will be set if the result, when converted back to seconds, is not exactly equal to the original seconds value. @result The resulting CMTime. */ CM_EXPORT CMTime CMTimeMakeWithSeconds( Float64 seconds, int32_t preferredTimeScale) __OSX_AVAILABLE_STARTING(__MAC_10_7,__IPHONE_4_0); CMTime firstframe=CMTimeMake(1,10); CMTime lastframe=CMTimeMake(10, 10); CMTime是专门用来表示影片事件用的类别，用法为：CMTimeMake(time, timeScale)。其中，time指的是时间，而不是秒，而时间要换算成秒，此时就要用到第二个参数timeScale。timeScale指的是1秒需要有几个frame构成（可以看作为fps），因此真正要表达的时间就是 time/timeScale，才会是秒。 上面的代码可以理解为，视频的fps（帧率）是10，firstframe是第一帧，在视频中的时间为0.1秒，lastframe是第10帧，在视频中的时间为1秒。 或者换种写法：CMTime curFrame = CMTimeMake(第几帧， 帧率）。看看另一篇博客的写法：解釋何謂iOS中的CMTimeMake， 要使用VPN翻墙才能打开这篇文章。这么看，CMTime firstframe=CMTimeMake(32，16);CMTime lastframe=CMTimeMake(48, 24); 这两个都表示2秒的时间。但是帧率是完全不同的。 CMTimeMakeWithSeconds和CMTimeMake区别在于，第一个函数的第一个参数可以是float，其他一样。 以上解释的比较清楚了，为加深印象，可看下stackoverflow上Trying to understand CMTime一文。 综述如下： CMTimeMake(a,b) a当前第几帧, b每秒钟多少帧.当前播放时间a/b CMTimeMakeWithSeconds(a,b) a当前时间,b每秒钟多少帧.]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在iOS平台上编译WebRTC]]></title>
    <url>%2F2016%2F03%2F14%2Fuse-webrtc-codec-in-ios-platform%2F</url>
    <content type="text"></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016年3月买房纪实]]></title>
    <url>%2F2016%2F03%2F10%2Fbuy-house-in-2016-03%2F</url>
    <content type="text"><![CDATA[去年11月就计划着今年要买房，根据市场行情，我看涨房价走势。但是没有快速入手，原因有很多。主要原因有考虑将来自己要待在哪里生活和发展。今年春节后，一线城市，强二线城市的房价蹭蹭蹭往上窜，此时不入手，更待何时。中国的房地产行情是独具中国特色的，按照市场经济规律来评判在中国行不通，身在中国看懂国家大政策就足够用来分析行情，市场经济规律与我等百姓干系不大。这也是中国房地产的奇特之处。很多经济学者和评论员看衰中国房地产，不如任志强的一句“有钱就买”来的深刻、清晰、明了。任志强，咱们的任大炮同志开炮中国房地产、政府，政府封了他的新浪微博，并发文批判他丧失党性人性，政治上的事千丝万缕，咱百姓插不上嘴，但是房地产这码事，明眼人都能看出点道道。这里不讨论任大炮同志，也不想讨论大部分网民。网民怎么怎么样，大炮同志所作所说怎样怎样，互联网这个大平台充当了事件辐射的工具，跟政府作对能有好果子吃么，跟大部分网民对着干实属浪费精力。事实上，2016年春节到现在，房地产市场普涨，这里不考虑三四线城市的行情，除非实在活不下去了，才有人回三四线城市的。 2月25号下定决心入手，马上开始搜集楼盘信息，3月4号下午，3月5号上午家里人看了一些楼盘后，经过与我商量，在3月5号中午交了定金，9号我与开发商签了商品房购买合同，就这样把房买了。3月1号国家发布降准的信息后，楼市一片涨涨涨，对于刚需者来说是利好。对于是否充当了开发商的接盘侠，我觉得有刚需、楼市利好、能拿出钱付首付、能承受每月还贷，那就买。中国文化中，家这个概念在每个人心中都占很大的分量。 计划买第二套房，但是考虑到还有一些重要的事情上花钱的地方很多，这个想法只能搁浅了。4月份开始，一线城市，二线城市的房价肯定是唰唰唰的往上涨，涨的会更猛。想买房，就不要指望房价跌，至于什么时候是拐点，刚需者有时间等吗？我觉得一等，就更没机会买房了。]]></content>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回顾2015，展望2016]]></title>
    <url>%2F2015%2F12%2F31%2Freview-2015-outlook-2016%2F</url>
    <content type="text"><![CDATA[今天是2015年12月31号，2015年最后的一天，这一年过的真快，自己在这一年中做了什么，在2016年需要做什么，我需要花点时间来总结和规划下。2015年的最后一天，没有了以往的那种希望时间快速过去的欲望，而是一种淡淡的对时间流逝的可惜和害怕，害怕变老，害怕时间过得太快，要做的事情没有按时完成。]]></content>
      <tags>
        <tag>工作</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rtmp直播推流（二）－－将音频／视频推送到流媒体服务器]]></title>
    <url>%2F2015%2F11%2F16%2Fpush-video-or-audio-to-server-in-rtmp-live-play%2F</url>
    <content type="text"><![CDATA[参考我之前写的一篇文章利用FFmpeg+x264将iOS摄像头实时视频流编码为h264文件 参看文章ffmpeg综合应用示例（一）——摄像头直播 ， ffmpeg综合应用示例（四）——摄像头直播的视音频同步 参看文章最简单的基于FFmpeg的移动端例子：IOS 推流器， 最简单的基于FFmpeg的推流器（以推送RTMP为例） 使用librtmp发布h.264可参看文章最简单的基于librtmp的示例：发布H.264（H.264通过RTMP发布）， 最简单的基于librtmp的示例：发布（FLV通过RTMP发布） 通过以上文章，就可以将采集到的音频／视频数据推送到流媒体服务器，至于观看延迟方面，需要结合实际情况区处理了。]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rtmp直播推流（一）－－flv格式解析与封装]]></title>
    <url>%2F2015%2F11%2F13%2Fflv-analysis-in-rtmp-live-play%2F</url>
    <content type="text"><![CDATA[flv文件格式分析，可参看RTMP中FLV流到标准h264、aac的转换，该文章写的很清晰。 flv封装格式解析，可参看视音频数据处理入门：FLV封装格式解析，文章图文并貌，很直观。 flv文件封装，可参看将h.264视频流封装成flv格式文件（一.flv格式）， 将h.264视频流封装成flv格式文件（二.开始动手） 使用rtmp协议发送flv文件，可参看rtmp协议简单解析以及用其发送h264的flv文件 通过以上几篇好文，对flv文件格式，封装flv文件以及通过rtmp协议与流媒体服务器交互就有清晰的认识。 附上两张图 图一为 FLV Format with AVC Video Tag （H.264） 图二为 FLV Format with AAC Audio Tag (AAC)]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS开发需要掌握的知识点]]></title>
    <url>%2F2015%2F10%2F29%2FiOS-development-need-abilities%2F</url>
    <content type="text"><![CDATA[任何一个方向，都需要基本功扎实。软件开发领域，数据结构与算法，操作系统，网络协议（tcp/ip协议族）是立足的基础。 下面列举iOS开发需要掌握的知识点： 一、数据结构与算法（一）数据结构1. 链表2. 队列3. 树（二叉树）4. 哈希表（即散列表）（二）算法1. 查找算法2. 排序算法二、操作系统（一）缓存（二）分页（三）生产者与消费者三、网络协议（tcp/ip协议族）（一）http协议（http/https）（二）tcp协议（三）udp协议（四）ip协议四、Socket、Thread（套接字、线程）（一）Socket（套接字）（二）Thread（线程）四、iOS方向知识点（一）AutoLayout / SizeClass（二）KVO/KVC（三）NSNotification（四）Block（五）Protocol/Delegate（六）Grand Central Dispatch（七）NSOperation Queue（八）NSRunloop（九）Runtime（十）HTTP Request：Post、Get（十一）Json、XML（十二）CoreData、Sqlite（十三）LLDB]]></content>
      <tags>
        <tag>iOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[音视频编解码中常用的编译脚本]]></title>
    <url>%2F2015%2F10%2F29%2Ffrequently-used-audio-and-video-codec-script%2F</url>
    <content type="text"><![CDATA[FFmpegiOS：https://github.com/kewlbear/FFmpeg-iOS-build-script x264iOS：https://github.com/kewlbear/x264-ios fdk-aaciOS：https://github.com/verybigdog/fdk-aac-ios 或 https://github.com/kewlbear/fdk-aac-build-script-for-iOS，不支持iOS arm64，但是可以借鉴 librtmpiOS：https://github.com/saiten/ios-librtmp OpenSSLiOS：https://github.com/x2on/OpenSSL-for-iPhone]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS上使用VideoToolBox硬解码h264]]></title>
    <url>%2F2015%2F10%2F24%2Fuse-VideoToolBox-decode-h264%2F</url>
    <content type="text"><![CDATA[以下内容摘自我博客的编译iOS平台上使用的X264库一文。 从iOS8开始，苹果开放了硬解码和硬编码API，框架为VideoToolbox.framework， 此框架需要在iOS8及以上的系统上才能使用。 此框架中的硬解码API是几个纯C函数，在任何OC或者 C++代码里都可以使用。使用的时候，首先，要把 VideoToolbox.framework 添加到工程里，并且在要使用该API的文件中包含头文件 #include &lt;VideoToolbox/VideoToolbox.h&gt;，然后，就可以畅快的高效的对视频流进行硬编码了。 其实至少从iPhone4开始，苹果就是支持硬件解码了，但是硬解码API框架VideoToolBox一直是私有API，如果调用这个私有库，那么app在必须在越狱的设备上运行，正常的App如果想提交到AppStore是不允许使用私有API的。 点评：对于如何使用iOS平台上的VideoToolbox框架对图像数据进行硬编码，github上已经有很多demo了，虽然苹果官方文档对VideoToolbox介绍的不全面，但足以搞清楚整个流程，至于细节需要花些时间自己去搞了，比如时间戳的计算。]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用FFmpeg和rtmp实现iOS设备上摄像头视频流的推送]]></title>
    <url>%2F2015%2F10%2F13%2Fuse-ffmpeg-and-rtmp-to-push-iOS-camera-streaming%2F</url>
    <content type="text"></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac上安装Android Studio]]></title>
    <url>%2F2015%2F10%2F09%2Finstall-android-studio-ide%2F</url>
    <content type="text"><![CDATA[Google在国内被墙，有关android开发的一些工具在Google上下载都需要使用VPN服务。我使用过一些免费与付费的VPN，想使用较好的服务质量就需要花点小钱，付费VPN中 uuu vpn 很好用，有windows，Mac客户端，使用比较方便，代理服务稳定，快速，比 green vpn 好用多了，green vpn很烂，买过几次包月的套餐，向其客服吐槽过其VPN简直烂的让人无语，总之，对green vpn 没有什么好印象了。 一、使用VPN服务 具体使用什么VPN看个人喜好，我使用的是uuu vpn。在Mac上启用 uuu vpn，翻墙。 二、在Google官网下载Android Studio Android Studio 的官方下载地址为：http://developer.android.com/sdk/index.html。我的系统是Mac，选择Mac版的Android Studio IDE。 下载android studio： 下载后，下载页面出现一下安装提示信息 Installing Android Studio，重要信息如下： 在安装android studio 之前，必须安装 JDK 6 或者 JDK 的更高版本，只安装 JRE 是不够的。要开发 android 5.0 或更高的android系统版本程序，就必须安装 JDK 7。要知道自己的Mac系统上是否安装了 JDK ，在Mac终端执行命令 javac -version，如下： 从图中的命令执行结果可知，Mac上安装的 JDK 的版本为 JDK 8 Update 40 。 如果检查到 JDK 没有安装，或者版本低于6，那么，就需要下载新版本的JDK，并安装。]]></content>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用FFmpeg开发iOS播放器（第一部分）]]></title>
    <url>%2F2015%2F10%2F08%2Fuse-ffmpeg-to-create-a-iOS-player-part1%2F</url>
    <content type="text"><![CDATA[一、开发需求 支持多种流媒体协议（rtsp, rtmp, hls） 播放，暂停，拖动 录像，截图 语音对讲，视频对讲 视频上传流媒体服务器 二、开发准备工作 查看流媒体协议知识点 理解音视频流的分离与复合 编译FFmpeg库 通读FFmpeg源码中ffplay.c 理解OpengGL ES的渲染流程 三、开发执行 记录开发过程中的心得 完成iOS播放器库(*.a)]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Mac OSX上安装ffmpeg && ffmpeg命令行将h264封装为mp4]]></title>
    <url>%2F2015%2F09%2F24%2Finstall-ffmpeg-to-MacOSX-and-use-ffmpeg-to-transform-h264-to-mp4%2F</url>
    <content type="text"><![CDATA[ffmpeg功能强大，可以通过命令行来对音视频进行处理。为了使用其功能，我在Mac上对其进行了安装。 我的Mac OS X 系统版本：OS X Yosemite, 10.10.14 关于ffmpeg在Mac OS X上的编译，FFmpeg上有官方文档说明：https://trac.ffmpeg.org/wiki/CompilationGuide/MacOSX。该文档给出了3种方法： ffmpeg through Homebrew Compiling FFmpeg yourself Manual install of the dependencies without Homebrew 看了这三种方法的官方说明后，我选择了第一种，因为最简单。 首先，Mac上要安装Homebrew在终端执行命令，ruby -e “$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849devtemobideMac-mini:~ sunminmin$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"==&gt; This script will install:/usr/local/bin/brew/usr/local/Library/.../usr/local/share/man/man1/brew.1 Press RETURN to continue or any other key to abort==&gt; /usr/bin/sudo /bin/chmod g+rwx /Library/Caches/HomebrewPassword:==&gt; Downloading and installing Homebrew... remote: Counting objects: 225571, done.remote: Compressing objects: 100% (59354/59354), done.remote: Total 225571 (delta 165037), reused 225482 (delta 164969)Receiving objects: 100% (225571/225571), 51.57 MiB | 1.19 MiB/s, done.Resolving deltas: 100% (165037/165037), done.From https://github.com/Homebrew/homebrew * [new branch] master -&gt; origin/masterHEAD is now at 82d7dc4 tomcat: update 8.0.17 bottle.==&gt; Installation successful!==&gt; Next stepsRun `brew doctor` before you install anythingRun `brew help` to get started 其次，安装ffmpeg在终端执行命令，brew install ffmpeg 12345678910111213141516171819202122232425262728293031323334353637383940devtemobideMac-mini:~ sunminmin$ brew install ffmpeg==&gt; Installing dependencies for ffmpeg: x264, lame, libvo-aacenc, xvid==&gt; Installing ffmpeg dependency: x264==&gt; Downloading https://homebrew.bintray.com/bottles/x264-r2533.yosemite.bottle.######################################################################## 100.0%==&gt; Pouring x264-r2533.yosemite.bottle.tar.gz🍺 /usr/local/Cellar/x264/r2533: 9 files, 3.4M==&gt; Installing ffmpeg dependency: lame==&gt; Downloading https://homebrew.bintray.com/bottles/lame-3.99.5.yosemite.bottle######################################################################## 100.0%==&gt; Pouring lame-3.99.5.yosemite.bottle.1.tar.gz🍺 /usr/local/Cellar/lame/3.99.5: 25 files, 2.1M==&gt; Installing ffmpeg dependency: libvo-aacenc==&gt; Downloading https://homebrew.bintray.com/bottles/libvo-aacenc-0.1.3.yosemite######################################################################## 100.0%==&gt; Pouring libvo-aacenc-0.1.3.yosemite.bottle.tar.gz🍺 /usr/local/Cellar/libvo-aacenc/0.1.3: 15 files, 336K==&gt; Installing ffmpeg dependency: xvid==&gt; Downloading https://homebrew.bintray.com/bottles/xvid-1.3.3.yosemite.bottle.######################################################################## 100.0%==&gt; Pouring xvid-1.3.3.yosemite.bottle.2.tar.gz🍺 /usr/local/Cellar/xvid/1.3.3: 9 files, 1.3M==&gt; Installing ffmpeg==&gt; Downloading https://homebrew.bintray.com/bottles/ffmpeg-2.6.3.yosemite.bottle.ta######################################################################## 100.0%==&gt; Pouring ffmpeg-2.6.3.yosemite.bottle.tar.gz==&gt; CaveatsFFmpeg has been built without libfaac for licensing reasons;libvo-aacenc is used by default.To install with libfaac, you can: brew reinstall ffmpeg --with-faacYou can also use the experimental FFmpeg encoder, libfdk-aac, orlibvo_aacenc to encode AAC audio: ffmpeg -i input.wav -c:a aac -strict experimental output.m4aOr: brew reinstall ffmpeg --with-fdk-aac ffmpeg -i input.wav -c:a libfdk_aac output.m4a==&gt; Summary🍺 /usr/local/Cellar/ffmpeg/2.6.3: 204 files, 42M 再次，查看ffmpeg info在终端执行命令，brew info ffmpeg 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788devtemobideMac-mini:~ sunminmin$ brew info ffmpegffmpeg: stable 2.6.3 (bottled), HEADPlay, record, convert, and stream audio and videohttps://ffmpeg.org//usr/local/Cellar/ffmpeg/2.6.3 (204 files, 42M) * Poured from bottleFrom: https://github.com/Homebrew/homebrew/blob/master/Library/Formula/ffmpeg.rb==&gt; DependenciesBuild: pkg-config ✘, texi2html ✘, yasm ✘Recommended: x264 ✔, lame ✔, libvo-aacenc ✔, xvid ✔Optional: faac ✘, fontconfig ✘, freetype ✘, theora ✘, libvorbis ✘, libvpx ✘, rtmpdump ✘, opencore-amr ✘, libass ✘, openjpeg ✘, speex ✘, schroedinger ✘, fdk-aac ✘, opus ✘, frei0r ✘, libcaca ✘, libbluray ✘, libsoxr ✘, libquvi ✘, libvidstab ✘, x265 ✘, openssl ✘, libssh ✘, webp ✘==&gt; Options--with-faac Build with faac support--with-fdk-aac Enable the Fraunhofer FDK AAC library--with-ffplay Enable FFplay media player--with-fontconfig Build with fontconfig support--with-freetype Build with freetype support--with-frei0r Build with frei0r support--with-libass Enable ASS/SSA subtitle format--with-libbluray Build with libbluray support--with-libcaca Build with libcaca support--with-libquvi Build with libquvi support--with-libsoxr Enable the soxr resample library--with-libssh Enable SFTP protocol via libssh--with-libvidstab Enable vid.stab support for video stabilization--with-libvorbis Build with libvorbis support--with-libvpx Build with libvpx support--with-opencore-amr Enable Opencore AMR NR/WB audio format--with-openjpeg Enable JPEG 2000 image format--with-openssl Enable SSL support--with-opus Build with opus support--with-rtmpdump Enable RTMP protocol--with-schroedinger Enable Dirac video format--with-speex Build with speex support--with-theora Build with theora support--with-tools Enable additional FFmpeg tools--with-webp Enable using libwebp to encode WEBP images--with-x265 Enable x265 encoder--without-lame Disable MP3 encoder--without-libvo-aacenc Disable VisualOn AAC encoder--without-qtkit Disable deprecated QuickTime framework--without-x264 Disable H.264 encoder--without-xvid Disable Xvid MPEG-4 video encoder--HEAD Install HEAD version==&gt; CaveatsFFmpeg has been built without libfaac for licensing reasons;libvo-aacenc is used by default.To install with libfaac, you can: brew reinstall ffmpeg --with-faacYou can also use the experimental FFmpeg encoder, libfdk-aac, orlibvo_aacenc to encode AAC audio: ffmpeg -i input.wav -c:a aac -strict experimental output.m4aOr: brew reinstall ffmpeg --with-fdk-aac ffmpeg -i input.wav -c:a libfdk_aac output.m4a 经过这3步，现在就可以使用ffmpeg的强大功能了。 现在ffmpeg的版本更新很快，今年3月份发布了 FFmpeg 2.6.1，7月份发布了 FFmpeg 2.7.2，中间还有一些其它版本，比如2.7，2.7.1，这些版本我都在iOS平台上编译使用过，今年9月份，FFmpeg 的版本更新到了2.8。今年，我见证了FFmpeg更新最频繁的的时刻。 经过前面的3步，在Mac上安装了 ffmpeg 2.6.3 的版本，过段时间，homebrew上ffmpeg 的安装源就会更新，若想升级ffmpeg，就需要执行下面的第4步操作了。 最后，升级ffmpeg的版本若想升级ffmpeg的版本，可以在终端执行命令，brew update &amp;&amp; brew upgrade ffmpeg 实例安装好了ffmpeg，就要试试其功能了。采用文章利用x264将iOS摄像头实时视频流编码为h264文件配套工程X264-Encode-for-iOS中的h264文件，该文件地址为2015-09-17 18:05:20.h264，使用ffmpeg 命令将其打包为 .mp4容器格式的文件 将该h264文件下载，在终端上，执行命令进入存放该文件的目录 1devtemobideMac-mini:~ sunminmin$ cd /Users/dev.temobi/Desktop/sunmmMainPrj/ZZ_Z_Github_clone 进入到该目录后，执行命令，ffmpeg -i 2015-09-17\ 18_05_20.h264 2015-09-17.mp4 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748devtemobideMac-mini:ZZ_Z_Github_clone sunminmin$ ffmpeg -i 2015-09-17\ 18_05_20.h264 2015-09-17.mp4ffmpeg version 2.6.3 Copyright (c) 2000-2015 the FFmpeg developers built with Apple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn) configuration: --prefix=/usr/local/Cellar/ffmpeg/2.6.3 --enable-shared --enable-pthreads --enable-gpl --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-libx264 --enable-libmp3lame --enable-libvo-aacenc --enable-libxvid --enable-vda libavutil 54. 20.100 / 54. 20.100 libavcodec 56. 26.100 / 56. 26.100 libavformat 56. 25.101 / 56. 25.101 libavdevice 56. 4.100 / 56. 4.100 libavfilter 5. 11.102 / 5. 11.102 libavresample 2. 1. 0 / 2. 1. 0 libswscale 3. 1.101 / 3. 1.101 libswresample 1. 1.100 / 1. 1.100 libpostproc 53. 3.100 / 53. 3.100Input #0, h264, from '2015-09-17 18_05_20.h264': Duration: N/A, bitrate: N/A Stream #0:0: Video: h264 (Constrained Baseline), yuv420p, 480x360, 15 fps, 15 tbr, 1200k tbn, 30 tbc[libx264 @ 0x7fada2818000] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX[libx264 @ 0x7fada2818000] profile High, level 2.1[libx264 @ 0x7fada2818000] 264 - core 144 r2533 c8a773e - H.264/MPEG-4 AVC codec - Copyleft 2003-2015 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=15 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00Output #0, mp4, to '2015-09-17.mp4': Metadata: encoder : Lavf56.25.101 Stream #0:0: Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuv420p, 480x360, q=-1--1, 15 fps, 15360 tbn, 15 tbc Metadata: encoder : Lavc56.26.100 libx264Stream mapping: Stream #0:0 -&gt; #0:0 (h264 (native) -&gt; h264 (libx264))Press [q] to stop, [?] for helpframe= 64 fps=0.0 q=27.0 size= 60kB time=00:00:00.80 bitrate= 609.7kbits/s frame= 87 fps= 85 q=27.0 size= 187kB time=00:00:02.33 bitrate= 655.2kbits/s frame= 120 fps= 77 q=27.0 size= 366kB time=00:00:04.53 bitrate= 661.5kbits/s frame= 149 fps= 72 q=27.0 size= 444kB time=00:00:06.46 bitrate= 562.4kbits/s frame= 178 fps= 69 q=27.0 size= 508kB time=00:00:08.40 bitrate= 495.4kbits/s frame= 196 fps= 56 q=-1.0 Lsize= 651kB time=00:00:12.93 bitrate= 412.2kbits/s video:648kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.389284%[libx264 @ 0x7fada2818000] frame I:2 Avg QP:20.10 size: 5888[libx264 @ 0x7fada2818000] frame P:126 Avg QP:22.22 size: 4340[libx264 @ 0x7fada2818000] frame B:68 Avg QP:23.94 size: 1537[libx264 @ 0x7fada2818000] consecutive B-frames: 43.9% 27.6% 6.1% 22.4%[libx264 @ 0x7fada2818000] mb I I16..4: 31.4% 51.4% 17.2%[libx264 @ 0x7fada2818000] mb P I16..4: 8.6% 10.2% 4.6% P16..4: 46.2% 11.9% 4.2% 0.0% 0.0% skip:14.2%[libx264 @ 0x7fada2818000] mb B I16..4: 1.1% 0.6% 0.6% B16..8: 52.3% 4.9% 0.8% direct: 3.2% skip:36.5% L0:51.0% L1:45.1% BI: 3.9%[libx264 @ 0x7fada2818000] 8x8 transform intra:43.2% inter:63.2%[libx264 @ 0x7fada2818000] coded y,uvDC,uvAC intra: 42.6% 57.1% 10.4% inter: 19.2% 28.2% 0.7%[libx264 @ 0x7fada2818000] i16 v,h,dc,p: 20% 51% 10% 19%[libx264 @ 0x7fada2818000] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 24% 38% 22% 2% 2% 2% 4% 2% 5%[libx264 @ 0x7fada2818000] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 21% 43% 12% 3% 3% 3% 6% 4% 5%[libx264 @ 0x7fada2818000] i8c dc,h,v,p: 51% 32% 14% 3%[libx264 @ 0x7fada2818000] Weighted P-Frames: Y:7.1% UV:5.6%[libx264 @ 0x7fada2818000] ref P L0: 78.9% 10.4% 8.3% 2.3% 0.1%[libx264 @ 0x7fada2818000] ref B L0: 93.9% 5.6% 0.5%[libx264 @ 0x7fada2818000] ref B L1: 96.2% 3.8%[libx264 @ 0x7fada2818000] kb/s:405.98 到此，h264文件封装为.mp4格式的过程结束。 查看2015-09-17.mp4 文件，如图 2015-09-17.mp4 文件可以使用Quick Time Player，VLC 正常播放。 2015-09-17.mp4 文件的下载地址为：2015-09-17.mp4 2015-09-17 18:05:20.h264 文件的下载地址为：2015-09-17 18:05:20.h264]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[采用工具软件分析h264文件]]></title>
    <url>%2F2015%2F09%2F23%2Fuse-tool-to-analyze-h264-file%2F</url>
    <content type="text"><![CDATA[通过文章利用x264将iOS摄像头实时视频流编码为h264文件 和 利用FFmpeg+x264将iOS摄像头实时视频流编码为h264文件，实现了如何采集实时视频流并压缩为h264编码格式的文件。对于采集到h264文件，里面究竟有些什么，下面就通过工具软件来一探究竟。 采用文章利用x264将iOS摄像头实时视频流编码为h264文件配套工程X264-Encode-for-iOS中的h264文件，该文件地址为2015-09-17 18:05:20.h264 工具软件 UITraEdit H.264 的基本流（elementary stream,ES）的结构分为两层，包括视频编码层（VCL）和网络适配层（NAL）。视频编码层负责高效的视频内容表示，而网络适配层负责以网络所要求的恰当的方式对数据进行打包和传送。引入NAL并使之与VCL分离带来的好处包括两方面：其一、使信号处理和网络传输分离，VCL 和NAL 可以在不同的处理平台上实现；其二、VCL 和NAL 分离设计，使得在不同的网络环境内，网关不需要因为网络环境不同而对VCL比特流进行重构和重编码。 H.264 的基本流由一系列NALU （Network Abstraction Layer Unit ）组成，不同的NALU数据量各不相同。H.264 草案指出，当数据流是储存在介质上时，在每个NALU 前添加起始码：0x000001 或 0x00000001，用来指示一个NALU 的起始和终止位置。在这样的机制下，在码流中检测起始码，作为一个NALU得起始标识，当检测到下一个起始码时，当前NALU结束。 H.264 码流中每个帧的开头的3~4个字节是H.264 的start_code（起始码），0x00000001或者0x000001。3字节的0x000001只有一种场合下使用，就是一个完整的帧被编为多个slice（片）的时候，包含这些slice的NALU 使用3字节起始码。其余场合都是4字节0x00000001的。 每个NALU单元由一个字节的 NALU头（NALU Header）和若干个字节的载荷数据（RBSP）组成。其中NALU 头的格式如图所示： NALU头结构 长度：1byte forbidden_bit(1bit) + nal_reference_bit(2bit) + nal_unit_type(5bit) F：forbidden_zero_bit.1 位，如果有语法冲突，则为 1。当网络识别此单元存在比特错误时，可将其设为 1，以便接收方丢掉该单元。 NRI：nal_ref_idc.2 位，用来指示该NALU 的重要性等级。值越大，表示当前NALU越重要。具体大于0 时取何值，没有具体规定。 Type：5 位，指出NALU 的类型。 h264中NALU类型取值如下图(图片来至《新一代视频压缩编码标准H.264》) 将 2015-09-17 18:05:20.h264 文件用UITraEdit打开，效果如下图 由于数据量较大，我挑选了其中3段数据来分析。 分析第一段数据： 00 00 00 01 67 00 00 00 01 为NALU的起始标志。 00 00 00 01 后面的 67 为前面说的占1个字节的NALU头。将十六进制的67转换为二进制，得 0110 0111。 字段 所占bit位数 二进制 十进制 类型 forbidden_bit 1 0 0 nal_reference_bit 2 11 3 NALU 的重要性等级系数 nal_unit_type 5 00111 7 序列参数集，sps 00 00 00 01 68 00 00 00 01 为NALU的起始标志。 00 00 00 01 后面的 68 为前面说的占1个字节的NALU头。将十六进制的68转换为二进制，得 0110 1000。 字段 所占bit位数 二进制 十进制 类型 forbidden_bit 1 0 0 nal_reference_bit 2 11 3 NALU 的重要性等级系数 nal_unit_type 5 01000 8 图像参数集，pps 00 00 03 00 H.264规定，当检测到0x000000时，也可以表征当前NAL的结束。那么NAL中数据出现0x000001或0x000000时怎么办？H.264引入了防止竞争机制，如果编码器检测到NAL数据存在0x000001或0x000000时，编码器会在最后个字节前插入一个新的字节0x03，这样： 0x000000－&gt;0x00000300 0x000001－&gt;0x00000301 0x000002－&gt;0x00000302 0x000003－&gt;0x00000303 解码器检测到0x000003时，把03抛弃，恢复原始数据（脱壳操作）。解码器在解码时，首先逐个字节读取NAL的数据，统计NAL的长度，然后再开始解码。 分析第二段数据： 00 00 00 01 65 或 00 00 01 65 00 00 00 01 或 00 00 01 为NALU的起始标志。 00 00 00 01 或 00 00 01 后面的 65 为前面说的占1个字节的NALU头。将十六进制的65转换为二进制，得 0110 0101。 字段 所占bit位数 二进制 十进制 类型 forbidden_bit 1 0 0 nal_reference_bit 2 11 3 NALU 的重要性等级系数 nal_unit_type 5 00101 5 IDR图像中的片 IDR图像中的片，即 I帧。 我在用UITraEdit打开的2015-09-17 18:05:20.h264文件中搜索 00 00 00 01 65，没有找到该标志。搜索 00 00 01 65，存在该标志，说明一个完整的帧被编为多个slice（片）。 分析第三段数据： 00 00 00 01 41 00 00 00 01 为NALU的起始标志。 00 00 00 01 后面的 41 为前面说的占1个字节的NALU头。将十六进制的41转换为二进制，得 0100 0001。 字段 所占bit位数 二进制 十进制 类型 forbidden_bit 1 0 0 nal_reference_bit 2 10 2 NALU 的重要性等级系数 nal_unit_type 5 00001 1 不分区，非IDR图像的片 在baseline的档次中nal_unit_type表示的就是P帧，因为baseline没有B帧。 以上，就是对H264的结构和机制的一些分析。 扩展阅读： H264–1–编码原理以及I帧B帧P帧 H264–2–语法及结构]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用FFmpeg+x264将iOS摄像头实时视频流编码为h264文件]]></title>
    <url>%2F2015%2F09%2F18%2Fuse-ffmpeg-and-x264-encode-iOS-camera-video-to-h264%2F</url>
    <content type="text"><![CDATA[一、编译x264库如何编译x264源码获取支持iOS平台的静态库，可参考我的文章《编译iOS平台上使用的X264库》 二、编译FFmpeg库如何编译FFmpeg源码获取支持iOS平台的静态库，可参考我的博客园上的文章《实战FFmpeg－－编译iOS平台使用的FFmpeg库（支持arm64的FFmpeg2.6.2）》 三、将x264库编译进FFmpeg库通过步骤二，知道了如何编译FFmpeg库，但是要在FFmpeg中使用x264进行h264编码，还需要修改步骤二中的脚本。 步骤二中，使用的脚本的下载地址为：build-ffmpeg.sh 现在，FFmpeg的最新版本是 2.8，iOS系统的最新版本是 iOS 9.0.2，Xcode 最新版本是 Xcode 7.0.1，从 Xcode 7.0 开始支持 bitcode 选项了，bitcode 是什么，在百度上搜一搜就知道了。 看一看从下载地址获取的脚本，脚本内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154#!/bin/sh# directoriesSOURCE="ffmpeg-2.8"FAT="FFmpeg-iOS"SCRATCH="scratch"# must be an absolute pathTHIN=`pwd`/"thin"# absolute path to x264 library#X264=`pwd`/fat-x264#FDK_AAC=`pwd`/fdk-aac/fdk-aac-iosCONFIGURE_FLAGS="--enable-cross-compile --disable-debug --disable-programs \ --disable-doc --enable-pic"if [ "$X264" ]then CONFIGURE_FLAGS="$CONFIGURE_FLAGS --enable-gpl --enable-libx264"fiif [ "$FDK_AAC" ]then CONFIGURE_FLAGS="$CONFIGURE_FLAGS --enable-libfdk-aac"fi# avresample#CONFIGURE_FLAGS="$CONFIGURE_FLAGS --enable-avresample"ARCHS="arm64 armv7 x86_64 i386"COMPILE="y"LIPO="y"DEPLOYMENT_TARGET="6.0"if [ "$*" ]then if [ "$*" = "lipo" ] then # skip compile COMPILE= else ARCHS="$*" if [ $# -eq 1 ] then # skip lipo LIPO= fi fifiif [ "$COMPILE" ]then if [ ! `which yasm` ] then echo 'Yasm not found' if [ ! `which brew` ] then echo 'Homebrew not found. Trying to install...' ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" \ || exit 1 fi echo 'Trying to install Yasm...' brew install yasm || exit 1 fi if [ ! `which gas-preprocessor.pl` ] then echo 'gas-preprocessor.pl not found. Trying to install...' (curl -L https://github.com/libav/gas-preprocessor/raw/master/gas-preprocessor.pl \ -o /usr/local/bin/gas-preprocessor.pl \ &amp;&amp; chmod +x /usr/local/bin/gas-preprocessor.pl) \ || exit 1 fi if [ ! -r $SOURCE ] then echo 'FFmpeg source not found. Trying to download...' curl http://www.ffmpeg.org/releases/$SOURCE.tar.bz2 | tar xj \ || exit 1 fi CWD=`pwd` for ARCH in $ARCHS do echo "building $ARCH..." mkdir -p "$SCRATCH/$ARCH" cd "$SCRATCH/$ARCH" CFLAGS="-arch $ARCH" if [ "$ARCH" = "i386" -o "$ARCH" = "x86_64" ] then PLATFORM="iPhoneSimulator" CFLAGS="$CFLAGS -mios-simulator-version-min=$DEPLOYMENT_TARGET" else PLATFORM="iPhoneOS" CFLAGS="$CFLAGS -mios-version-min=$DEPLOYMENT_TARGET -fembed-bitcode" if [ "$ARCH" = "arm64" ] then EXPORT="GASPP_FIX_XCODE5=1" fi fi XCRUN_SDK=`echo $PLATFORM | tr '[:upper:]' '[:lower:]'` CC="xcrun -sdk $XCRUN_SDK clang" CXXFLAGS="$CFLAGS" LDFLAGS="$CFLAGS" if [ "$X264" ] then CFLAGS="$CFLAGS -I$X264/include" LDFLAGS="$LDFLAGS -L$X264/lib" fi if [ "$FDK_AAC" ] then CFLAGS="$CFLAGS -I$FDK_AAC/include" LDFLAGS="$LDFLAGS -L$FDK_AAC/lib" fi TMPDIR=$&#123;TMPDIR/%\/&#125; $CWD/$SOURCE/configure \ --target-os=darwin \ --arch=$ARCH \ --cc="$CC" \ $CONFIGURE_FLAGS \ --extra-cflags="$CFLAGS" \ --extra-ldflags="$LDFLAGS" \ --prefix="$THIN/$ARCH" \ || exit 1 make -j3 install $EXPORT || exit 1 cd $CWD donefiif [ "$LIPO" ]then echo "building fat binaries..." mkdir -p $FAT/lib set - $ARCHS CWD=`pwd` cd $THIN/$1/lib for LIB in *.a do cd $CWD echo lipo -create `find $THIN -name $LIB` -output $FAT/lib/$LIB 1&gt;&amp;2 lipo -create `find $THIN -name $LIB` -output $FAT/lib/$LIB || exit 1 done cd $CWD cp -rf $THIN/$1/include $FATfiecho Done 三（一）、注意内容11234567891011121314151617181920# absolute path to x264 library#X264=`pwd`/fat-x264#FDK_AAC=`pwd`/fdk-aac/fdk-aac-iosCONFIGURE_FLAGS="--enable-cross-compile --disable-debug --disable-programs \ --disable-doc --enable-pic"if [ "$X264" ]then CONFIGURE_FLAGS="$CONFIGURE_FLAGS --enable-gpl --enable-libx264"fiif [ "$FDK_AAC" ]then CONFIGURE_FLAGS="$CONFIGURE_FLAGS --enable-libfdk-aac"fi# avresample#CONFIGURE_FLAGS="$CONFIGURE_FLAGS --enable-avresample" 要将x264编译进FFmpeg中，需要取消对该句代码的注销 X264=`pwd`/fat-x264 即， #X264=`pwd`/fat-x264 改为 X264=`pwd`/fat-x264 三（二）、注意内容2经过步骤三（一）的修改后，在Mac终端执行FFmpeg，脚本的时候，可能会存在因为bitcode引起编译出错。 需要将脚本中 1CFLAGS="$CFLAGS -mios-version-min=$DEPLOYMENT_TARGET -fembed-bitcode" 修改为 1CFLAGS="$CFLAGS -mios-version-min=$DEPLOYMENT_TARGET" 四、总结编译x264以及支持x264的FFmpeg的步骤 编译得到 x264 静态库 将存放x264静态库（头文件，库文件）的文件夹名称改为 fat-x264 （因为编译FFmpeg的脚本中定义存放x264文件的文件夹名称为fat-x264，看步骤三（一）中的内容） 修改编译FFmpeg的脚本build-ffmpeg.sh，具体见步骤 三（一），三（二） 将编译FFmpeg的脚本build-ffmpeg.sh 与 fat-x264 存放到同一个目录下 在Mac终端执行脚本build-ffmpeg.sh 最后x264静态库，支持x264的FFmpeg静态库，内容如下 五、获取iOS设备摄像头实时视频六，采用x264和FFmpeg对iOS实时视频流编码为h264七、完整的代码下载地址采用FFmpeg+x264将iOS摄像头实时视频流编码为h264文件的工程的下载地址为：FFmpeg-X264-Encode-for-iOS]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用x264将iOS摄像头实时视频流编码为h264文件]]></title>
    <url>%2F2015%2F09%2F17%2Fuse-x264-encode-iOS-camera-video-to-h264%2F</url>
    <content type="text"><![CDATA[代码见我的Github代码托管区域：X264-Encode-for-iOS]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译iOS平台上使用的X264库]]></title>
    <url>%2F2015%2F09%2F16%2Fbuild-X264-library-for-iOS-platform%2F</url>
    <content type="text"><![CDATA[x264是一种免费的、具有更优秀算法的符合H.264/MPEG-4 AVC视频压缩编码标准格式的编码库。它同xvid一样都是开源项目，但x264是采用H.264标准的，而xvid是采用MPEG-4早期标准的。由于H.264是2003年正式发布的最新的视频编码标准，因此，在通常情况下，x264压缩出的视频文件在相同质量下要比xvid压缩出的文件要小，或者也可以说，在相同体积下比xvid压缩出的文件质量要好。它符合GPL许可证。 从iOS8开始，苹果开放了硬解码和硬编码API，框架为 VideoToolbox.framework， 此框架需要在iOS8及以上的系统上才能使用。 此框架中的硬解码API是几个纯C函数，在任何OC或者 C++代码里都可以使用。使用的时候，首先，要把 VideoToolbox.framework 添加到工程里，并且在要使用该API的文件中包含头文件 #include &lt;VideoToolbox/VideoToolbox.h&gt;，然后，就可以畅快的高效的对视频流进行硬编码了。 其实至少从iPhone4开始，苹果就是支持硬件解码了，但是硬解码API框架VideoToolBox一直是私有API，如果调用这个私有库，那么app在必须在越狱的设备上运行，正常的App如果想提交到AppStore是不允许使用私有API的。 此处，不讨论iOS硬编解码框架VideoToolbox.framework的使用。现在，使用较多的h264编码工具还是X264开源库。要想在iOS平台上使用X264编码视频流，首先要知道如何编译运行在iOS上的X264静态库。 编译X264的一个靠谱的脚本为https://github.com/kewlbear/x264-ios，但是，使用该脚本有个不方便的，就是要先下载X264并解压好，同时要下载https://github.com/libav/gas-preprocessor并将gas-preprocessor.pl拷贝到/usr/local/bin/下，并且赋予管理员权限，才能启动脚本进行编译。 为了使用脚本一步到位得到X264静态库，我在该脚本的基础上做了一些修改。 第一处修改：# sunminmin blog: http://depthlove.github.io/ # modified by sunminmin, 2015/09/07 #ARCHS=&quot;arm64 armv7s x86_64 i386 armv7&quot; ARCHS=&quot;arm64 x86_64 i386 armv7&quot; 第二处修改：# begin: added by sunminmin, 2015/09/07 if [ ! -r $GAS_PREPROCESSOR ] then echo &apos;gas-preprocessor.pl not found. Trying to install...&apos; (curl -L https://github.com/libav/gas-preprocessor/blob/master/gas-preprocessor.pl \ -o /usr/local/bin/gas-preprocessor.pl \ &amp;&amp; chmod +x /usr/local/bin/gas-preprocessor.pl) \ || exit 1 fi if [ ! -r $SOURCE ] then echo &apos;x264 source not found. Trying to download...&apos; curl https://download.videolan.org/pub/x264/snapshots/x264-snapshot-20140930-2245.tar.bz2 | tar xj &amp;&amp; ln -s x264-snapshot-20140930-2245 x264 || exit 1 fi # end: added by sunminmin, 2015/09/07 第三处修改：# begin: added by sunminmin, 2015/09/07 echo &quot;copy config.h to ...&quot; for ARCH in $ARCHS do cd $CWD echo &quot;copy $SCRATCH/$ARCH/config.h to $THIN/$ARCH/$include&quot; cp -rf $SCRATCH/$ARCH/config.h $THIN/$ARCH/$include || exit 1 done echo &quot;building success!&quot; # end: added by sunminmin, 2015/09/07 修改后的脚本下载地址：https://github.com/depthlove/x264-iOS-build-script 原始脚本下载地址：https://github.com/kewlbear/x264-ios 后续文章后写如何使用x264库对iOS设备摄像头实时视频流进行h264软编码。]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[音视频传输解决方案]]></title>
    <url>%2F2015%2F07%2F02%2Faudio-video-transmission-plan%2F</url>
    <content type="text"><![CDATA[协议流媒体协议：RTSP（Real Time Streaming Protocol），RTMP（Real Time Messaging Protocol），HLS(http live streaming) 会话描述协议：SDP（Session Description Protocol） 实时传输控制协议：RTCP（Real-time Transport Control Protocol或RTP Control Protocol） 实时传输协议：RTP（Real-time Transport Protocol） 流的类型流的类型：TS流，RTP流 容器类型 .mp4 音频编解码格式视频编解码格式BSD socket Introduction to BSD Sockets 知识点的维基百科解释维基百科：rtsp－实时流协议 维基百科：rtmp－实时消息传递协议 维基百科：HLS－HTTP实时流 维基百科：SDP－会话描述协议 维基百科：RTCP－实时传输控制协议 维基百科：RTP－实时传输协议 维基百科：BSD socket]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[硕士毕业一年的感受]]></title>
    <url>%2F2015%2F07%2F01%2Ffeeling-of-OneYear-after-master-graduation%2F</url>
    <content type="text"><![CDATA[时间过的很快，转眼毕业一年了，去年的今天拿到硕士毕业证和庆祝毕业的场景在脑海中清晰可见。可是，时间就这么过去了。 在一个北方的城市，待了七年（2007～2014），自己从奔二的年纪跨越到奔三的年纪，想想觉得时间真的好可怕。在自己渴望成长的年龄段，父母变老了，弟弟妹妹们都已到了读大学的年纪，小外甥都上幼儿园了。现在回忆过去，我的感受是处在最美最纯的场景中的时候，却拼命的渴望摆脱这个单纯的生活，追求复杂而且功利的生活，眼前的美好却不曾珍惜。而今，处在自己曾今刻意追赶的生活中，不过是人生阶段必经的过程，可惜的是以往该放慢脚步体验生活的阶段再也回不去了。 深处在一个以学习就是为了找一份好工作，拿到高薪的大环境中，作为学生深受这种氛围的影响，变得唯利是图，变得焦躁不安，变得对未来恐慌，而不把握当下和珍惜当下最美的时光。对过去的追忆无外乎两种，一种是结合现在的处境反思以前的人生阶段的遗憾，一种是回想过去美好的记忆。是甜蜜是懊悔，只有当事人知道，毕竟个人的经历是独一无二的。 2011年本科毕业的时候，我说我不想家，可是自从读研之后，体验到人生滋味，家就是心灵休憩和疗伤的港湾，开始念家了。人生的每个阶段，都有其明显的特点，当明白这些道理的时候，可是已经从时间的长河中走过来了，想回去却再也回不去，明白又能怎样，最多向比自己小的弟弟妹妹们说，或者充当和下一辈人的经验之谈，但是这有什么用呢，他们不曾经历，听过或许就忘了。 现在，对于生活已经不做太多指望，最好的奋斗阶段过去了，奋斗的条件也不在。年龄决定了选择生活的范围。这就是为什么工作几年之后的人，激情不在，平平淡淡的原因。 对于在大学期间是否找对象这件事，我在学生期间计划是自己要读博士，很多未知的因素，没有生存能力，还是不谈为好。虽然，现在年纪不小了，有基本生存能力了，自己的心态，以及社会的因素，周围环境的变化，我选择顺其自然。我对自己的认识是，不喜欢强求别人，所谓的死缠烂打只会让自己显得无聊，没什么意思，也最烦这样的场面。 硕士毕业前就参加工作了，没有体会到一般学生能在暑假休息一段时间才上班的感觉，现在想来，毕业后的一段时间应该是调整心态，做好规划，休整疲惫身躯的好光阴。 过往似云烟，珍惜当下。]]></content>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我对毕业生进大公司与小公司的看法]]></title>
    <url>%2F2015%2F06%2F18%2Fview-graduates-which-enter-the-large-companies-or-small-companies%2F</url>
    <content type="text"><![CDATA[2014年7月1日毕业，2014年4月23日第一天正式上班，1个半月就从试用期转正，到今天已经工作1年2个月了。工作了1年多，重新思考以前自己面对的问题，以前对毕业之后究竟要找一个什么样的公司很模糊，听到的是进大公司好，福利好，做颗小螺丝钉，小公司加班多，待遇差，锻炼人。究竟大公司与小公司对初入社会的毕业生今后的职业生涯与生存状态有多大影响？ 小公司与大公司的直接明了的区别就是职员人数，几十个人的公司虽在业务经济效益上可能取胜人数上百的大公司，但是整个公司的建制体系跟数百人上千人的公司是无法比的。 我的第一份工作是在一家小型互联网公司，在小公司行为与装束可以随意，上班可以穿个拖鞋就啪啪啪的去公司开工了。若想在小公司开拓眼界以及技术的延伸，就不要指望公司的条件了，自个掏钱自个多关注信息。以前，总想着公司能请些牛人给我们做讲座，给我们充充电，或者希望公司能安排去参加一些好的讲座，这些在小公司简直就是自己在瞎想，不切实际。 对于小公司锻炼人这个说法，其实我并不认同，为什么能锻炼人，不就是公司在使用少的人力资源在员工上获取更大的回报。但是，员工收到利益上的回报有变化吗，没有。锻炼人，也是在员工花更多的私人时间泡在公司加班或在家加班的投入所致，这是投入大量时间之后，应有的收获。与小公司能锻炼人其实并无因果关系。小公司能发挥自己更大的能力，有更大的自由发挥空间？公司项目与业务的决策，一个小职员究竟有多大的份量，刚出来的毕业生懂得不多，几乎是不懂，就是想发挥也没地。 小公司做事的流程好吗？没有大公司复杂的流程，但面临的是上面的指令下达后，朝令夕改。这个对于程序员来说，实在是不堪重负的。在小公司天天见到老板，在大公司除了能看见或认识所在部门的一些人，其它部门的人基本就是陌生的，好像在另一个未知区域。 在大一点的公司里面做事，首先是格局与视野层面上对自己以后的发展有好处，其次，现在就职的公司就是以后跳槽或转行的平台，对加薪和升职影响很大。 非诚勿扰里面有句经典话语，“宁愿坐在宝马车上哭，也不愿坐在自行车上笑”，虽然这句对人的道德观和价值观有很大的扭曲，但是直接反映了这个社会的真实情况，作为工薪一族，“宁愿在大公司拼，也不愿在小公司做”，好的职业背景，带给自己的不仅仅是金钱上的优势，更有助于成就一番或大或小的事业。]]></content>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[帮助平台部解决重庆项目的bug]]></title>
    <url>%2F2015%2F06%2F15%2Fhelp-platform-department-to-solve-the-bug-of-chongqing-project%2F</url>
    <content type="text"><![CDATA[属于网络研发部的我，职位是播放器开发工程师，主要工作是播放器库的开发与维护。今天，收到老大（网络研发部经理）的工作安排，帮助平台部iOS工程师做重庆的项目，为期十五天左右。 工作就是修项目的bug，好比接手烂尾工程紧急修复在规定的时间段完工。百分之九十的程序员，是不愿意看别人写的代码的，因为代码不规矩，看一句代码要吐槽N句。 既然接手了，就要完成工作，花了点时间看了质量部测试技术提交上来的bug反馈单，带着些疑问又与测试技术沟通了下，确定解决这些问题一个星期内可以完成。但是经过我亲自去测试这个项目的时候，bug远不止反馈单上的那么点，只能说写这个项目的人太渣了。 维修师傅，is me!]]></content>
      <tags>
        <tag>工作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac搭建hexo博客]]></title>
    <url>%2F2015%2F06%2F12%2Fuse-hexo-create-blog-in-mac%2F</url>
    <content type="text"><![CDATA[一、安装git启动Mac终端，在终端执行命令ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; brew install git 二、安装Node.jsnvm (Node Version Manager)，详见官方介绍：https://github.com/creationix/nvm#node-version-manager 在终端执行命令 git clone git://github.com/creationix/nvm.git ~/.nvm cd /Users/dev.temobi/.nvm （备注：/Users/dev.temobi/.nvm 是执行上一条命令的结果显示.nvm所在的路径） sh install.sh 补充说明：对于Mac系统而言，可以跳过步骤一、步骤二，直接去node.js官网https://nodejs.org/en/#download下载软件，然后安装到Mac电脑上，node.js 就成功安装到了电脑上了，采用这种方法更简单，更直接。三、安装hexo在终端执行命令 npm install -g hexo 创建文件夹，根据自己实际情况，文件夹名可以任取，文件夹的路径可以选择你想放文件夹的路径。我创建的文件夹为 ZZ_HexoBlog，路径为 /Users/dev.temobi/Desktop/sunmmMainPrj/ZZ_HexoBlog 进入文件夹ZZ_HexoBlog所在路径，在终端执行命令 cd /Users/dev.temobi/Desktop/sunmmMainPrj/ZZ_HexoBlog 继续执行命令 hexo init npm install 现在已经搭建起本地的hexo博客了。 执行命令 hexo generate hexo server 然后到浏览器输入 http://0.0.0.0:4000/ 就可以预览博客了 四、部署hexo博客到github在自己的github账号中创建RepositoryRepository name需要符合username.github.io的命名规则。比如我的Github账号是depthlove，我就要创建名为depthlove.github.io 的仓库。 获取自己的Repository地址我的为 https://github.com/depthlove/depthlove.github.io.git 修改_config.yml文件在终端上执行命令 vim _config.yml 找到 deploy: type: 修改为： deploy: type: git repo: https://github.com/depthlove/depthlove.github.io.git branch: master 保存退出 在终端执行命令 ls -al ~/.ssh，查看是否已经有SSH keys，如果之前在该电脑上使用过git管理过代码，结果是~/.ssh中会存在 id_rsa id_rsa.pub known_hosts 三个文件 在终端继续执行命令 ls -al ~/.ssh 若之前没有使用过git，该目录就是空的。有用过git管理项目结果如下 total 32 drwx------ 5 dev.temobi staff 170 6 8 17:00 . drwxr-xr-x+ 69 dev.temobi staff 2346 6 12 13:47 .. -rw------- 1 dev.temobi staff 1675 6 8 16:46 id_rsa -rw-r--r--@ 1 dev.temobi staff 399 6 8 17:04 id_rsa.pub -rw-r--r-- 1 dev.temobi staff 1593 6 8 18:27 known_hosts 执行命令 ssh-keygen -t rsa -C &quot;depthlove@163.com&quot; depthlove@163.com 是注册github账号时所用的邮箱 执行上面的命令后，会出现一些提示，什么都不用管，直接回车，再回车 继续输入命令 ssh-agent -s 结果为 SSH_AUTH_SOCK=/var/folders/0r/y1__8yjx579743zfttfrflsr0000gq/T//ssh-a9zU09oKAnpw/agent.17852; export SSH_AUTH_SOCK; SSH_AGENT_PID=17853; export SSH_AGENT_PID; echo Agent pid 17853; 继续输入命令 ssh-add ~/.ssh/id_rsa 结果为 Identity added: /Users/dev.temobi/.ssh/id_rsa (/Users/dev.temobi/.ssh/id_rsa) 如果执行ssh-add ~/.ssh/id_rsa 出错，就输入以下指令： eval `ssh-agent -s` ssh-add 现在就成功生成了 SSH keys。 把SSH keys添加到github账户中。前往文件夹 ~/.ssh，用文本编辑器打开文件id_rsa.pub，复制其中的内容，然后粘贴到github add SSH keys的文本框中。操作完之后，github会要求输入github的登陆密码，输入完密码就成功添加了 SSH keys。 若还不放心，还可以通过 ssh -T git@github.com 命令查看SSH keys是否添加成功，继续在终端执行命令 ssh -T git@github.com 结果如下 Are you sure you want to continue connecting (yes/no)?yes // 输入yes Warning: Permanently added the RSA host key for IP address &apos;192.xx.xxx.xxx’ to the list of known hosts. Hi depthlove! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 到这里准备工作都已做好，现在就是部署了。 继续在终端执行命令 hexo generate hexo deploy 结果出现错误 ERROR Deployer not found: git 此时，就要检查为什么出现这种错误了，解决方法是上网百度寻求解决方案，还有一种就是靠自己的经验了。我发现在我修改文件_config.yml的时候发现 # Deployment ## Docs: http://hexo.io/docs/deployment.html deploy: type: 发现有个word文档说明，而且我在执行hexo deploy出错，说明解决方法肯定在word链接中，结果是答案真的就在这里面。 在浏览器中输入 http://hexo.io/docs/deployment.html 然后按照网页上的文档说明，继续在终端上执行命令 npm install hexo-deployer-git --save 最后在终端上执行命令 hexo deploy OK，成功了！结果如下 INFO Deploying: git INFO Setting up Git deployment... 初始化空的 Git 版本库于 /Users/dev.temobi/Desktop/sunmmMainPrj/ZZ_HexoBlog/.deploy_git/.git/ [master（根提交） e8f14ef] First commit Committer: dev.temobi &lt;dev.temobi@devtemobideMac-mini.local&gt; 您的姓名和邮件地址基于登录名和主机名进行了自动设置。请检查它们正确 与否。您可以通过下面的命令对其进行明确地设置以免再出现本提示信息： git config --global user.name &quot;Your Name&quot; git config --global user.email you@example.com 设置完毕后，您可以用下面的命令来修正本次提交所使用的用户身份： git commit --amend --reset-author …… …… …… create mode 100644 index.html create mode 100644 js/script.js delete mode 100644 placeholder Username for &apos;https://github.com&apos;: depthlove // 输入自己的github用户名 Password for &apos;https://depthlove@github.com&apos;: // 输入自己的github密码 To https://github.com/depthlove/depthlove.github.io.git + f0b2e27...cd56463 master -&gt; master (forced update) 分支 master 设置为跟踪来自 https://github.com/depthlove/depthlove.github.io.git 的远程分支 master。 INFO Deploy done: git 现在，就可以在浏览器中输入 http://depthlove.github.io ，访问我的hexo博客了 注意：每次修改本地文件后，都要在hexo博客所在的目录下，执行命令 hexo generate 保存；想要上传文件到Github时，就应该在hexo博客所在的目录下先执行命令 hexo generate 保存之后，再执行命令 hexo deploy 补充hexo现在支持更加简单的命令格式了，比如： hexo g == hexo generate hexo d == hexo deploy hexo s == hexo server hexo n == hexo new]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔谈cocoapods的安装与使用]]></title>
    <url>%2F2015%2F05%2F11%2Ftalk-about-cocoapods-install-and-use%2F</url>
    <content type="text"><![CDATA[因为要重构播放器库，所以就需要参考网上的开源项目，在播放器开源项目这块，kxmovie开源项目是值得参考的一个项目。在github下载下来后，运行该工程，发现其用到了cocoapods来管理第三方库，以前我做项目都是将第三方库直接下载然后将源文件导入到工程，这种做法有其好处也存在一定的弊端，好处是便于项目的维护，方便的知道过去使用的第三方库是个什么情况，还可以根据实际需求修改，弊端就是第三方库的更新需要自己下载最新的再将旧的替换（手动更新）。通过cocoapods来管理第三方库，可以获取到最新的第三库将其引用到项目中，而且不需要自己手动去添加该第三方库的依赖库，虽然cocoapods用起来方便，但是也不一定全好，因为项目运行链接第三方库的时候，比如之前自己改过cocoapods引用进来的第三方库，这时就悲剧了，加载的第三方库是重新从网络上获取的，网上一些开发者也提到了，通过cocoapods管理项目中的第三方库不便于项目回滚。所以，是否选择cocoapods要根据实际情况来定。 要使用cocoapods，对于之前没有安装过cocoapods的开发者来说，首先就是要在Mac上安装cocoapods，在Mac终端执行命令 sudo gem install cocoapods，执行结果如下 但是没有发现cocoapods源，因为ruby的软件源rubygems.org使用的亚马逊的云服务，被墙了，需要更新一下ruby的源，使用其他能支持的源—-&gt;国内淘宝的源： gem sources --remove https://rubygems.org/ gem sources -a http://ruby.taobao.org/ gem sources -l 执行上面的命令结果如下： 现在，更新源成功了，可以进行安装了，继续在Mac终端执行命令 sudo gem install cocoapods，执行结果如下： …………. …………. 接下来在Mac终端输入以下命令：pod setup 注意：This process will likely take a while as this command clones the CocoaPods Specs repository into ~/.cocoapods/ on your computer. 来看看我们安装的cocoapods的版本信息，在Mac终端上执行命令 pod –version，执行结果如下： OK，cocoapods安装成功了。 若要卸载cocoapods， 就在Mac终端执行命令 sudo gem uninstall cocoapods 参考文章：用CocoaPods做iOS程序的依赖管理 iOS.CocoaPods.0 OS X升级到10.10之后使用pod出现问题的解决方法 osx升级到10.10后，用pod install报错最终解决办法 cocoapods安装好了，接下来就该用它来做事了。使用CocoaPods管理第三方库的例子如下： 使用Xcode,在工程根目录下，新建立一个空白的Podfile文档，然后在里面添加以下内容 platform:ios,&apos;6.0&apos; pod &apos;FMDB&apos;, &apos;~&gt; 2.0&apos; pod &apos;AFNetworking&apos;, &apos;~&gt; 1.1.0&apos; pod &apos;JSONKit&apos;,&apos;~&gt;1.4&apos; 保存，然后配置工程， 在系统终端中，使用cd命令切换到项目根目录下，输入命令： pod install 执行完之后，CocoaPods在工程目录下创建了一个文件夹“Pods”，该文件夹存放所有依赖的库，另外还创建了一个.xcworkspace文件，配置完之后需使用.xcworkspace文件打开工程。 参考文章：CocoaPods安装和使用教程 iOS系列译文：深入理解 CocoaPods 对于kxmoive这个工程，它使用cocoapods就引用了一个库，对于我而言，就因为引用一个库反复折腾cocoapods，肯定不爽，浪费时间。所以，我就想删掉kxmoive工程中cocoapods的所有相关东西。但是，删除cocoapods后，出现了如下错误： diff: /../Podfile.lock: No such file or directory diff: /Manifest.lock: No such file or directory error: The sandbox is not in sync with the Podfile.lock. Run ‘pod install’ or update your CocoaPods installation. 我参考文章 从工程中删除Cocoapods ，顺利解决了这个问题。对于上面的这个报错，当工程中有使用cocoapods的时候，运行项目也可能会出现这个问题，那就按照报错提示，重新更新pod，即在Mac终端执行pod install，参考文章 Xcode工程使用CocoaPods管理第三方库新建工程时出现错误]]></content>
      <tags>
        <tag>iOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔谈FFmpeg（二）]]></title>
    <url>%2F2015%2F04%2F28%2Ftalk-about-FFmpeg-part2%2F</url>
    <content type="text"><![CDATA[经过前面的学习对FFmpeg的基本流程已经很熟悉了，现在到了掌握其中细节的时候了，用FFmpeg做播放器解码操作中，涉及到了一些结构体，这些结构之间到底有什么关系，它们是怎样协同工作的呢。文章 FFMPEG中最关键的结构体之间的关系 对这些结构间的关系进行了分析，详细内容如下： FFMPEG中结构体很多。最关键的结构体可以分成以下几类： a) 解协议（http,rtsp,rtmp,mms）AVIOContext，URLProtocol，URLContext主要存储视音频使用的协议的类型以及状态。URLProtocol存储输入视音频使用的封装格式。每种协议都对应一个URLProtocol结构。（注意：FFMPEG中文件也被当做一种协议“file”） b) 解封装（flv,avi,rmvb,mp4）AVFormatContext主要存储视音频封装格式中包含的信息；AVInputFormat存储输入视音频使用的封装格式。每种视音频封装格式都对应一个AVInputFormat 结构。 c) 解码（h264,mpeg2,aac,mp3）每个AVStream存储一个视频/音频流的相关数据；每个AVStream对应一个AVCodecContext，存储该视频/音频流使用解码方式的相关数据；每个AVCodecContext中对应一个AVCodec，包含该视频/音频对应的解码器。每种解码器都对应一个AVCodec结构。 d)存数据视频的话，每个结构一般是存一帧；音频可能有好几帧 解码前数据：AVPacket 解码后数据：AVFrame 他们之间的对应关系如下所示： 上面提到的这些结构到底是干嘛用的，中国传媒大学的一个博士写了一系列的结构体的分析的文章，在这里列一个列表，需要好好看下： FFMPEG结构体分析：AVFrameFFMPEG结构体分析：AVFormatContextFFMPEG结构体分析：AVCodecContextFFMPEG结构体分析：AVIOContextFFMPEG结构体分析：AVCodecFFMPEG结构体分析：AVStreamFFMPEG结构体分析：AVPacket]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔谈FFmpeg（一）]]></title>
    <url>%2F2015%2F04%2F27%2Ftalk-about-FFmpeg-part1%2F</url>
    <content type="text"><![CDATA[现在的工作是播放器库的开发，可不是调用iOS系统自带的播放器框架进行一些简单的功能和界面定制，这些没什么含量。涉及iOS开发有3个年头了，现在的工作算是有点含金量了。涉及播放器的开发，FFmpeg的架构和功能是必须清楚的。FFmpeg自带的三个工程：ffplay， ffmpeg， ffprobe。这三个工程的代码量太大，如何切入进去，一窥其中的奥秘为自己所用呢？从核心切入，编码和解码。编码和解码的核心API接口就那十几个，通过这些深入然后剖析源代码，目标就明确了。 就我个人而言，首先要了解FFmpeg整个的运行机制，哪一部分工作需要调用FFmpeg的哪一块，这个必须清楚。播放器库的开发，解码播放这就是核心，我就需要从FFmpeg的解码流程入手了。FFmpeg源代码结构图 - 解码 这篇文章太好了，看得我两眼放光，精华。这篇文章读透了，完全可以把控FFmpeg的使用。我接下来的学习任务，那就是认真研读和敲代码研习，光看是不顶用的，需要动手写。 FFmpeg源代码结构图 - 解码中包含的信息太多了，对于在音视频领域中的初学者来说，首先要看FFmpeg源代码结构图 - 解码 中提到的简单的基于FFMPEG+SDL的视频播放器 ver2 （采用SDL2.0）入手。心急吃不了热豆腐，心急就不能静下心来搞懂深层次的问题。FFmpeg的解码过程调用的API依次为： 开始—-&gt; av_register_all(); avformat_open_input()； av_find_stream_info(); avcodec_find_decoder(); av_read_frame(); 获取到packet—-&gt; avcodec_decode_video2(); 对于FFmpeg的解码流程，先知道了有上面那些方法，但是这些方法是用来做什么，从方法名上能看出其功能，要深入的理解才行。借鉴别人的经验，收集有效的资源是非常重要的。对FFmpeg解码流程讲解的比较好的一篇博文是 FFMpeg的解码流程 ，配合文章 ffmpeg解码流程 一起服下，效果更好。因为当当看 简单的基于FFMPEG+SDL的视频播放器 ver2 （采用SDL2.0） 还不足以搞透整个流程。通过研习这三篇文章，整个流程和方法功能应该就吃透了。 学习任何东西什么时候该从局部把控全局，那就是有了自己的切入点之后，在这个点上摸爬滚打搞得比较熟练之后，就需要把控全局，对FFmpeg框架的整体要有把控。对于FFmpeg框架讲解的比较好的入门读物是 FFMpeg框架代码阅读 。]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈一谈做iOS播放器库开发所涉及的知识点]]></title>
    <url>%2F2015%2F04%2F22%2Ftalk-about-knowledge-points-of-developing-iOS-player%2F</url>
    <content type="text"><![CDATA[在自己研究生毕业的时候，想着能找上一份做视频编解码的工作，可惜没有如愿，最后到了一家iOS游戏渠道公司去做游戏支付业务的SDK开发，我的iOS正式开发生涯就这么开始了。 在那家iOS游戏渠道没做上一年，就离职了，至于怎么离职的，后续文章会谈一谈，以此来梳理下自己的职业规划。说了这多了，进入正题吧，今年3月份找上了一家做音视频服务的公司，做iOS播放器的开发，职位是播放器开发工程师，就是我现在所在的公司咯。 要开发一套属于自己的播放器库，不利用移动设备上自带的播放器来播放音频、视频，要用到哪些知识点呢，下面以我熟悉公司播放器库的前提下，说一说我的看法。 任何客户端只要跟服务器打交道，少不了通讯协议。音视频这块涉及的实时流相关协议很多，有RTSP、RTMP、MMS、HLS、RTP、RTCP、SDP、TCP、UDP、HTTP等。 客户端从服务器上获取到的音视频数据，要知道容器与编码方式的区别，封装音视频数据的容器类型主要有AVI(.avi), MPG(.mpg/.mpeg/.dat), VOB(.vob), MP4, 3GP, ASF(.wmv/.asf), RM(.rm/.rmvb), MOV(.mov), MKV, WAV, TS。 客户端从服务器上获取到了音视频数据，该如何进行解码显示，首先要知道音频、视频的的编码方式，客户端要显示音视频数据需要根据编码的方式进行相应的解码操作，目前常见的编码类型有MPEG系列、H.26X系列、微软windows media系列、Real Media系列、QuickTime系列。 上面说到了一些流媒体协议、流媒体数据的封装类型以及编码方式。而我们要做一款播放器首先是要对以上知识要了解的。 公司的业务涉及最多的是rtsp这块。服务器端为rtsp流媒体服务器，客户端也就是播放器库采用FFMpeg进行解码、OPenGL ES进行YUV视频数据渲染。 播放器库与服务器端进行交互，涉及到RTSP协议的请求，传输层协议采用的TCP、UDP协议，所以要对TCP连接的三次握手要熟悉，这其中也就涉及到网络编程中的SOCKET编程知识了，BSD socket编程是需要掌握的。 播放器库对从服务器上请求到的音频、视频rtp包，要进行解包，就是去掉一些协议的头获取到音视频数据段。获取到这些数据后，不能直接播放，需要进行解码操作。视频解码出来一般为Planar 4:2:0 YUV格式。要显示YUV视频图像就需要利用OPenGL ES进行渲染了。 播放器在工作时，视频数据要进行解码放入数据缓存区，数据缓存区的解码后的数据被取出交给OpenGL进行渲染，所以多线程是必不可少的环节了。开辟2个线程，一个线程进行解码处理，另一个线程进行视频数据的渲染。多线程中常使用的是POSIX thread多线程编程。 虽然开发的iOS播放器库，但是底层的东西大部分是c语言的东西，比如用到开源库FFMpeg，以及一些上层的对FFMpeg的封装，数据缓存区，所以c语言和数据结构的基础要扎实，什么函数指针，内存分配与管理，数据结构中的单链表那得玩得比较溜。 总结一下，需要具备的知识有 rtsp、sdp、tcp、udp、ip协议（rtsp的DESCRIBE、OPTION、SETUP、PLAY、PAUSE、TEARDOWN；tcp连接的三次握手／断开的四次握手） socket（bsd socket） 多线程（posix thread） opengl es FFmpeg（知道用它来解码） YUV420（知道它的原理与格式） 音视频同步（时间戳的处理） C语言指针（void *、函数指针、回调函数） 内存管理（堆区、栈区、静态区、内存对齐） 数据结构（单链表）]]></content>
      <tags>
        <tag>音视频处理</tag>
      </tags>
  </entry>
</search>
